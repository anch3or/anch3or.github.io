<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>Generative_Adversarial_Networks</title>
    <link href="/2020/05/05/Generative-Adversarial-Networks/"/>
    <url>/2020/05/05/Generative-Adversarial-Networks/</url>
    
    <content type="html"><![CDATA[<h1 id="生成对抗网络（Generative-Adversarial-Networks）"><a href="#生成对抗网络（Generative-Adversarial-Networks）" class="headerlink" title="生成对抗网络（Generative Adversarial Networks）"></a>生成对抗网络（Generative Adversarial Networks）</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><h2 id="1-GAN生成对抗网络的原理及实现"><a href="#1-GAN生成对抗网络的原理及实现" class="headerlink" title="1. GAN生成对抗网络的原理及实现"></a>1. GAN生成对抗网络的原理及实现</h2><h3 id="模型结构及优化"><a href="#模型结构及优化" class="headerlink" title="模型结构及优化"></a>模型结构及优化</h3><p><img src="https://i.loli.net/2020/05/05/tXHW4NKBMneArUw.png" srcset="/img/loading.gif" style="zoom:120%;" /></p><p>目标函数：</p><script type="math/tex; mode=display">V\left(D,G\right)=\mathbb{E}_{\mathbf{x}\sim p_{data}\left(\mathbf{x}\right)}\left[\log D\left(\mathbf{x}\right)\right]+\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}\left(\mathbf{z}\right)}\left[\log\left(1-D\left(G\left(\mathbf{z}\right)\right)\right)\right]</script><p>其中，$\mathbf{x}$为服从分布$p_{data}\left(\mathbf{x}\right)$的样本数据，$\mathbf{z}$为服从先验分布$p_\mathbf{z}\left(\mathbf{z}\right)$的输入随机噪声变量，生成器$G\left(\cdot\right)$为多层感知机表示的输入随机噪声变量到数据空间的映射函数，$G\left(\mathbf{z}\right)\sim p_G$服从从样本数据$\mathbf{x}$学习到的分布$p_G$，判别器$D\left(\cdot\right)$为多层感知机表示的区分数据分布$p_{data}$还是生成器分布$p_G$的判别函数。</p><p>优化问题：</p><script type="math/tex; mode=display">\min_G \max_D V\left(D,G\right)</script><p>给定任意生成器$G$，判别器$D$的训练目标是最大化目标函数$V\left(G,D\right)$</p><script type="math/tex; mode=display">\begin{aligned} V(G, D) &=\int_{\mathbf{x}} p_{data}(\mathbf{x}) \log (D(\mathbf{x})) d \mathbf{x}+\int_{\mathbf{z}} p_{\mathbf{z}}(\mathbf{z}) \log (1-D(G(\mathbf{z}))) d \mathbf{z} \\ &=\int_{\mathbf{x}} p_{data}(\mathbf{x}) \log (D(\mathbf{x}))+p_{G}(\mathbf{x}) \log (1-D(\mathbf{x})) d \mathbf{x} \end{aligned}</script><p>令</p><script type="math/tex; mode=display">\begin{align}\frac{\partial}{\partial D\left(\mathbf{x}\right)}\int_{\mathbf{x}}\left(p_{data}(\mathbf{x}) \log (D(\mathbf{x}))+p_{G}(\mathbf{x}) \log (1-D(\mathbf{x}))\right)d \mathbf{x}=\frac{p_{data}\left(\mathbf{x}\right)}{D\left(\mathbf{x}\right)}-\frac{p_G\left(\mathbf{x}\right)}{1-D\left(\mathbf{x}\right)}=0\end{align}</script><p>得最优判别器</p><script type="math/tex; mode=display">D^{*}_G\left(\mathbf{x}\right)=\frac{p_{data}\left(\mathbf{x}\right)}{p_{data}\left(\mathbf{x}\right)+p_G\left(\mathbf{x}\right)}</script><p>将最优判别器$D_G^{*}\left(\mathbf{x}\right)$代入，得目标函数</p><script type="math/tex; mode=display">\begin{aligned} C(G) &=\max _{D} V(G, D)=V\left(G,D_G^*\right) \\ &=\mathbb{E}_{\mathbf{x} \sim p_{data }}\left[\log D_{G}^{*}(\mathbf{x})\right]+\mathbb{E}_{\mathbf{z} \sim p_{\mathbf{z}}}\left[\log \left(1-D_{G}^{*}(G(\mathbf{z}))\right)\right] \\ &=\mathbb{E}_{\mathbf{x} \sim p_{data}}\left[\log D_{G}^{*}(\mathbf{x})\right]+\mathbb{E}_{\mathbf{x} \sim p_{G}}\left[\log \left(1-D_{G}^{*}(\mathbf{x})\right)\right] \\ &=\mathbb{E}_{\mathbf{x} \sim p_{data }}\left[\log \frac{p_{data }(\mathbf{x})}{P_{data }(\mathbf{x})+p_{G}(\mathbf{x})}\right]+\mathbb{E}_{\mathbf{x} \sim p_{G}}\left[\log \frac{p_{G}(\mathbf{x})}{p_{data}(\mathbf{x})+p_{G}(\mathbf{x})}\right] \\&=-\log2+\int_{\mathbf{x}} p_{data}(\mathbf{x}) \log \left(\frac{p_{data}\left(\mathbf{x}\right)}{\left(p_{data}\left(\mathbf{x}\right)+p_G\left(\mathbf{x}\right)\right)/2}\right) d \mathbf{x} -\log2+\int_{\mathbf{x}}p_{G}(\mathbf{x}) \log \left(\frac{p_{G}\left(\mathbf{x}\right)}{\left(p_{data}\left(\mathbf{x}\right)+p_G\left(\mathbf{x}\right)\right)/2}\right) d \mathbf{x} \\&=-2\log2+KL\left(p_{data}\Big|\Big|\frac{p_{darta}+p_G}{2}\right)+KL\left(p_G\Big|\Big|\frac{p_{darta}+p_G}{2}\right) \\&=-2\log2+2\cdot JSD\left(p_{data}\|p_G\right)\end{aligned}</script><p>优化问题：</p><script type="math/tex; mode=display">\min_G C\left(G\right)</script><p>由于$p_{data}$和$p_G$两个分布的JS散度是非负的，当且仅当$p_{data}=p_G$时JS散度等于$0$，目标函数$C\left(G\right)$的最小值为$C^{*}=-2\log2$，即生成器完整的复现了数据的生成过程。</p><h3 id="小批量随机梯度下降训练算法"><a href="#小批量随机梯度下降训练算法" class="headerlink" title="小批量随机梯度下降训练算法"></a>小批量随机梯度下降训练算法</h3><p>生成对抗网络的小批量随机梯度下降训练算法：<br>输入：数据分布$p_{data}\left(\mathbf{x}\right)$，输入噪声先验分布$p_\mathbf{z}\left(\mathbf{z}\right)$，训练轮次$n$，判别器训练步数$k$，小批量$m$；<br>输出：判别器$D\left(\mathbf{x};\theta_d\right)$和生成器$G\left(\mathbf{z};\theta_g\right)$  </p><ol><li><p>for 训练轮次$n$ do<br>1.1 for 判别器训练步数$k$ do<br>   1.1.1 从输入噪声分布$p_\mathbf{z}\left(\mathbf{z}\right)$生成$m$个小样本噪声样本$\left\{\mathbf{z}^{\left(1\right)},\cdots,\mathbf{z}^{\left(m\right)}\right\}$<br>   1.1.2 从数据分布$p_{data}\left(\mathbf{z}\right)$生成$m$个小样本数据$\left\{\mathbf{x}^{\left(1\right)},\cdots,\mathbf{x}^{\left(m\right)}\right\}$<br>   1.1.3 通过增加随机梯度更新判别器 </p><script type="math/tex; mode=display">\nabla_{\theta_d}\frac{1}{m}\sum_{i=1}^m\left[\log D\left(\mathbf{x}^{\left(i\right)}\right)+\log\left(1-D\left(G\left(\mathbf{z}^{\left(i\right)}\right)\right)\right)\right]</script><p>1.2 end for<br>1.3  从输入噪声分布$p_\mathbf{z}\left(\mathbf{z}\right)$生成$m$个小样本噪声样本$\left\{\mathbf{z}^{\left(1\right)},\cdots,\mathbf{z}^{\left(m\right)}\right\}$<br>1.4 通过降低随机梯度更新生成器 </p><script type="math/tex; mode=display">\nabla_{\theta_g}\frac{1}{m}\sum_{i=1}^m\log\left(1-D\left(G\left(\mathbf{z}^{\left(i\right)}\right)\right)\right)</script></li><li><p>end for</p></li></ol><h2 id="2-深度卷积生成对抗网络"><a href="#2-深度卷积生成对抗网络" class="headerlink" title="2 深度卷积生成对抗网络"></a>2 深度卷积生成对抗网络</h2><h3 id="模型架构特点"><a href="#模型架构特点" class="headerlink" title="模型架构特点"></a>模型架构特点</h3><ol><li>判别器中使用跨步卷积（strided convolutions）替换池化层；生成器中使用微步卷积（fractional-strided convolutions）替换池化层；</li><li>判别器和生成器都使用批归一化；</li><li>判别器和生成器都移除全连接隐层；</li><li>生成器中的所有层都使用ReLU激活函数，但输出层除外，后者使用Tanh激活函数；</li><li>判别器中的所有层都使用LeakyReLU激活函数。</li></ol><p><img src="https://i.loli.net/2020/05/05/E3NARGH1nmebVwQ.png" srcset="/img/loading.gif" style="zoom:120%;" /></p><h2 id="3-生成对抗网络的改进及发展"><a href="#3-生成对抗网络的改进及发展" class="headerlink" title="3. 生成对抗网络的改进及发展"></a>3. 生成对抗网络的改进及发展</h2><h3 id="对数据分布-p-data-和生成分布-p-G-加入噪声"><a href="#对数据分布-p-data-和生成分布-p-G-加入噪声" class="headerlink" title="对数据分布$p_{data}$和生成分布$p_G$加入噪声"></a>对数据分布$p_{data}$和生成分布$p_G$加入噪声</h3><p>当数据分布$p_{datea}$与生成分布$p_G$的支撑集是高维空间中的低维流形时，$p_{data}$与$p_G$重叠部分测度为$0$的概率为$1$，即分布$p_{data}$与分布$p_G$没有重叠或重叠非常少。而当分布$p_{data}$与分布$p_G$没有重叠或重叠非常少时，JS散度为常量，很难衡量两个分布的距离。使得生成器$G\left(\mathbf{z}\right)$的梯度为$0$，造成梯度消失。</p><p>目标函数：</p><script type="math/tex; mode=display">\begin{aligned}C(G) &=\mathbb{E}_{\mathbf{x} \sim p_{data+\epsilon}}\left[\log D_{G}^{*}(\mathbf{x})\right]+\mathbb{E}_{\mathbf{x} \sim p_{G+\epsilon}}\left[\log \left(1-D_{G}^{*}(\mathbf{x})\right)\right] \\&=-2\log2+2\cdot JSD\left(p_{data+\epsilon}\|p_{G+\epsilon}\right)\end{aligned}</script><p>其中，$p_{data+\epsilon}$为加入噪声后的数据分布，$p_{G+\epsilon}$为加入噪声后的生成分布。</p><h3 id="使用Wasserstein距离进行分布度量"><a href="#使用Wasserstein距离进行分布度量" class="headerlink" title="使用Wasserstein距离进行分布度量"></a>使用Wasserstein距离进行分布度量</h3><p>Wasserstein距离（Earth-Mover距离）：</p><script type="math/tex; mode=display">\begin{align}W\left(p_{data},p_G\right)&=\inf_{\gamma\left(\mathbf{x},\mathbf{y}\right)\in\Gamma\left(p_{data},p_G\right)}\mathbb{E}_{\left(\mathbf{x},\mathbf{y}\right)\sim\gamma\left(\mathbf{x},\mathbf{y}\right)}\left[d\left(\mathbf{x},\mathbf{y}\right)\right] \\&=\inf_{\gamma\left(\mathbf{x},\mathbf{y}\right)\in\Gamma\left(p_{data},p_G\right)}\sum_{\left(\mathbf{x},\mathbf{y}\right)}\gamma\left(\mathbf{x},\mathbf{y}\right)d\left(\mathbf{x},\mathbf{y}\right)\end{align}</script><p>其中，$\Gamma\left(p_{data},p_G\right)$为分布$p_{data}$与$p_G$的所有可能的联合分布集合，$d\left(\mathbf{x},\mathbf{y}\right)$为$\mathbf{x}$和$\mathbf{y}$之间的欧氏距离。</p><p><img src="https://i.loli.net/2020/05/05/rEVLYf8QUHNxIO9.png" srcset="/img/loading.gif" alt="wd" style="zoom:150%;" /></p><p>由于Wasserstein距离可表示为</p><script type="math/tex; mode=display">W\left(p_{data},p_G\right)=\frac{1}{K}\sup_{\|f\|_L\leq K}\mathbb{E}_{\mathbf{x}\sim p_{data}}\left[f\left(\mathbf{x}\right)\right]-\mathbb{E}_{\mathbf{x}\sim p_G}\left[f\left(\mathbf{x}\right)\right]</script><p>其中，$|f|_L$为函数$f$的Lipschitz常数。</p><p>所以可得</p><script type="math/tex; mode=display">K\cdot W\left(p_{data},p_G\right)\approx\max_{w;\|f_w\|_L\leq K}\mathbb{E}_{\mathbf{x}\sim p_{data}}\left[f_w\left(\mathbf{x}\right)\right]-\mathbb{E}_{\mathbf{x}\sim p_G}\left[f_w\left(\mathbf{x}\right)\right]</script><p>其中，$f_w$为由参数$w$表示的函数。</p><p>WGAN生成器损失：</p><script type="math/tex; mode=display">-\mathbb{E}_{\mathbf{x}\sim p_G}\left[f_w\left(\mathbf{x}\right)\right]</script><p>WGAN判别器损失：</p><script type="math/tex; mode=display">\mathbb{E}_{\mathbf{x}\sim p_{data}}\left[f_w\left(\mathbf{x}\right)\right]-\mathbb{E}_{\mathbf{x}\sim p_G}\left[f_w\left(\mathbf{x}\right)\right]</script><h2 id="4参考资料："><a href="#4参考资料：" class="headerlink" title="4参考资料："></a>4参考资料：</h2><p>《Generative Adversarial Nets》</p><p>《UNSUPERVISED REPRESENTATION LEARNING WITH DEEP CONVOLUTIONAL GENERATIVE ADVERSARIAL NETWORKS》</p><p>《Wasserstein GAN》 </p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>GAN</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transfer_Learning</title>
    <link href="/2020/05/05/Transfer-Learning/"/>
    <url>/2020/05/05/Transfer-Learning/</url>
    
    <content type="html"><![CDATA[<h1 id="迁移学习（Transfer-Learning）"><a href="#迁移学习（Transfer-Learning）" class="headerlink" title="迁移学习（Transfer Learning）"></a>迁移学习（Transfer Learning）</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><h2 id="1-迁移学习的形式化定义"><a href="#1-迁移学习的形式化定义" class="headerlink" title="1 迁移学习的形式化定义"></a>1 迁移学习的形式化定义</h2><p>领域$\mathcal{D}$：数据及数据的概率分布<br>源领域$\mathcal{D}_s=\{\left(\mathbf{x}_i,y_i\right)|P_s\left(\mathbf{x}_i\right)\}_{i=1}^n$<br>目标领域$\mathcal{D}_t=\{\mathbf{x}_j|P_t\left(\mathbf{x}_j\right)\}_{j=n+1}^{n+m}$</p><p>迁移学习（Transfer Learning）：给定有标记源领域$\mathcal{D}_s=\{\left(\mathbf{x}_i,y_i\right)|P_s\left(\mathbf{x}_i\right)\}_{i=1}^n$和无标记目标领域$\mathcal{D}_t=\{\mathbf{x}_j|P_t\left(\mathbf{x}_j\right)\}_{j=n+1}^{n+m}$，其中$P_s\left(\mathbf{x}_i\right)\neq P_t\left(\mathbf{x}_j\right)$。迁移学习的目的是借助源领域$\mathcal{D}_s$的知识，来学习目标领域$\mathcal{D}_t$的知识（标签）。</p><p>领域自适应（Domain Adaptation）：给定有标记源领域$\mathcal{D}_s=\{\left(\mathbf{x}_i,y_i\right)|P_s\left(\mathbf{x}_i\right)\}_{i=1}^n$和无标记目标领域$\mathcal{D}_t=\{\mathbf{x}_j|P_t\left(\mathbf{x}_j\right)\}_{j=n+1}^{n+m}$，<br>其中，$\mathbf{x}_i\in\mathcal{X}_s,\mathbf{x}_j\in\mathcal{X}_t$，且$\mathcal{X}_s=\mathcal{X}_t,P_s\left(\mathbf{x}_i\right)\neq P\left(\mathbf{x}_j\right)$；$y_i\in\mathcal{Y}_s,y_j\in\mathcal{Y}_t$，且$\mathcal{Y}_s=\mathcal{Y}_t,Q_s\left(y_i|\mathbf{x}_i\right)= Q\left(y_j|\mathbf{x}_j\right)$。<br>迁移学习的目的是，利用有标记源领域数据$\mathcal{D}_s$学习一个分类器$f:\mathbf{x}_s\to y_s$来预测目标领域数据$\mathcal{D}_t$的标签$y_t\in\mathcal{Y}_t$。</p><p>度量是描述源领域$\mathcal{D}_s$和目标领域$\mathcal{D}_t$之间的距离。  </p><script type="math/tex; mode=display">DISTANCE\left(\mathcal{D}_s,\mathcal{D}_t\right)=DistanceMeasure\left(\cdot,\cdot\right)</script><p>最大均值差异（Maximum mean discrepancy，MMD），度量再生希尔伯特空间中两个分布的距离，是一种核学习方法。</p><script type="math/tex; mode=display">MMD^2\left(X,Y\right)=\bigg{\|}\sum_{i=1}^{n1}\phi\left(\mathbf{x}_i\right)-\sum_{j=1}^{n2}\phi\left(\mathbf{y}_j\right)\bigg{\|}_\mathcal{H}^2</script><p>其中，$\phi\left(\cdot\right)$是将数据映射到满足$\left<K\left(x,\cdot\right),K\left(\cdot,y\right)\right>_\mathcal{H}=K\left(x,y\right)$条件的希尔伯特空间的映射。</p><h2 id="2-迁移学习方法"><a href="#2-迁移学习方法" class="headerlink" title="2 迁移学习方法"></a>2 迁移学习方法</h2><h3 id="边缘分布自适应"><a href="#边缘分布自适应" class="headerlink" title="边缘分布自适应"></a>边缘分布自适应</h3><p>边缘分布自适应的目标是减少源领域$\mathcal{D}_s$和目标领域$\mathcal{D}_t$之间的边缘概率分布的距离，从而完成迁移学习。<br>边缘分布自适应方法使用$P_s\left(\mathbf{x}\right)$和$P_t\left(\mathbf{x}\right)$之间的距离来近似两个领域之间的差异：  </p><script type="math/tex; mode=display">DISTANCE\left(\mathcal{D}_s,\mathcal{D}_t\right)\approx\|P_s\left(\mathbf{x}\right)-P_t\left(\mathbf{x}\right)\|</script><p>迁移成分分析（Transfer Component Aualysis，TCA）<br>由于$P_s\left(\mathbf{x}\right)\neq P_t\left(\mathbf{x}\right)$，假设存在映射$\phi\left(\cdot\right)$，使得$P_s\left(\phi(\left(\mathbf{x}\right)\right)\approx P_t\left(\phi\left(\mathbf{x}\right)\right)$，则$P\left(y_s|\phi\left(\mathbf{x}_s\right)\right)\approx P\left(y_t|\phi\left(\mathbf{x}_t\right)\right)$。</p><script type="math/tex; mode=display">\phi^*=\mathop{\arg\min}_{\phi} DISTANCE\left(\mathcal{D}_{s,\phi},\mathcal{D}_{t,\phi}\right)</script><p>迁移成分分析中，源领域与目标领域之间的距离度量</p><script type="math/tex; mode=display">DISTANCD\left(\mathcal{D}_{s,\phi},\mathcal{D}_{t,\phi}\right)=\bigg{\|}\frac{1}{n_1}\sum_{i=1}^{n_1}\phi\left(\mathbf{x}_i\right)-\frac{1}{n_2}\sum_{j=1}^{n_2}\phi\left(\mathbf{x}_j\right)\bigg{\|}_\mathcal{H}</script><p>其中，$n_1,n_2$分别为源领域与目标领域的样本个数。</p><p>引入核矩阵</p><script type="math/tex; mode=display">K=\begin{bmatrix} K_{s,s} & K_{s,t} \\ K_{t,s} & K_{t,t} \end{bmatrix}</script><p>其中，$K_{\cdot,\cdot}=K\left(\mathbf{x}_\cdot,\mathbf{x}_\cdot\right)$<br>系数矩阵$L=\left(l_{ij}\right)$<br>其中，</p><script type="math/tex; mode=display">l_{ij}=\left\{\begin{align}\frac{1}{n_1^2} & \quad \mathbf{x}_i,\mathbf{x}_j\in\mathcal{D}_s \\\frac{1}{n_2^2} & \quad \mathbf{x}_i,\mathbf{x}_j\in\mathcal{D}_t \\-\frac{1}{n_1 n_2} &  otherwise\end{align}\right.</script><p>则</p><script type="math/tex; mode=display">K^*=\mathop{\arg\min}_K \mathop{tr}\left(KL\right)-\lambda\mathop{tr}\left(K\right)</script><p>其中，$\mathop{tr}\left(\cdot\right)$为矩阵的迹</p><h3 id="条件分布自适应"><a href="#条件分布自适应" class="headerlink" title="条件分布自适应"></a>条件分布自适应</h3><p>条件分布自适应的目标是减少源领域$\mathcal{D}_s$和目标领域$\mathcal{D}_t$之间的条件概率分布的距离，从而完成迁移学习。<br>条件分布自适应方法使用$Q_s\left(y_s|\mathbf{x}_s\right)$和$Q_t\left(y_t|\mathbf{x}_t\right)$之间的距离来近似两个领域之间的差异：  </p><script type="math/tex; mode=display">DISTANCE\left(\mathcal{D}_s,\mathcal{D}_t\right)\approx\|Q_s\left(y_s|\mathbf{x}_s\right)-Q_t\left(y_t|\mathbf{x}_t\right)\|</script><h3 id="联合分布自适应"><a href="#联合分布自适应" class="headerlink" title="联合分布自适应"></a>联合分布自适应</h3><p>联合分布自适应的目标是减少源领域$\mathcal{D}_s$和目标领域$\mathcal{D}_t$之间的联合概率分布的距离，从而完成迁移学习。<br>联合分布自适应方法使用$P_s\left(\mathbf{x}\right)$和$P_t\left(\mathbf{x}\right)$之间的距离以及$Q_s\left(y_s|\mathbf{x}_s\right)$和$Q_t\left(y_t|\mathbf{x}_t\right)$之间的距离来近似两个领域之间的差异：  </p><script type="math/tex; mode=display">DISTANCE\left(\mathcal{D}_s,\mathcal{D}_t\right)\approx\left(1-\mu\right)\|P_s\left(\mathbf{x}\right)-P_t\left(\mathbf{x}\right)\|+\mu\|Q_s\left(y_s|\mathbf{x}_s\right)-Q_t\left(y_t|\mathbf{x}_t\right)\|</script><h2 id="3-深度迁移学习"><a href="#3-深度迁移学习" class="headerlink" title="3 深度迁移学习"></a>3 深度迁移学习</h2><p>损失函数</p><script type="math/tex; mode=display">l=l_c\left(\mathcal{D}_s,y_s\right)+\lambda l_A\left(\mathcal{D}_s,\mathcal{D}_t\right)</script><p>其中，$l_c\left(\mathcal{D}_s,y_s\right)$为网络模型在源领域的损失，$l_A\left(\mathcal{D}_s,\mathcal{D}_t\right)$为网络模型的自适应损失。</p><h3 id="深度领域混淆（Deep-Domain-Confusion，DDC）"><a href="#深度领域混淆（Deep-Domain-Confusion，DDC）" class="headerlink" title="深度领域混淆（Deep Domain Confusion，DDC）"></a>深度领域混淆（Deep Domain Confusion，DDC）</h3><p>损失函数</p><script type="math/tex; mode=display">l=l_c\left(\mathcal{D}_s,y_s\right)+\lambda MMD^2\left(\mathcal{D}_s,\mathcal{D}_t\right)</script><p>其中，$MMD$为最大值均方差异。<br><img src="https://i.loli.net/2020/05/05/29luQad7riv3hwT.png" srcset="/img/loading.gif"  align=center  width = "400" height = "400" /></p><h3 id="深度适应网络（Deep-Adaptation-Networks，DAN）"><a href="#深度适应网络（Deep-Adaptation-Networks，DAN）" class="headerlink" title="深度适应网络（Deep Adaptation Networks，DAN）"></a>深度适应网络（Deep Adaptation Networks，DAN）</h3><p>多核MMD</p><script type="math/tex; mode=display">\mathcal{K}\triangleq\{k=\sum_{u=1}^m\beta_u k_u,\beta_u\geq 0\}</script><p>优化目标</p><script type="math/tex; mode=display">\min_{\Theta}\frac{1}{n_a}J\left(\theta\left(\mathbf{x}_i^a\right),y_i^a\right)+\lambda\sum_{l=l_1}^{l_2}   d_k^2 \left(\mathcal{D}_s^l,\mathcal{D}_t^l \right)</script><p><img src="https://i.loli.net/2020/05/05/7xBRfp6giAz53EF.png" srcset="/img/loading.gif"  align=center  width = "600" height = "200" /></p><h3 id="深度对抗网络迁移"><a href="#深度对抗网络迁移" class="headerlink" title="深度对抗网络迁移"></a>深度对抗网络迁移</h3><h4 id="生成对抗网络GAN"><a href="#生成对抗网络GAN" class="headerlink" title="生成对抗网络GAN"></a>生成对抗网络GAN</h4><p><img src="https://i.loli.net/2020/05/05/tXHW4NKBMneArUw.png" srcset="/img/loading.gif"  align=center  width = "500" height = "250" /></p><p>目标函数：  </p><script type="math/tex; mode=display">V\left(D,G\right)=\mathbb{E}_{\mathbf{x}\sim p_{data}\left(\mathbf{x}\right)}\left[\log D\left(\mathbf{x}\right)\right]+\mathbb{E}_{\mathbf{z}\sim p_\mathbf{z}\left(\mathbf{z}\right)}\left[\log\left(1-D\left(G\left(\mathbf{z}\right)\right)\right)\right]</script><p>其中，$\mathbf{x}$为服从分布$p_{data}\left(\mathbf{x}\right)$的样本数据，$\mathbf{z}$为服从先验分布$p_\mathbf{z}\left(\mathbf{z}\right)$的输入随机噪声变量，生成器$G\left(\cdot\right)$为多层感知机表示的输入随机噪声变量到数据空间的映射函数，$G\left(\mathbf{z}\right)\sim p_G$服从从样本数据$\mathbf{x}$学习到的分布$p_G$，判别器$D\left(\cdot\right)$为多层感知机表示的区分数据分布$p_{data}$还是生成器分布$p_G$的判别函数。</p><p>优化问题：</p><script type="math/tex; mode=display">\min_G \max_D V\left(D,G\right)</script><h4 id="领域对抗神经网络（Domain-Adversarial-Neural-Network，DANN）"><a href="#领域对抗神经网络（Domain-Adversarial-Neural-Network，DANN）" class="headerlink" title="领域对抗神经网络（Domain-Adversarial Neural Network，DANN）"></a>领域对抗神经网络（Domain-Adversarial Neural Network，DANN）</h4><p>生成器进行源领域与目标领域特征提取，使得判别器无法对两个领域的差异进行判别。</p><p>领域对抗损失函数</p><script type="math/tex; mode=display">l_d=\frac{1}{n} \sum_{i=1}^{n} \mathcal{L}_{d}^{i}(\mathbf{W}, \mathbf{b}, \mathbf{u}, z)+\frac{1}{n^{\prime}} \sum_{i=n+1}^{N} \mathcal{L}_{d}^{i}(\mathbf{W}, \mathbf{b}, \mathbf{u}, z)</script><p>其中$\mathcal{L}_d$表示为</p><script type="math/tex; mode=display">\mathcal{L}_{d}\left(G_{d}\left(G_{f}\left(\mathbf{x}_{i}\right)\right), d_{i}\right)=d_{i} \log \frac{1}{G_{d}\left(G_{f}\left(\mathbf{x}_{i}\right)\right)}+\left(1-d_{i}\right) \log \frac{1}{1-G_{d}\left(G_{f}\left(\mathbf{x}_{i}\right)\right)}</script><h3 id="微调网络-finetune"><a href="#微调网络-finetune" class="headerlink" title="微调网络 finetune"></a>微调网络 finetune</h3><p><img src="https://i.loli.net/2020/05/05/GVsE7ubUQFoDR2T.png" srcset="/img/loading.gif"  align=center  width = "400" height = "400" /></p><h2 id="4-参考文献"><a href="#4-参考文献" class="headerlink" title="4 参考文献"></a>4 参考文献</h2><p>Li, Y., Wang, N., Shi, J., Hou, X., and Liu, J. (2018). Adaptive batch normalization for practical domain adaptation. Pattern Recognition, 80:109–117.<br>Long, M., Cao, Y., Wang, J., and Jordan, M. (2015a). Learning transferable features with deep adaptation networks. In ICML, pages 97–105.<br>Tzeng, E., Hoffman, J., Darrell, T., and Saenko, K. (2015). Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE International Conference on Computer Vision, pages 4068–4076, Santiago, Chile. IEEE.<br>Tzeng, E., Hoffman, J., Saenko, K., and Darrell, T. (2017). Adversarial discriminative domain adaptation. In CVPR, pages 2962–2971.<br>Tzeng, E., Hoffman, J., Zhang, N., et al. (2014). Deep domain confusion: Maximizing for domain invariance. arXiv preprint arXiv:1412.3474.<br>Bousmalis, K., Trigeorgis, G., Silberman, N., Krishnan, D., and Erhan, D. (2016). Domain separation networks. In Advances in Neural Information Processing Systems, pages 343–351.<br>Shen, J., Qu, Y., Zhang, W., and Yu, Y. (2018). Wasserstein distance guided representation learning for domain adaptation. In AAAI.  </p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>domain adaptation</tag>
      
      <tag>finetune</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nerual_Probabilistic_Language_Model</title>
    <link href="/2020/05/05/Nerual-Probabilistic-Language-Model/"/>
    <url>/2020/05/05/Nerual-Probabilistic-Language-Model/</url>
    
    <content type="html"><![CDATA[<h1 id="A-Neural-Probabilistic-Language-Model"><a href="#A-Neural-Probabilistic-Language-Model" class="headerlink" title="A Neural Probabilistic Language Model"></a>A Neural Probabilistic Language Model</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><h2 id="1-统计语言模型与n-gram模型"><a href="#1-统计语言模型与n-gram模型" class="headerlink" title="1 统计语言模型与n-gram模型"></a>1 统计语言模型与n-gram模型</h2><p>统计语言模型可描述为给定前序词序列后，下一单词出现的条件概率的乘积：</p><script type="math/tex; mode=display">\hat P\left(w_1^T\right)=\prod_{t=1}^T\hat P\left(w_t|w_1^{t-1}\right)</script><p>其中，$w_t$是第$t$个单词，$w_i^j=\left(w_i,w_{i+1},\dots,w_{j-1},w_j\right)$是从第$i$个单词到第$j$个单词的子序列。</p><p>n-gram模型可描述为给定前$n-1$个单词后，第$n$个单词出现的条件概率：</p><script type="math/tex; mode=display">\hat P\left(w_t|w_1^{t-1}\right)\approx P\left(w_t|w_{t-n+1}^{t-1}\right)</script><h2 id="2-神经网络语言模型"><a href="#2-神经网络语言模型" class="headerlink" title="2 神经网络语言模型"></a>2 神经网络语言模型</h2><p><img src="https://i.loli.net/2020/05/05/Zud3VClSxYbsK49.png" srcset="/img/loading.gif"  align=center  width = "450" height = "350" /></p><p>训练集$w_1,w_2,\dots,w_T$是单词序列。单词表$V$是由单词组成的规模很大但是有限的集合，$w_t\in V$，$|V|$是单词表$V$中单词个数。</p><p>学习的目标</p><script type="math/tex; mode=display">f\left(w_t,w_{t-1},\dots,w_{t-n+2},w_{t-n+1}\right)=\hat P\left(w_t|w_1^{t-1}\right)</script><p>需要满足的约束条件：</p><ul><li>$f\left(w_t,w_{t-1},\dots,w_{t-n+2},w_{t-n+1}\right)&gt;0$</li><li>$\sum_{i=1}^{|V|}f\left(i,w_{t-1},\dots,w_{t-n+2},w_{t-n+1}\right)=1$</li></ul><p>将函数$f\left(w_t,w_{t-1},\dots,w_{t-n+2},w_{t-n+1}\right)=\hat P\left(w_t|w_1^{t-1}\right)$分解为两部分：</p><ol><li>从单词表$V$中任意元素$i$到实向量$C\left(i\right)\in\mathbb{R}^m$的映射$C$。映射$C$表示单词表中每个单词的分布式特征向量。在实践中，映射$C$表示为一个$|V|\times m$的自由参数矩阵；</li><li>使用映射$C$表达的每个单词的概率函数$g$。概率函数$g$是从输入词序列的单词上下文特征向量$\left(C\left(w_{t-n+1}\right),\dots,C\left(w_{t-1}\right)\right)$，到下一单词$w_t$的条件概率分布的映射。概率函数$g$的输出是一个向量，向量中第$i$个元素是概率$\hat P\left(w_t=i|w_1^{t-1}\right)$<script type="math/tex; mode=display">f\left(i,w_{t-1},\dots,w_{t-n+1}\right)=g\left(i, C\left(w_{t-1}\right),\dots,C\left(w_{t-n+1}\right)\right)</script></li></ol><p>函数$f$是两个映射$C$和$g$的组合。这两个映射都与各自的参数关联。映射$C$的参数是特征向量本身，被表示为一个$|V|\times m$的矩阵，矩阵的第$i$行是单词$i$的特征向量。函数$g$可以被一个基于参数集$\omega$的前馈神经网络或者卷积神经网络实现或其他参数化函数实现。则整体参数集合是$\theta=\left(C,\omega\right)$。</p><p>训练是通过寻找能够使最大化语料库的带罚项对数似然的$\theta$实现的:</p><script type="math/tex; mode=display">L\left(\theta\right)=\frac{1}{T}\sum_t\log f\left(w_t,w_{t-1},\dots,w_{t-n+1};\theta\right)+R\left(\theta\right)  \\  \theta^*=\mathop{\arg\max}_\theta L\left(\theta\right)</script><p>其中，$R\left(\theta\right)$是正则化项，是权重的惩罚。</p><p>神经网络在单词特征映射之后有一个隐藏层，并且可以选择将单词特征直接连接到输出层。因此，实际上有两个隐藏层，共享的单词特征映射层$C$和普通的双曲正切隐藏层。神经网络输出层采用$softmax$输出层，</p><script type="math/tex; mode=display">\hat P\left(w_t|w_{t-1},\dots,w_{t-n+1}\right)=\frac{e^{y_{w_t}}}{\sum_i e^{y_i}}</script><p>其中，$y_i$是每个输出单词$i$的未归一化的$\log$概率。</p><p>使用参数$b,W,U,d,H$计算每个输出单词$i$的未归一化的$\log$概率</p><script type="math/tex; mode=display">y=b+Wx+U\tanh\left(d+Hx\right)</script><p>其中，$x$是单词特征层激活向量，由矩阵C中的输入单词特征串联组成</p><script type="math/tex; mode=display">x=\left(C\left(w_{t-1}\right),C\left(w_{t-2}\right),\dots,C\left(w_{t-n+1}\right)\right)</script><p>令$h$为隐藏单元个数，$m$为每个单词的特征维数。当没有从单词特征到输出的直接连接时，矩阵$W$设置为$\mathbf{0}$。模型自由参数包括:输出偏置向量$b\in\mathbb{R}^{|V|}$，隐藏层偏置向量$d\in\mathbb{R}^h$，隐藏层到输出层权值矩阵$U\in\mathbb{R}^{|V|\times h}$，单词特征输出权值矩阵$W\in\mathbb{R}^{|V|\times\left(n-1\right)\cdot m}$，隐藏层权值矩阵$H\in\mathbb{R}^{h\times\left(n-1\right)\cdot m}$，单词特征矩阵$C\in\mathbb{R}^{|V|\times m}$</p><script type="math/tex; mode=display">\theta=\left(b,d,W,U,H,C\right)</script><p>自由参数共有$h\left(1+\left(n-1\right)m\right)+|V|\left(1+nm+h\right)$个。</p><p>神经网络上的随机梯度提升是指在训练语料库的第$t$个单词后进行以下迭代更新</p><script type="math/tex; mode=display">\theta=\theta+\varepsilon\frac{\partial\log\hat{P}\left(w_t|w_{t-1},\dots,w_{t-n+1}\right)}{\partial\theta}</script><p>其中，$\varepsilon$是学习率。</p><p>第$i$个处理器，第$t$个样本的计算：</p><ol><li><p>前向计算<br>（a）单词特征层执行前向计算：</p><script type="math/tex; mode=display">x\left(k\right)\leftarrow C\left(w_{t-k}\right),k=1,2,\dots,n-1  \\x=\left(x\left(1\right),x\left(2\right),\dots,x\left(n-1\right)\right)</script><p>（b）隐藏层执行前向计算：</p><script type="math/tex; mode=display">o\leftarrow d+Hx \\a\leftarrow\tanh\left(o\right)</script><p>（c）在第$i$个块内的输出结点执行前向计算：</p><script type="math/tex; mode=display">s_i\leftarrow 0</script><p>$\quad\qquad在第i个块中使用下标j进行循环$</p><script type="math/tex; mode=display">y_j\leftarrow b_j+aU_j</script><p>$\quad\qquad如果有直接连接，y_j\leftarrow y_j+xW_j$</p><script type="math/tex; mode=display">p_j\leftarrow e^{y_j}  \\s_i\leftarrow s_i+p_j</script><p>（d）在所有处理器中计算并分享$S=\sum_i s_i$<br>（e）概率规范化<br>$\quad\qquad在第i个块中使用下标j进行循环$</p><script type="math/tex; mode=display">p_j\leftarrow p_j/S</script><p>（f）更新$\log$似然函数。</p></li><li><p>后向计算与参数更新（学习率$\varepsilon$）<br>（a）第$i$个块内的输出结点进行后向梯度计算：<br>$\quad\qquad清空梯度向量\frac{\partial L}{\partial a}和\frac{\partial L}{\partial x}$<br>$\quad\qquad在第i个块中使用下标j进行循环$</p><script type="math/tex; mode=display">\frac{\partial L}{\partial y_j}\leftarrow 1_{j==w_t}-p_j  \\b_j\leftarrow b_j+\varepsilon\frac{\partial L}{\partial y_j}</script><p>$\quad\qquad如果有直接连接，\frac{\partial L}{\partial x}\leftarrow\frac{\partial L}{\partial x}+\frac{\partial L}{\partial y_j}W_j$</p><script type="math/tex; mode=display">\frac{\partial L}{\partial a}\leftarrow\frac{\partial L}{\partial a}+\frac{\partial L}{\partial y_j}U_j</script><p>$\quad\qquad如果有直接连接，W_j\leftarrow W_j+\varepsilon\frac{\partial L}{\partial y_j}x$</p><script type="math/tex; mode=display">U_j\leftarrow U_j+\varepsilon\frac{\partial L}{\partial y_j}a</script><p>（b）在所有处理器中计算并分享$\frac{\partial L}{\partial x}$和$\frac{\partial L}{\partial a}$<br>（c）后向传播并更新隐藏层权值：<br>$\quad\qquad在1和h之间使用下标k进行循环$  </p><script type="math/tex; mode=display">\frac{\partial L}{\partial o_k}\leftarrow\left(1-a_k^2\right)\frac{\partial L}{\partial a_k}</script><script type="math/tex; mode=display">\frac{\partial L}{\partial x}\leftarrow\frac{\partial L}{\partial x}+H\frac{\partial L}{\partial o}  \\d\leftarrow d+\varepsilon\frac{\partial L}{\partial o}  \\H\leftarrow H+\varepsilon\frac{\partial L}{\partial o}x</script><p>（d）更新单词特征向量：<br>$\quad\qquad在1和n-1之间使用下标k进行循环$</p><script type="math/tex; mode=display">C\left(w_{t-k}\right)\leftarrow C\left(w_{t-k}\right)+\varepsilon\frac{\partial L}{\partial x\left(k\right)}</script><p>$\quad\qquad其中，\frac{\partial L}{\partial x\left(k\right)}是\frac{\partial L}{\partial x}的第k个分量$。</p></li></ol>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>language model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Feedforward_Nerual_Network</title>
    <link href="/2020/05/05/Feedforward-Nerual-Network/"/>
    <url>/2020/05/05/Feedforward-Nerual-Network/</url>
    
    <content type="html"><![CDATA[<h1 id="前馈神经网络（Feedforward-Neural-Network，FNN）"><a href="#前馈神经网络（Feedforward-Neural-Network，FNN）" class="headerlink" title="前馈神经网络（Feedforward Neural Network，FNN）"></a>前馈神经网络（Feedforward Neural Network，FNN）</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><h2 id="1-前馈神经网络结构及前向传播"><a href="#1-前馈神经网络结构及前向传播" class="headerlink" title="1 前馈神经网络结构及前向传播"></a>1 前馈神经网络结构及前向传播</h2><p>训练数据集<br>\begin{align} \\&amp; T = \left\{ \left( \mathbf{x}_{1}, y_{1} \right), \left( \mathbf{x}_{2}, y_{2} \right), \cdots, \left(\mathbf{x}_i,y_i\right),\cdots,\left( \mathbf{x}_{N}, y_{N} \right) \right\} \end{align}<br>其中，$\mathbf{x}_{i}$为第$i$个特征向量（实例），$\mathbf{x}_{i}=\left( x^{\left(1\right)}_i,x^{\left(2\right)}_i,\ldots ,x^{\left(j\right)}_i,\ldots ,x^{\left(n\right)}_i\right) ^{T} \in \mathcal{X} \subseteq \mathbb{R}^{n}$；$y_{i}$为$\mathbf{x}_{i}$的类别标记，类别标记表示为类别位置为1，其余位置为0的类别向量（one-hot编码）,$\mathbf{y}_i\in\{0,1\}^m$。</p><p>前馈神经网络输入层（层 1）  \begin{align} &amp; \mathbf{a}^{1}=\left( a_{1}^{1},a_{2}^{1},\ldots ,a_{j}^{1},\ldots ,a_{n}^{1}\right) ^{T}\\ &amp; a_{j}^{1}=x^{\left(j\right)}_i\quad\left( j=1,2,\ldots ,n\right) \end{align}</p><p>前馈神经网络隐藏层（层 2） \begin{align} &amp; \mathbf{a}^{2}=\left( a_{1}^{2},a_{2}^{2},\ldots ,a_{j}^{2},\ldots  ,a_{p}^{2}\right) ^{T}\\ &amp; a_{j}^{2}=\sigma \left( z_{j}^{2}\right) \\ &amp; z_{j}^{2}= \sum _{k}w_{jk}^{2}\cdot a_{k}^{1}+b_{j}^{2}\quad\left( j=1,2,\ldots ,p\right) \\&amp; \mathbf{z}^{2}=\left( z_{1}^{2},z_{2}^{2},\ldots ,z_{j}^{2},\ldots ,z_{p}^{2}\right) ^{T}\end{align}</p><p>前馈神经网络输出层（层 3） \begin{align} &amp; \mathbf{a}^{3}=\left( a_{1}^{3},a_{2}^{3},\ldots ,a_{j}^{3},\ldots,a_{m}^{3}\right) ^{T}\\ &amp; a_{j}^{3}=\sigma \left( z_{j}^{3}\right) \\ &amp; z_{j}^{3}= \sum _{k}w_{jk}^{3}\cdot a_{k}^{2}+b_{j}^{3}\quad\left( j=1,2,\ldots ,m\right) \\&amp; \mathbf{z}^{3}=\left( z_{1}^{3},z_{2}^{3},\ldots ,z_{j}^{3},\ldots ,z_{m}^{3}\right) ^{T}\end{align}</p><p>预测输出 \begin{align} &amp; \hat{\mathbf{y}}=\left( \hat y_{1},\hat y_{2},\ldots ,\hat y_{j},\ldots ,\hat y_{m}\right) ^{T}\\ &amp; \hat y_{j}=a_{j}^{3}\quad\left( j=1,2,\ldots ,m\right)\end{align}  </p><p>实际输出 \begin{align} &amp; \mathbf{y}=\left( y_{1},y_{2},\ldots ,y_{j},\ldots ,y_{m}\right) ^{T}  \quad\left( j=1,2,\ldots ,m\right) \end{align} </p><p>其中，$\sigma\left(\cdot\right)$为激活函数。</p><h2 id="2-参数学习"><a href="#2-参数学习" class="headerlink" title="2 参数学习"></a>2 参数学习</h2><p>单个实例$\mathbf{x}$的损失函数$C_{\mathbf{x}}\left(\mathbf{y},\hat{\mathbf{y}}\right)$为平方损失函数、</p><script type="math/tex; mode=display">\begin{align} & C_{\mathbf{x}}=\dfrac {1} {2}\left\| \mathbf{y}-\hat {\mathbf{y}}\right\| ^{2}=\dfrac {1} {2}\sum _{j}\left( y_{j}-\hat {y}_{j}\right) ^{2} \end{align}</script><p>目标函数（经验风险）  \begin{align} &amp; C=\dfrac {1} {N}\sum _{\mathbf{x}}C_{\mathbf{x}} \end{align} </p><p>第$l$层参数$w_{jk}^l$更新为  </p><script type="math/tex; mode=display">\begin{align}w_{jk}^l & \leftarrow w_{jk}^l-\alpha \dfrac {\partial C} {\partial w_{jk}^{l}}  \\&=w_{jk}^l-\alpha\frac{1}{N}\sum_{\mathbf{x}}\dfrac {\partial C_x} {\partial w_{jk}^{l}}\end{align}</script><p>第$l$层参数$b_j^l$更新为    </p><script type="math/tex; mode=display">\begin{align}b_j^l & \leftarrow b_j^l-\alpha \dfrac {\partial C} {\partial b_j^{l}}  \\&=b_j^l-\alpha\frac{1}{N}\sum_{\mathbf{x}}\dfrac {\partial C_x} {\partial b_j^{l}}\end{align}</script><h2 id="3-误差反向传播"><a href="#3-误差反向传播" class="headerlink" title="3 误差反向传播"></a>3 误差反向传播</h2><p>定义第$l$层的第$j$个神经元上的误差 \begin{align} &amp; \delta _{j}^{l}\equiv \dfrac {\partial C_{\mathbf{x}}} {\partial z_{j}^{l}} \quad\left( l=2,3\right)\end{align} </p><p>输出层误差 </p><script type="math/tex; mode=display">\begin{align} \delta _{j}^{3}&=\dfrac {\partial C_{\mathbf{x}}} {\partial z_{j}^{3}} \\ & =\dfrac {\partial C_{\mathbf{x}}} {\partial a_{j}^{3}}\cdot\dfrac {\partial a_{j}^{3}} {\partial z_{j}^{3}} \\ & =\dfrac {\partial \left(\frac{1}{2}\sum_{j=1}^m \left(y_j-\hat{y}_j\right)^2\right)} {\partial \hat{y}_j}\cdot \sigma '\left( z_{j}^{3}\right) \\& = \left(\hat{y}_j-y_j\right) \cdot \sigma'\left( z_{j}^{3} \right)\quad\left( j=1,2,\ldots ,m\right)\end{align}</script><p>隐藏层误差</p><script type="math/tex; mode=display">\begin{align} \delta _{j}^{2}&=\dfrac {\partial C_{\mathbf{x}}} {\partial z_{j}^{2}}\\ & =\sum _{k}\dfrac {\partial C_{\mathbf{x}}} {\partial z_{k}^{3}}\cdot \dfrac {\partial z_{k}^{3}} {\partial z_{j}^{2}} \\ & = \sum _{k} \dfrac {\partial z_{k}^{3}} {\partial z_{j}^{2}}\cdot\delta _{k}^{3}\\ & = \sum _{k} \dfrac {\partial \left( \sum _{j}w_{kj}^{3}\cdot a_{j}^{2}+b_{k}^{3}\right)} {\partial z_{j}^{2}}\cdot\delta _{k}^{3}\\ & = \sum _{k} \dfrac {\partial \left( \sum _{j}w_{kj}^{3}\cdot  \sigma \left( z_{j}^{2}\right)+b_{k}^{3}\right)} {\partial z_{j}^{2}}\cdot\delta _{k}^{3}\\ & = \sum _{k} w_{kj}^{3}\cdot \sigma '\left( z_{j}^{2}\right) \cdot\delta _{k}^{3} \\ & = \sigma '\left( z_{j}^{2}\right) \cdot\sum _{k} w_{kj}^{3} \delta _{k}^{3} \quad\left( j=1,2,\ldots ,p\right)\quad\left( k=1,2,\ldots ,m\right)\end{align}</script><h2 id="4-梯度下降算法"><a href="#4-梯度下降算法" class="headerlink" title="4 梯度下降算法"></a>4 梯度下降算法</h2><p>损失函数在隐藏层（层2）／输出层（层3）关于偏置的梯度 \begin{align} &amp; \dfrac {\partial C_{\mathbf{x}}} {\partial b_{j}^{l}}=\dfrac {\partial C_{\mathbf{x}}} {\partial z_{j}^{l}}\cdot \dfrac {\partial z_{j}^{l}} {\partial b_{j}^{l}}=\delta _{j}^{l}\cdot \dfrac {\partial \left( \sum _{k}w_{jk}^{l}a_{k}^{l-1}+b_{j}^{l}\right) } {\partial b_{j}^{l}}=\delta _{j}^{l}\quad\left( l=2,3\right)\end{align} </p><p>损失函数在隐藏层（层2）／输出层（层3）关于权值的梯度\begin{align} &amp; \dfrac {\partial C_{\mathbf{x}}} {\partial w_{jk}^{l}}=\dfrac {\partial C_{\mathbf{x}}} {\partial z_{j}^{l}}\cdot \dfrac {\partial z_{j}^{l}} {\partial w_{jk}^{l}}=\delta _{j}^{l}\cdot \dfrac {\partial \left( \sum _{k}w_{jk}^{l}a_{k}^{l-1}+b_{j}^{l}\right) } {\partial w_{jk}^{l}}=\delta _{j}^{l}\cdot a_{k}^{l-1}\quad\left( l=2,3\right)\end{align} </p><p>算法4.1 偏置与权值的梯度计算算法<br>输入：实例$\mathbf{x}=\left(x^{\left(1\right)},\cdots,x^{\left(n\right)}\right)^\top$<br>输出：损失函数在隐藏层（层2）／输出层（层3）关于偏置及权值的梯度$\left(\dfrac {\partial C_{\mathbf{x}}} {\partial b_{j}^{l}}\right)$和$\left(\dfrac {\partial C_{\mathbf{x}}} {\partial w_{jk}^{l}}\right)$   </p><ol><li>为输入层设置对应的激活值$\left(a_j^1\right)_{j=1}^n=\left(x^{\left(j\right)}\right)_{j=1}^n$ </li><li>前向传播：对每个$l（l=2,3）$计算\begin{align} &amp;a_{j}^{l}=\sigma \left( z_{j}^{l}\right) \\ &amp; z_{j}^{l}= \sum _{k}w_{jk}^{l}\cdot a_{k}^{l-1}+b_{j}^{l}\end{align}  </li><li>计算输出层误差$\left(\delta _{j}^{3}\right)_{j=1}^m$；  </li><li>计算误差反向传播：隐藏层误差$\left(\delta _{j}^{2}\right)_{j=1}^p$；  </li><li>计算损失函数在隐藏层（层2）／输出层（层3）关于偏置及权值的梯度$\left(\dfrac {\partial C_{\mathbf{x}}} {\partial b_{j}^{l}}\right)$和$\left(\dfrac {\partial C_{\mathbf{x}}} {\partial w_{jk}^{l}}\right)$。</li></ol><p>算法4.2 梯度下降算法：<br>输入：训练实例集合$T$<br>输出：偏置和权值的更新</p><ol><li>对每个训练实例$\mathbf{x}$：设置对应的输入激活$\mathbf{a}^{\mathbf{x},1}$，并执行以下步骤：<ul><li>前向传播：计算$\mathbf{z}^{\mathbf{x},l}=\mathbf{W}^l\mathbf{a}^{\mathbf{x},l-1}+\mathbf{b}^l$及$\mathbf{a}^{\mathbf{x},l}=\sigma\left(\mathbf{z}^{\mathbf{x},l}\right)$，其中$l=2,3,\cdots,L$。</li><li>输出层误差：$\delta^{\mathbf{x},L}=\nabla_{\mathbf{a}^L}C_\mathbf{x}\odot\sigma’\left(\mathbf{z}^{\mathbf{x},L}\right)$</li><li>误差反向传播：对每个$l=L-1,L-2,\cdots,2$，计算$\delta^{\mathbf{x},l}=\left(\left(\mathbf{W}^{l+1}\right)^\top\delta^{\mathbf{x},l+1}\right)\odot\sigma’\left(\mathbf{z}^{\mathbf{x},l}\right)$。</li></ul></li><li>梯度下降：对每个$l=L-1,L-2,\cdots,2$，根据$\mathbf{W}^l\leftarrow\mathbf{W}^l-\frac{\eta}{N}\sum_\mathbf{x}\delta^{\mathbf{x},l}\left(\mathbf{a}^{\mathbf{x},l-1}\right)^\top$和$\mathbf{b}^l\leftarrow\mathbf{b}^l-\frac{\eta}{N}\sum_\mathbf{x}\delta^{\mathbf{x},l}$更新权重和偏置。</li></ol><h2 id="5-前馈神经网络的改进"><a href="#5-前馈神经网络的改进" class="headerlink" title="5 前馈神经网络的改进"></a>5 前馈神经网络的改进</h2><h3 id="交叉熵损失函数"><a href="#交叉熵损失函数" class="headerlink" title="交叉熵损失函数"></a>交叉熵损失函数</h3><p>单个实例$\mathbf{x}$的损失函数$C_{\mathbf{x}}$为交叉熵损失函数</p><script type="math/tex; mode=display">C_{\mathbf{x}}=-\sum _{j}\left[y_j\log\hat{y}_j+\left(1-y_j\right)\log\left(1-\hat{y}_j\right)\right]</script><h3 id="正则化目标函数"><a href="#正则化目标函数" class="headerlink" title="正则化目标函数"></a>正则化目标函数</h3><p>$L_2$正则化的目标函数（结构风险） </p><script type="math/tex; mode=display">C=\frac{1}{N}\left(\sum _{\mathbf{x}}C_{\mathbf{x}}+ \frac{\lambda}{2}\sum_w w^2\right)</script><h3 id="权值初始化"><a href="#权值初始化" class="headerlink" title="权值初始化"></a>权值初始化</h3><p>权值初始化<br>设对$l$层有有$n_{in}^l$个输入神经元，使用均值为$0$，标准差为$\frac{1}{\sqrt{n_{in}^l}}$的高斯分布初始化$l$层的权值</p><script type="math/tex; mode=display">w^l\sim N\left(0,\frac{1}{n_{in}^l}\right)</script><h3 id="早停止"><a href="#早停止" class="headerlink" title="早停止"></a>早停止</h3><p>当验证集上的错误率不再下降时，就停止迭代。停止迭代需要根据实际任务上进行优化。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nerual network</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Recurrent_Nerual_Network</title>
    <link href="/2020/05/05/Recurrent-Nerual-Network/"/>
    <url>/2020/05/05/Recurrent-Nerual-Network/</url>
    
    <content type="html"><![CDATA[<h1 id="循环神经网络（Recurrent-Neural-Network，RNN）"><a href="#循环神经网络（Recurrent-Neural-Network，RNN）" class="headerlink" title="循环神经网络（Recurrent Neural Network，RNN）"></a>循环神经网络（Recurrent Neural Network，RNN）</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><h2 id="1-循环神经网络结构"><a href="#1-循环神经网络结构" class="headerlink" title="1 循环神经网络结构"></a>1 循环神经网络结构</h2><p>数据集  </p><script type="math/tex; mode=display">\big\{\left(\mathbf{x}_t,\mathbf{y}_t\right)\big\}_{t=1}^T</script><p>其中，<br>第$t$时刻输入数据$\mathbf{x}_t=\left(x_t^{\left(1\right)},x_t^{\left(2\right)},\dots,x_t^{\left(n\right)}\right)^{\top}\in\mathbb{R}^n,$<br>第$t$时刻输出数据$\mathbf{y}_t=\left(y_t^{\left(1\right)},y_t^{\left(2\right)},\dots,y_t^{\left(m\right)}\right)^{\top}\in\mathbb{R}^m$</p><p>循环神经网络模型结构</p><script type="math/tex; mode=display">\begin{align} \left\{ \begin{array}{**lr**} \mathbf{h}_t=f\left(\mathbf{W}\centerdot\mathbf{x}_t+\mathbf{U}\centerdot\mathbf{h}_{t-1}\right) & \\ {\mathbf{y}}_t=f\left( \mathbf{V}\centerdot\mathbf{h}_t\right) \\ \end{array} \right. \end{align}</script><p>其中，$\mathbf{h}$为隐状态，$f\left(\cdot\right)$为非线性激活函数，$\mathbf{U},\mathbf{W},\mathbf{V}$为模型参数。</p><h2 id="2-长短期记忆神经网络（Long-Short-Term-Memory，LSTM）"><a href="#2-长短期记忆神经网络（Long-Short-Term-Memory，LSTM）" class="headerlink" title="2 长短期记忆神经网络（Long Short Term Memory，LSTM）"></a>2 长短期记忆神经网络（Long Short Term Memory，LSTM）</h2><p><img src="https://i.loli.net/2020/05/05/l2Q9vT3UkqAnPFj.png" srcset="/img/loading.gif"  align=center  width = "450" height = "300" /></p><p>遗忘们：</p><script type="math/tex; mode=display">\mathbf{f}_t=\sigma\left(W_{fh}\mathbf{h}_{t-1}+W_{fx}\mathbf{x}_t+\mathbf{b}_f\right)</script><p>输入门：</p><script type="math/tex; mode=display">\begin{align}\mathbf{i}_t &= \sigma\left(W_{ih}\mathbf{h}_{t-1}+W_{ix}\mathbf{x}_t+\mathbf{b}_i\right)  \\\tilde{\mathbf{c}}_t &= \tanh\left(W_{ch}\mathbf{h}_{t-1}+W_{cx}\mathbf{x}_t+\mathbf{b}_c\right)\end{align}</script><p>输出门：</p><script type="math/tex; mode=display">\begin{align}\mathbf{c}_t&=\mathbf{f}_t\circ\mathbf{c}_{t-1}+\mathbf{i}_t\circ\tilde{\mathbf{c}}_t \\\mathbf{o}_t&=\sigma\left(W_{oh}\mathbf{h}_{t-1}+W_{ox}\mathbf{x}_t+\mathbf{b}_o\right) \\\mathbf{h}_t&=\mathbf{o}_t\circ\tanh\left(\mathbf{c}_t\right)\end{align}</script><h2 id="3-门控循环单元（Gated-Recurrent-Unit，GRU）"><a href="#3-门控循环单元（Gated-Recurrent-Unit，GRU）" class="headerlink" title="3 门控循环单元（Gated Recurrent Unit，GRU）"></a>3 门控循环单元（Gated Recurrent Unit，GRU）</h2><p><img src="https://i.loli.net/2020/05/05/DsVYvbOZwpicgNC.png" srcset="/img/loading.gif"  align=center  width = "450" height = "300" /></p><p>更新门：</p><script type="math/tex; mode=display">\mathbf{z}_{t}=\sigma\left(W_{z} \cdot\left[\mathbf{h}_{t-1}, \mathbf{x}_{t}\right]\right)</script><p>重置门：</p><script type="math/tex; mode=display">\mathbf{r}_{t}=\sigma\left(W_{r} \cdot\left[\mathbf{h}_{t-1}, \mathbf{x}_{t}\right]\right)</script><p>输出门：</p><script type="math/tex; mode=display">\begin{aligned} \tilde{\mathbf{h}}_{t} &=\tanh \left(W \cdot\left[\mathbf{r}_{t} \circ \mathbf{h}_{t-1}, \mathbf{x}_{t}\right]\right) \\ \mathbf{h} &=\left(1-\mathbf{z}_{t}\right) \circ \mathbf{h}_{t-1}+\mathbf{z}_{t} \circ \tilde{\mathbf{h}}_{t} \end{aligned}</script>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nerual network</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Convolutional_Nerual_Network</title>
    <link href="/2020/05/05/Convolutional-Nerual-Network/"/>
    <url>/2020/05/05/Convolutional-Nerual-Network/</url>
    
    <content type="html"><![CDATA[<h1 id="卷积神经网络（Convolutional-Neural-Network，CNN）"><a href="#卷积神经网络（Convolutional-Neural-Network，CNN）" class="headerlink" title="卷积神经网络（Convolutional Neural Network，CNN）"></a>卷积神经网络（Convolutional Neural Network，CNN）</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><p>卷积神经网络一般由<strong><em>卷积层</em></strong>、<strong><em>汇聚层</em></strong>和<strong><em>全连接层</em></strong>交叉堆叠而成的<strong><em>前馈神经网络</em></strong>，使用<strong><em>误差反向传播算法</em></strong>进行训练。</p><h2 id="1-卷积基础"><a href="#1-卷积基础" class="headerlink" title="1 卷积基础"></a>1 卷积基础</h2><p><strong>一维卷积</strong>：信号发生器每个时刻$t$产生一个信号$x_t$，生成信号序列$\left(x_1,x_2,\cdots,x_t,\cdots\right)$。其信息衰减率为$w_k$，即在$k-1$个时间步长后信息为原来的$w_k$倍，将$\left(w_1,w_2,\cdots,w_k,\cdots\right)$称为滤波器或卷积核。</p><p>设滤波器序列为$\left(w_1,w_2,\cdots,w_k,\cdots,w_m\right)$，其与信号序列$\left(x_1,x_2,\cdots,x_t,\cdots\right)$的卷积</p><script type="math/tex; mode=display">y_t=\sum_{k=1}^m w_k\cdot x_{t-k+1}</script><p>信号序列$\mathbf{x}$和滤波器$\mathbf{w}$的卷积</p><script type="math/tex; mode=display">\mathbf{y}=\mathbf{w}\otimes\mathbf{x}</script><p><strong>二维卷积</strong>：给定二维图像数据$X\in\mathbb{R}^{M\times N}$和二维滤波器$W\in\mathbb{R}^{m\times n}$，其卷积为</p><script type="math/tex; mode=display">y_{ij}=\sum_{u=1}^m\sum_{v=1}^n w_{uv}\cdot x_{i-u+1, j-v+1}</script><p><strong>互相关</strong>：给定二维图像数据$X\in\mathbb{R}^{M\times N}$和二维卷积核$W\in\mathbb{R}^{m\times n}$，其互相关定义为</p><script type="math/tex; mode=display">y_{ij}=\sum_{u=1}^m\sum_{v=1}^n w_{uv}\cdot x_{i+u-1,j+v-1}</script><p>图像数据$X\mathbb{R}^{M\times N}$和卷积核$W\in\mathbb{R}^{m\times n}$的互相关</p><script type="math/tex; mode=display">Y=W\otimes X</script><p>其中$Y\in\mathbb{R}^{M-m+1,N-n+1}$为输出矩阵。</p><p>互相关和卷积的区别在于卷积核发生了反转（在两个维度上颠倒次序，即旋转$180$度）。因此互相关也称为不反转卷积。</p><p><strong>卷积扩展</strong></p><ol><li>卷积核步长：卷积核在滑动时的间隔；  </li><li>数据零填充：在输入数据各维度的两端进行补零。</li></ol><p>假设一维卷积的输入向量元素个数为$M$，卷积核元素个数为$m$，卷积核步长为$s$，输入数据两端各$p$个零填充，则卷积输出向量个数为$\left(M-m+2p\right)/s+1$。</p><p><strong>窄卷积（Narrow Convolution）</strong>：步长$s=1$，两端补零$p=0$，卷积输出长度为$M-m+1$。<br><strong>宽卷积（Wide Convolution）</strong>：步长$s=1$，两端补零$p=m-1$，卷积后输出长度为$M+m-1$。<br><strong>等宽卷积（Equal-Width Convolution）</strong>：步长$s=1$，两端补零$p=\left(m-1\right)/2$，卷积后输出长度为$M$。</p><p><strong>卷积交换性</strong><br>二维图像数据$X\in\mathbb{R}^{M\times N}$和二维卷积核$Y\in\mathbb{R}^{m\times n}$，对图像数据$X$的两个维度进行零填充（两端各补$m-1$个和$n-1$个零），得到全填充的图像数据$\tilde{X}\in\mathbb{R}^{\left(M+m-1\right)\times\left(N+n-1\right)}$。图像数据$X$和卷积核$W$的宽卷积</p><script type="math/tex; mode=display">W\tilde{\otimes}X\triangleq W\otimes\tilde{X}</script><p>其中$\tilde{\otimes}$为宽卷积操作。</p><p>宽卷积具有交换性，即</p><script type="math/tex; mode=display">W\tilde{\otimes}X=X\tilde{\otimes}W</script><h2 id="2-卷积神经网络结构"><a href="#2-卷积神经网络结构" class="headerlink" title="2 卷积神经网络结构"></a>2 卷积神经网络结构</h2><h3 id="卷积层"><a href="#卷积层" class="headerlink" title="卷积层"></a>卷积层</h3><p>卷积层提取空间局部区域特征。</p><p>卷积层结构：</p><ul><li>输入特征映射组：$X\in\mathbb{R}^{M\times N\times D}$为三维张量，其中每个切片矩阵$X^d\in\mathbb{R}^{M\times N}$为一个输入特征映射，$1\leq d\leq D$。</li><li>输出特征映射组：$Y\in\mathbb{R}^{M’\times N’ \times P}$为三维张量，其中每个切片矩阵$Y^p\in\mathbb{R}^{M’ \times N’}$为一个输出特征映射，$1 \leq p \leq P$。</li><li>卷积核：$W\in\mathbb{R}^{m\times n\times D\times P}$为四维张量，其中每个切片矩阵$W^{p,d}\in\mathbb{R}^{m\times n}$为一个二维卷积核，$1\leq d\leq D,1 \leq p \leq P$。</li></ul><p><img src="https://i.loli.net/2020/05/05/STJL3cWIl6hd1tm.png" srcset="/img/loading.gif"  align=center  width = "650" height = "250" /></p><p>输入特征映射组$X$到输入特征映射$Y^p$</p><script type="math/tex; mode=display">\begin{align}Z^p&=W^p\otimes X+b^p=\sum_{d=1}^D W^{p,d}\otimes X^d+b^p \\Y^p &= f\left(Z^p\right)\end{align}</script><p><img src="https://i.loli.net/2020/05/05/N6YXO7hZrJ9pltI.png" srcset="/img/loading.gif"  align=center  width = "450" height = "250" /></p><p>在输入为$X\in\mathbb{R}^{M\times N\times D}$，卷积核为$W\in\mathbb{R}^{m\times n\times D\times P}$，输出为$Y\in\mathbb{R}^{M’\times N’ \times P}$的卷积层，共需要$\left(m\times n\right)\times D\times P+P$个参数。</p><h3 id="汇聚层（子采样层、池化层）"><a href="#汇聚层（子采样层、池化层）" class="headerlink" title="汇聚层（子采样层、池化层）"></a>汇聚层（子采样层、池化层）</h3><p>汇聚层进行特征选择，降低特征数量。</p><p>汇聚层的输入层特征映射组为$X\in\mathbb{R}^{M\times N\times D}$，对其中每个特征映射$X^d$，将其划分为多个区域$R_{m,n}^d,1\leq m\leq M,1\leq n\leq N$。汇聚是对每个区域进行采样，得到一个值作为该区域的概括。</p><p>常用汇聚函数：</p><ul><li>最大汇聚（Maximum Pooling）：取区域内最大值作为输出<script type="math/tex; mode=display">Y_{m,n}^d=\max_{i\in R_{m,n}^d} x_i</script></li><li>平均汇聚（Mean Pooling）：去区域内平均值作为输出<script type="math/tex; mode=display">Y_{m,n}^d=\frac{1}{|R_{m,n}^d|}\sum_{i\in R_{m,n}^d}x_i</script></li></ul><p>典型汇聚层是将每个特征映射划分为$k\times k$个不重叠区域，然后使用最大汇聚方式进行采样。<br><img src="https://i.loli.net/2020/05/05/uPn85soHmZ4Yt6F.png" srcset="/img/loading.gif"  align=center  width = "450" height = "250" /></p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>典型的卷积神经网络是有卷积层、汇聚层、全连接层交叉堆叠而成。<br><img src="https://i.loli.net/2020/05/05/ZmXSHAw8zphysEe.png" srcset="/img/loading.gif"  align=center  width = "650" height = "150" /></p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nerual network</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>CRF</title>
    <link href="/2020/05/05/CRF/"/>
    <url>/2020/05/05/CRF/</url>
    
    <content type="html"><![CDATA[<h1 id="条件随机场（Conditional-Random-Field，CRF）"><a href="#条件随机场（Conditional-Random-Field，CRF）" class="headerlink" title="条件随机场（Conditional Random Field，CRF）"></a>条件随机场（Conditional Random Field，CRF）</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><p>图是由结点及连接结点的编组成的集合。<br>结点记作$v$，结点集合记作$V$；边记作$e$，边的集合记作$E$；图记作$G=\left(V,E\right)$。</p><p>概率图模型是由图表示的概率分布。设由联合分布$P\left(Y\right)$，$Y \in \mathcal{Y}$是一组随机变量。<br>由无向图$G=\left(V,E\right)$表示概率分布$P\left(Y\right)$，即在图$G$中，结点$v \in V$表示一个随机变量$Y_{v}$，$Y=\left(Y_{v}\right)_{v \in V}$；边$e \in E$表示随机变量之间的概率依赖关系。</p><p>成对马尔可夫性<br>\begin{align} \\&amp; P\left(Y_{u},Y_{v}|Y_{O}\right)=P\left(Y_{u}|Y_{O}\right)P\left(Y_{v}|Y_{O}\right)\end{align}<br>其中，设$u$和$v$是无向图$G$中任意两个没有边连接的结点，分别对应随机变量$Y_{u}$和$Y_{v}$；其它所有结点为$O$，对应的随机变量组是$Y_{O}$。<br>即，给定随机变量组$Y_{O}$的条件下随机变量$Y_{u}$和$Y_{v}$是条件独立的。</p><p>局部马尔可夫性<br>\begin{align} \\&amp; P\left(Y_{v},Y_{O}|Y_{W}\right)=P\left(Y_{v}|Y_{W}\right)P\left(Y_{O}|Y_{W}\right)\end{align}<br>其中，设$v \in V$是无向图$G$中任意一个结点，对应的随机变量是$Y_{v}$。$W$是与$v$有边连接的所有的结点，对应的随机变量组是$Y_{W}$。$O$是$v,W$以外的其它所有结点，对应的随机变量组是$Y_{O}$。<br>即，给定随机变量组$Y_{W}$的条件下随机变量$Y_{v}$和$Y_{O}$是条件独立的。</p><p>全局马尔可夫性<br>\begin{align} \\&amp; P\left(Y_{A},Y_{B}|Y_{C}\right)=P\left(Y_{A}|Y_{C}\right)P\left(Y_{B}|Y_{C}\right)\end{align}<br>其中，设结点集合$A,B$是在无向图$G$中被结点集合$C$分开的任意结点集合，对应的随机变量组是$Y_{A},Y_{B},Y_{C}$。<br>即，给定随机变量组$Y_{C}$的条件下随机变量$Y_{A}$和$Y_{B}$是条件独立的。</p><p>概率无向图模型（马尔可夫随机场）：设有联合概率分布$P\left(Y\right)$，由无向图$G=\left(V,E\right)$表示，在图$G$中，结点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率分布$P\left(Y\right)$满足成对、局部或全局马尔可夫性，就称次联合概率分布为概率无向图模型，或马尔可夫随机场。</p><p>团：无向图$G$中任意两个结点均有边连接的结点子集。<br>最大团：无向图$G$中的一个团，并且不能再加进任何一个结点使其成为一个更大的团。</p><p>概率无向图模型的联合概率分布<br>\begin{align} \\&amp; P\left(Y\right)=\dfrac{1}{Z} \prod_{C} \varPsi_{C}\left(Y_{C}\right) \end{align}<br>其中，$C$是无向图的最大团，$Y_{C}$是$C$的结点对应的随机变量；势函数$\varPsi_{C}\left(Y_{C}\right)$是严格正的<br>\begin{align} \\&amp; \varPsi_{C}\left(Y_{C}\right) = \exp \left\{-E\left(Y_{C}\right)\right\} \end{align}<br>$Z$是规范化因子，保证$P\left(Y\right)$构成一个概率分布<br>\begin{align} \\&amp; Z = \sum_{Y} \prod_{C} \varPsi_{C}\left(Y_{C} \right) \end{align}<br>乘积是在无向图所有的最大团上进行的。</p><p>设$X$与$Y$是随机变量，$P\left(Y|X\right)$是在给定$X$的条件下$Y$的条件概率分布，若随机变量$Y$构成一个由无向图$G=\left(V,E\right)$表示的马尔可夫随机场，即<br>\begin{align} \\&amp; P\left(Y_{v}|X,Y_{w},w \neq v \right)= P\left(X,Y_{w},w \thicksim v \right)\end{align}<br>对任意结点$v$成立，则称条件概率分布$P\left(Y|X\right)$为条件随机场。其中，$w \thicksim v$表示在图$G=\left(V,E\right)$中与结点$v$有边连接的所有结点$w$，$w \neq v$表示结点$v$以外的所有结点，$Y_{v}$与$Y_{w}$为结点$v$与$w$对应的随机变量。</p><p>设$X=\left(X_{1},X_{2}, \cdots, X_{n}\right)$，$Y=\left(Y_{1},Y_{2}, \cdots, Y_{n}\right)$均为线性链表示的随机变量序列，若在给定随机变量序列$X$的条件下，随机变量序列$Y$的条件概率分布$P\left(Y|X\right)$构成条件随机场，即满足马尔可夫性<br>\begin{align} \\&amp; P\left(Y_{i}|X,Y_{1},\cdots,Y_{i-1},Y_{i+1},\cdots,Y_{n} \right)= P\left(Y_{i}|X,Y_{i-1},Y_{i+1}\right)\end{align}<br>称条件概率分布$P\left(Y|X\right)$为线性链条件随机场。</p><p>线性链条件随机场的参数化形式<br>设$P\left(Y|X\right)$为线性链条件随机场，则在随机变量$X$取值为$x$的条件下，随机变量$Y$取值为$y$的条件概率<br>\begin{align} \\&amp; P\left(y|x\right)= \dfrac{1}{Z\left(x\right)} \exp \left(\sum_{i,k} \lambda_{k} t_{k} \left(y_{i-1},y_{i},x,i\right)+\sum_{i,l} \mu_{l}s_{l} \left(y_{i},x,i\right) \right)\end{align}<br>其中，<br>\begin{align} \\&amp; Z\left(x\right) = \sum_{y} \exp \left(\sum_{i,k} \lambda_{k} t_{k} \left(y_{i-1},y_{i},x,i\right)+\sum_{i,l} \mu_{l}s_{l} \left(y_{i},x,i\right) \right)\end{align}<br>$t_{k}$是定义在边上特征函数，称为转移特征，依赖于当前和前一个位置；$s_{l}$是定义在结点上的特征函数，称为状态特征，依赖于当前位置。特征函数$t_{k}$和$s_{l}$取值为1或0。$\lambda_{k}$和$\mu_{l}$是对应的权值，$Z\left(x\right)$是规范化因子，求和是在所有可能的输出序列上进行的。</p><p>线性链条件随机场的简化化形式<br>设有$K_{1}$个转移特征，$K_{2}$个状态特征，$K=K_{1}+K_{2}$，记<br>\begin{align} f_{k}\left(y_{i-1},y_{i},x,i\right) = \left\{<br>\begin{aligned}<br>\ &amp;  t_{k}\left(y_{i-1},y_{i},x,i\right), \quad k=1,2,\cdots,K_{1}<br>\\ &amp; s_{l}\left(y_{i},x,i\right), \quad k=K_{1}+l; \quad l=1,2,\cdots,K_{2}<br>\end{aligned}<br>\right.\end{align} </p><p>转移与状态特征在各个位置$i$求和，记作<br>\begin{align} \\&amp; f_{k} \left(y,x\right) = \sum_{i=1}^{n} f_{k}\left(y_{i-1},y_{i},x,i\right), \quad k=1,2, \cdots,K\end{align}</p><p>用$w_{k}$表示特征$f_{k}\left(y,x\right)$的权值，即<br>\begin{align} w_{k} = \left\{<br>\begin{aligned}<br>\ &amp;  \lambda_{k}, \quad k=1,2,\cdots,K_{1}<br>\\ &amp; \mu_{l}, \quad k=K_{1}+l; \quad l=1,2,\cdots,K_{2}<br>\end{aligned}<br>\right.\end{align} </p><p>则条件随机场可表示为<br>\begin{align} \\&amp; P\left(y|x\right)= \dfrac{1}{Z\left(x\right)} \exp \sum_{k=1}^{K} w_{k} f_{k}\left(y,x\right)<br>\\ &amp; Z\left(x\right) = \sum_{y} \exp \sum_{k=1}^{K} w_{k} f_{k}\left(y,x\right) \end{align}</p><p>以$w$表示权值向量，即<br>\begin{align} \\&amp; w=\left(w_{1},w_{2}, \cdots, w_{K}\right)^{T}\end{align}</p><p>以$F\left(y,x\right)$表示权局特征向量，即<br>\begin{align} \\&amp; F\left(y,x\right) = \left(f_{1}\left(y,x\right),f_{2}\left(y,x\right), \cdots, f_{K}\left(y,x\right)\right)^{T} \end{align}</p><p>则条件随机场可写成向量$w$与$F\left(y,x\right)$的内积的形式<br>\begin{align} \\&amp; P\left(y|x\right) = \dfrac{\exp \left(w \cdot F\left(y,x\right)\right)}{Z_{w}\left(x\right)} \end{align}<br>其中，<br>\begin{align} \\&amp; Z_{w}\left(x\right) = \sum_{y} \exp \left(w \cdot F\left(y,x\right)\right) \end{align}</p><p>假设$P_{w}\left(y|x\right)$是线性链条件随机场对给定观测序列$x$，相应的标记序列$y$的条件概率。引进特殊的起点标记$y_{0}=start$表示开始状态，特殊的终点标记$y_{n+1}=stop$表示终止状态。对观测序列$x$的每一个位置$i=1,2,\cdots,n+1$，定义一个$m$阶矩阵（$m$是标记$y_{i}$取值的个数）<br>\begin{align} \\&amp; M_{i}\left(x\right) = \left[ M_{i} \left(y_{i-1},y_{i}|x\right) \right]_{m \times m}<br>\\ &amp; M_{i} \left(y_{i-1},y_{i}|x\right) = \exp \left(W_{i} \left(y_{i-1},y_{i},|x\right)\right)<br>\\ &amp; W_{i} \left(y_{i-1},y_{i},|x\right) = \sum_{i=1}^{K} w_{k} f_{k} \left(y_{i-1},y_{i},x,i\right)\end{align}<br>则<br>\begin{align} \\&amp; P_{w}\left(y|x\right) = \dfrac{1}{Z_{w}\left(x\right)} \prod_{i=1}^{n+1} M_{i} \left(y_{i-1},y_{i}|x\right)\end{align}<br>其中，$Z_{w}$为规范化因子，是$n+1$个矩阵的乘积的元素，是以$start$为起点$stop$为终点通过状态的所有路径$y_{1}y_{2}\cdots y_{n}$的非规范化概率$\prod_{i=1}^{n+1} M_{i} \left(y_{i-1},y_{i}|x\right)$之和。<br>\begin{align} \\&amp; Z_{w}\left(x\right) = \left(M_{1}\left(x\right),M_{2}\left(x\right), \cdots,M_{n+1}\left(x\right)\right)_{start,stop}\end{align}</p><p>对每个指标$i=0,1,\cdots,n+1$，定义前向向量$\alpha_{i}\left(x\right)$<br>\begin{align} \alpha_{0}\left(y|x\right) = \left\{<br>\begin{aligned}<br>\ &amp;  1, \quad y=start<br>\\ &amp; 0, \quad 否则<br>\end{aligned}<br>\right.\end{align}<br>递推公式<br>\begin{align} \\&amp; \alpha_{i}^{T}\left(y_{i}|x\right) = \alpha_{i-1}^{T}\left(y_{i}|x\right) \left[M_{i}\left(y_{i-1},y_{i}|x\right)\right], \quad i=1,2,\cdots,n+1\end{align}<br>又表示为<br>\begin{align} \\&amp; \alpha_{i}^{T} \left(x\right) = \alpha_{i-1}^{T} \left(x\right) M_{i}\left(x\right)\end{align}<br>$\alpha_{i}\left(y_{i}|x\right)$表示在位置$i$的标记是$y_{i}$并且到位置$i$的前部分标记序列的非规范化概率，$y_{i}$可取的值又$m$个，所以$\alpha_{i}\left(x\right)$是$m$维列向量。</p><p>对每个指标$i=0,1,\cdots,n+1$，定义后向向量$\beta_{i}\left(x\right)$<br>\begin{align} \beta_{n+1}\left(y_{n+1}|x\right) = \left\{<br>\begin{aligned}<br>\ &amp;  1, \quad y_{n+1}=stop<br>\\ &amp; 0, \quad 否则<br>\end{aligned}<br>\right.\end{align}<br>递推公式<br>\begin{align} \\&amp; \beta_{i}\left(y_{i}|x\right) = \left[M_{i}\left(y_{i},y_{i+1}|x\right)\right]\beta_{i+1}\left(y_{i+1}|x\right) , \quad i=1,2,\cdots,n+1\end{align}<br>又表示为<br>\begin{align} \\&amp; \beta_{i} \left(x\right) = M_{i+1}\left(x\right) \beta_{i+1} \left(x\right) \end{align}<br>$\beta_{i}\left(y_{i}|x\right)$表示在位置$i$的标记是$y_{i}$并且到位置$i+1$到$n$的后部分标记序列的非规范化概率。</p><p>由前向－后向向量，得<br>\begin{align} \\&amp; Z\left(x\right)=\alpha_{n}^{T}\left(x\right) \cdot \mathbf{1} = \mathbf{1}^{T} \cdot \beta_{i}\left(x\right) \end{align}<br>其中，$\mathbf{1}$是元素均为1的$m$维列向量。 </p><p>标记序列中，在位置$i$是标记$y_{i}$的条件概率<br>\begin{align} \\&amp; P\left(Y_{i}=y_{i}|x\right) = \dfrac{\alpha_{i}^{T}\left(y_{i}|x\right) \beta_{i}\left(y_{i}|x\right)}{Z\left(x\right)}\end{align}</p><p>标记序列中，在位置$i－1$是标记$y_{i-1}$，在位置$i$是标记$y_{i}$的条件概率<br>\begin{align} \\&amp; P\left(Y_{i－1}=y_{i-1},Y_{i}=y_{i}|x\right) = \dfrac{\alpha_{i-1}^{T}\left(y_{i-1}|x\right) M_{i}\left(y_{i-1},y_{i}|x\right)\beta_{i}\left(y_{i}|x\right)}{Z\left(x\right)}\end{align}</p><p>其中，<br>\begin{align} \\&amp; Z\left(x\right)=\alpha_{n}^{T}\left(x\right) \cdot \mathbf{1}  \end{align}</p><p>特征函数$f_{k}$关于条件分布$P\left(Y|X\right)$的数学期望<br>\begin{align} \\&amp; E_{P\left(Y|X\right)} \left[f_{k}\right]=\sum_{y} P\left(y|x\right) f_{k}\left(y,x\right)<br>\\ &amp; = \sum_{i=1}^{n+1} \sum_{y_{i-1},y_{i}} f_{k}\left(y_{i-1},y_{i},x,i\right) \dfrac{\alpha_{i-1}^{T}\left(y_{i-1}|x\right) M_{i}\left(y_{i-1},y_{i}|x\right)\beta_{i}\left(y_{i}|x\right)}{Z\left(x\right)}\end{align}</p><p>假设经验分布为$\tilde{P}\left(x\right)$，特征函数$f_{k}$关于联合分布$P\left(X,Y\right)$的数学期望<br>\begin{align} \\&amp; E_{P\left(X,Y\right)} \left[f_{k}\right]=\sum_{x,y} P\left(x,y\right) \sum_{i=1}^{n+1} f_{k}\left(y_{i-1},y_{i},x,i\right)<br>\\ &amp; = \sum_{x} \tilde{P}\left(x\right) \sum_{y} P\left(y|x\right) \sum_{i=1}^{n+1} f_{k}\left(y_{i-1},y_{i},x,i\right)<br>\\ &amp; = \sum_{x} \tilde{P}\left(x\right) \sum_{i=1}^{n+1} \sum_{y_{i-1},y_{i}} f_{k}\left(y_{i-1},y_{i},x,i\right) \dfrac{\alpha_{i-1}^{T}\left(y_{i-1}|x\right) M_{i}\left(y_{i-1},y_{i}|x\right)\beta_{i}\left(y_{i}|x\right)}{Z\left(x\right)} \quad \quad k=1,2,\cdots,K<br>\end{align}</p><p>其中，<br>\begin{align} \\&amp; Z\left(x\right)=\alpha_{n}^{T}\left(x\right) \cdot \mathbf{1}  \end{align}</p><p>由训练数据集，得经验概率分布$\tilde{P}\left(X,Y\right)$。<br>训练数据的对数似然函数<br>\begin{align} \\&amp; L\left(w\right)= L_{\tilde{P}}\left(P_{w}\right)=\log \prod_{x,y} P_{w}\left(y|x\right)^{\tilde{P}\left(x,y\right)} = \sum_{x,y} \tilde{P}\left(x,y\right) \log P_{w}\left(y|x\right)\end{align}</p><p>当$P_{w}$是条件随机场模型时，对数似然函数<br>\begin{align} \\&amp; L\left(w\right)= \sum_{x,y} \tilde{P}\left(x,y\right) \log P_{w}\left(y|x\right)<br>\\ &amp; = \sum_{x,y} \left[\tilde{P}\left(x,y\right) \sum_{k=1}^{K} w_{k} f_{k}\left(y,x\right) - \tilde{P}\left(x,y\right) \log Z_{w}\left(x\right) \right]<br>\\ &amp; = \sum_{j=1}^{N} \sum_{k=1}^{K} w_{k} f_{k} \left(y_{j},x_{j}\right) - \sum_{j=1}^{N} \log Z_{w} \left(x_{j}\right) \end{align}</p><p>设模型当前参数向量<br>\begin{align} \\&amp; w = \left(w_{1},w_{2},\cdots,w_{K}\right)^{T}\end{align}<br>向量的增量<br>\begin{align} \\&amp; \delta = \left(\delta_{1},\delta_{2},\cdots,\delta_{K}\right)^{T}\end{align}<br>更新参数向量<br>\begin{align} \\&amp; w+\delta = \left(w_{1}+\delta_{1},w_{2}+\delta_{2},\cdots,w_{K}+\delta_{K}\right)^{T}\end{align}</p><p>关于转移特征$t_{k}$的更新方程<br>\begin{align} \\&amp; E_{\tilde{P}} \left[t_{k}\right]=\sum_{x,y} \tilde{P} \left(x,y\right) \sum_{i=1}^{n+1} t_{k}\left(y_{i-1},y_{i},x,i\right)<br>\\ &amp; = \sum_{x,y} \tilde{P} \left(x\right) P\left(y|x\right) \sum_{i=1}^{n+1} t_{k}\left(y_{i-1},y_{i},x,i\right)\exp\left(\delta_{k}T\left(x,y\right)\right) \quad \quad k=1,2,\cdots,K<br>\end{align}</p><p>关于转移特征$s_{l}$的更新方程<br>\begin{align} \\&amp; E_{\tilde{P}} \left[s_{l}\right]=\sum_{x,y} \tilde{P} \left(x,y\right) \sum_{i=1}^{n} s_{l}\left(y_{i},x,i\right)<br>\\ &amp; = \sum_{x,y} \tilde{P} \left(x\right) P\left(y|x\right) \sum_{i=1}^{n} s_{l}\left(y_{i},x,i\right)\exp\left(\delta_{K_{1}+l}T\left(x,y\right)\right) \quad \quad l=1,2,\cdots,K_{2}<br>\end{align}</p><p>其中，$T\left(x,y\right)$是在数据$\left(x,y\right)$中出现的所有特征函数的总和<br>\begin{align} \\&amp; T\left(x,y\right)=\sum_{k} f_{k}\left(y,x\right)=\sum_{k=1}^{K} \sum_{i=1}^{n+1} f_{k} \left(y_{i-1},y_{i},x,i\right)\end{align}</p><p>条件随机场模型学习的改进的迭代尺度法：<br>输入：特征函数$t_{1},t_{2},\cdots,t_{K_{1}}$，$s_{1},s_{2},\cdots,s_{K_{2}}$，经验分布$\tilde{P}\left(x,y\right)$<br>输出：参数估计值$\hat{w}\quad$ 模型$P_{\hat{w}}$ </p><ol><li>对所有$k \in \left\{1,2,\cdots,K\right\}$，取初值$w_{k}=0$ </li><li>对每一个$k \in \left\{1,2,\cdots,K\right\}$<br>2.1当$k=1,2,\cdots,K_{1}$时，令$\delta_{k}$是方程<br>\begin{align} \\&amp; \sum_{x,y} \tilde{P} \left(x\right) P\left(y|x\right) \sum_{i=1}^{n+1} t_{k}\left(y_{i-1},y_{i},x,i\right)\exp\left(\delta_{k}T\left(x,y\right)\right)=E_{\tilde{P}} \left[t_{k}\right]<br>\end{align}<br>的解<br>当$k=K_{1}+l,l=1,2,\cdots,K_{2}$时，令$\delta_{K_{1}+l}$是方程<br>\begin{align} \\&amp;  \sum_{x,y} \tilde{P} \left(x\right) P\left(y|x\right) \sum_{i=1}^{n} s_{l}\left(y_{i},x,i\right)\exp\left(\delta_{K_{1}+l}T\left(x,y\right)\right)=E_{\tilde{P}} \left[s_{l}\right]<br>\end{align}<br>的解<br>2.2更新$w_{k}$的值：$w_{k} \leftarrow w_{k}+\delta_{k}$  </li><li>如果不是所有的$w_{k}$都收敛，重复步骤2.<br>其中，<br>\begin{align} \\&amp; T\left(x,y\right)=\sum_{k} f_{k}\left(y,x\right)=\sum_{k=1}^{K} \sum_{i=1}^{n+1} f_{k} \left(y_{i-1},y_{i},x,i\right)\end{align}</li></ol><p>算法$S$是解决$T\left(x,y\right)$表示数据$\left(x,y\right)$中特征总数时，对不同数据$\left(x,y\right)$取值可能不同的问题。引入松弛特征<br>\begin{align} \\&amp; s\left(x,y\right) = S - \sum_{i=1}^{n+1} \sum_{k=1}^{K} f_{k} \left(y_{i-1},y_{i},x,i\right)\end{align}<br>其中，$S$是一个常数。选取足够大的常数$S$使得对训练数据集的所有数据$\left(x,y\right)$，$s\left(x,y\right) \geq 0$成立。这时特征总数可取$S$。</p><p>对转移特征$t_{k}$，$\delta_{k}$的更新方程<br>\begin{align} \\&amp; \sum_{x,y} \tilde{P} \left(x\right) P\left(y|x\right) \sum_{i=1}^{n+1} t_{k}\left(y_{i-1},y_{i},x,i\right)\exp\left(\delta_{k}S\right)=E_{\tilde{P}} \left[t_{k}\right]<br>\\ &amp; \delta_{k}=\dfrac{1}{S} \log \dfrac{E_{\tilde{P}}\left[t_{k}\right]}{E_{P}\left[t_{k}\right]}\end{align}<br>其中，<br>\begin{align} \\&amp; E_{P} \left[t_{k}\right]=\sum_{x} \tilde{P}\left(x\right) \sum_{i=1}^{n+1} \sum_{y_{i-1},y_{i}} t_{k}\left(y_{i-1},y_{i},x,i\right) \dfrac{\alpha_{i-1}^{T}\left(y_{i-1}|x\right) M_{i}\left(y_{i-1},y_{i}|x\right)\beta_{i}\left(y_{i}|x\right)}{Z\left(x\right)}<br>\end{align}</p><p>对状态特征$s_{l}$，$\delta_{k}$的更新方程<br>\begin{align} \\&amp; \sum_{x,y} \tilde{P} \left(x\right) P\left(y|x\right) \sum_{i=1}^{n} s_{l}\left(y_{i},x,i\right)\exp\left(\delta_{K_{1}}S\right)=E_{\tilde{P}} \left[s_{l}\right]<br>\\ &amp; \delta_{K_{1}+l}=\dfrac{1}{S} \log \dfrac{E_{\tilde{P}}\left[s_{l}\right]}{E_{P}\left[s_{l}\right]}\end{align}<br>其中，<br>\begin{align} \\&amp; E_{P} \left[s_{l}\right]=\sum_{x} \tilde{P}\left(x\right) \sum_{i=1}^{n} \sum_{y_{i-1},y_{i}} s_{l}\left(y_{i},x,i\right) \dfrac{\alpha_{i}^{T}\left(y_{i-1}|x\right) \beta_{i}\left(y_{i}|x\right)}{Z\left(x\right)}<br>\end{align}</p><p>算法$T$是解决算法$S$每步迭代增量向量变大，算法收敛变慢问题。对每个观测序列$x$计算其特征总数最大值$T\left(x\right)$<br>\begin{align} \\&amp; T\left(x\right) = \max_{y} T\left(x,y\right)=t\end{align}</p><p>关于转移特征参数的更新方程<br>\begin{align} \\&amp; E_{\tilde{P}} \left[t_{k}\right]=\sum_{x,y} \tilde{P} \left(x\right)  P\left(y|x\right) \sum_{i=1}^{n+1} t_{k}\left(y_{i-1},y_{i},x,i\right)\exp\left(\delta_{k}T\left(x,y\right)\right)<br>\\ &amp; = \sum_{x} \tilde{P} \left(x\right) \sum_{y} P\left(y|x\right) \sum_{i=1}^{n+1} t_{k}\left(y_{i-1},y_{i},x,i\right)\exp\left(\delta_{k}T\left(x,y\right)\right)<br>\\ &amp; = \sum_{x} \tilde{P}\left(x\right) a_{k,t} \exp\left(\delta_k \cdot t\right)<br>\\ &amp; = \sum_{t=0}^{T_{max}} a_{k,t} \beta_{k}^{t}<br>\end{align}<br>其中，$a_{k,t}$是特征$t_{k}$的期望，$\delta_{k}=\log \beta_{k}$，$\beta_{k}$是多项式方程唯一的实根，可以用牛顿法求得，从而得$\delta_{k}$。</p><p>关于状态特征的参数更新方程<br>\begin{align} \\&amp; E_{\tilde{P}} \left[s_{l}\right]= \sum_{x,y} \tilde{P} \left(x\right) P\left(y|x\right) \sum_{i=1}^{n} s_{l}\left(y_{i},x,i\right)\exp\left(\delta_{K_{1}+l}T\left(x,y\right)\right)<br>\\ &amp; = \sum_{x} \tilde{P} \left(x\right) \sum_{y} P\left(y|x\right) \sum_{i=1}^{n} s_{l}\left(y_{i},x,i\right)\exp\left(\delta_{K_{1}+l}T\left(x,y\right)\right)<br>\\ &amp; = \sum_{x} \tilde{P} \left(x\right) b_{l,t} \exp \left(\delta_{k} \cdot t\right)<br>\\ &amp; = \sum_{t=0}^{T_{max}} b_{l,t} \gamma_{l}^{t}<br>\end{align}<br>其中，$b_{l,t}$是特征$s_{l}$的期望，$\delta_{l}=\log \gamma_{l}$，$\gamma_{l}$是多项式方程唯一的实根，可以用牛顿法求得。</p><p>条件随机场的预测问题是给定条件随机场$P\left(Y|X\right)$和输入序列（观测序列）$x$，求条件概率最大的输出序列（标记序列）$y^{*}$，即对观测序列进行标注。<br>即，</p><script type="math/tex; mode=display">\begin{align}  y^{*}&=\arg\max_{y}P_{w}\left(y|x\right)\\ & = \arg\max_{y} \dfrac{\exp \left(w \cdot F\left(y,x\right)\right)}{Z_{w}\left(x\right)}\\ & = \arg\max_{y} \exp \left(w \cdot F\left(y,x\right)\right)\\ & = \arg\max_{y} \left(w \cdot F\left(y,x\right)\right) \end{align}</script><p>其中，<br>\begin{align} w &amp;=\left(w_{1},w_{2},\cdots,w_{K}\right)^{T}<br>\\ F\left(y,x\right)&amp;=\left(f_{1}\left(y,x\right),f_{2}\left(y,x\right),\cdots,f_{K}\left(y,x\right)\right)^{T}<br>\\ f_{k}\left(y,x\right)&amp;=\sum_{i=1}^{n} f_{k} \left(y_{i-1},y_{i},x,i\right),\quad \quad k=1,2,\cdots,K<br>\end{align}<br>等价的<br>\begin{align} \\&amp; \max_{y} \sum_{i=1}^{n} w \cdot F_{i} \left(y_{i-1},y_{i},x\right)\end{align}<br>其中，<br>\begin{align} \\&amp; F_{i}\left(y_{i-1},y_{i},x\right) = \left(f_{1}\left(y_{i-1},y_{i},x,i\right),f_{2}\left(y_{i-1},y_{i},x,i\right),\cdots,f_{K}\left(y_{i-1},y_{i},x,i\right)\right)^{T}\end{align}</p><p>条件随机场预测的维特比算法：<br>输入：模型特征函数$F\left(y,x\right)$和权值向量$w$，观测序列$x=\left(x_{1},x_{2},\cdots,x_{n}\right)$<br>输出：最优序列$y^{<em>}=\left(y_{1}^{</em>},y_{2}^{<em>},\cdots,y_{n}^{</em>}\right)$ </p><ol><li>初始化<br>\begin{align} \\&amp; \delta_{1}\left(j\right)=w \cdot F_{1}\left(y_{0}=start,y_{1}=j,x\right),\quad j=1,2,\cdots,m\end{align}</li><li>递推，对$i=2,3,\cdots,n$<br>\begin{align} \\&amp; \delta_{i}\left(l\right)=\max_{1 \leq j \leq m}\left\{\delta_{i-1}\left(j\right)+w \cdot F_{i}\left(y_{i-1}=j,y_{i}=l,x\right)\right\},\quad l=1,2,\cdots,m<br>\\ &amp; \varPsi_{i}\left(l\right) = \arg \max_{1 \leq j \leq m}\left\{\delta_{i-1}\left(j\right)+w \cdot F_{i}\left(y_{i-1}=j,y_{i}=l,x\right)\right\},\quad l=1,2,\cdots,m\end{align}</li><li>终止<br>\begin{align} \\&amp; \max_{y} \left(w \cdot F\left(y,x\right)\right)=\max_{1 \leq j \leq m} \delta_{n}\left(j\right)<br>\\ &amp; y_{n}^{*}=\arg \max_{1 \leq j \leq m}\delta_{n}\left(j\right) \end{align}</li><li>返回路径<script type="math/tex; mode=display">y_{i}^{*}=\varPsi_{i+1}\left(y_{i+1}^{*}\right),\quad i=n-1,n-2,\cdots,1</script>最优路径<script type="math/tex; mode=display">y_{i}^{*}=\left(y_{1}^{*},y_{2}^{*},\cdots,y_{n}^{*}\right)</script></li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>HMM</title>
    <link href="/2020/05/05/HMM/"/>
    <url>/2020/05/05/HMM/</url>
    
    <content type="html"><![CDATA[<h1 id="隐马尔科夫模型（Hidden-Markov-Model，HMM）"><a href="#隐马尔科夫模型（Hidden-Markov-Model，HMM）" class="headerlink" title="隐马尔科夫模型（Hidden Markov Model，HMM）"></a>隐马尔科夫模型（Hidden Markov Model，HMM）</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><h2 id="1-隐马尔科夫模型定义"><a href="#1-隐马尔科夫模型定义" class="headerlink" title="1 隐马尔科夫模型定义"></a>1 隐马尔科夫模型定义</h2><p>状态集合<script type="math/tex">Q=\left\{q_{1},q_{2},\ldots ,q_{N}\right\}</script></p><p>观测集合\begin{align} &amp; V=\left\{v_{1},v_{2},\ldots ,v_{M}\right\} \quad  \left| V\right| =M  \end{align}  </p><p>状态序列\begin{align} &amp; I=\left\{i_{1},i_{2},\ldots ,i_{t},\ldots,i_{T}\right\}  \quad i_{t}\in Q  \quad \left(t=1,2,\ldots,T \right)\end{align}  </p><p>观测序列\begin{align} &amp; O=\left\{o_{1},o_{2},\ldots ,o_{t},\ldots,o_{T}\right\}  \quad o_{t}\in V \quad \left(t=1,2,\ldots,T \right)\end{align}  </p><p>状态转移矩阵 \begin{align} &amp; A=\left[a_{ij}\right]_{N\times N} \end{align}</p><p>在$t$时刻处于状态$q_{i}$的条件下，在$t+1$时刻转移到状态$q_{j}$的概率\begin{align} &amp; a_{ij}= P\left( i_{t+1}=q_{j}|i_{t}=q_{i}\right) \quad \left(i=1,2,\ldots,N \right) \quad \left(j=1,2,\ldots,M \right)\end{align}</p><p>观测概率矩阵\begin{align} &amp; B=\left[b_{j}\left(k\right)\right]_{N\times M} \end{align}</p><p>在$t$时刻处于状态$q_{i}$的条件下，生成观测$v_{k}$的概率\begin{align} &amp; b_{j}\left(k\right)= P\left( o_{t}=v_{k}|i_{t}=q_{j}\right) \quad \left(k=1,2,\ldots,M \right) \quad \left(j=1,2,\ldots,N \right)\end{align}</p><p>初始概率向量\begin{align} &amp; \pi =\left( \pi _{i}\right)  \end{align}</p><p>在时刻$t=1$处于状态$q_{i}$的概率\begin{align} &amp; \pi_{i} =P\left( i_{1}=q_{i}\right) \quad \left(i=1,2,\ldots,N \right) \end{align}</p><p>隐马尔科夫模型\begin{align} &amp; \lambda =\left( A,B.\pi \right)  \end{align}</p><p>隐马尔科夫模型基本假设：</p><ol><li>齐次马尔科夫性假设：在任意时刻$t$的状态只依赖于时刻$t-1$的状态。\begin{align} &amp; P\left( i_{t}|i_{t-1},o_{t-1},\ldots,i_{1},o_{1}\right)=P\left(i_{t}|i_{t-1}\right) \quad \left(t=1,2,\ldots,T\right) \end{align}</li><li>观测独立性假设：任意时刻$t$的观测只依赖于时刻$t$的状态。\begin{align} &amp; P\left( o_{t}|i_{T},o_{T},i_{T-1},o_{T-1},\ldots,i_{t+1},o_{t+1},i_{t},i_{t-1},o_{t-1},\ldots,i_{1},o_{1}\right)=P\left(o_{t}|i_{t}\right) \quad \left(t=1,2,\ldots,T\right) \end{align}</li></ol><p>观测序列生成算法:<br>输入：隐马尔科夫模型$\lambda =\left( A,B.\pi \right)$,观测序列长度$T$;<br>输出：观测序列$O=\left\{o_{1},o_{2},\ldots ,o_{t},\ldots,o_{T}\right\}$；</p><ol><li>由初始概率向量$\pi$产生状态$i_{1}$；</li><li>$t=1$；</li><li>由状态$i_{t}$的观测概率分布$b_{j}\left(k\right)$生成$o_{t}$；</li><li>由状态$i_{t}$的状态转移概率分布$a_{i_{t}i_{t+1}}$生成状态$i_{t+1} \quad \left(i_{t+1}=1,2,\ldots,N\right)$；  </li><li>$t=t+1$；如果$t&lt;T$，转至3.；否则，结束。</li></ol><p>隐马尔科夫模型的3个基本问题：  </p><ol><li>概率计算：已知$\lambda =\left( A,B.\pi \right)$和$O=\left\{o_{1},o_{2},\ldots ,o_{t},\ldots,o_{T}\right\} $，计算$P\left(O| \lambda \right)$</li><li>学习：已知$O=\left\{o_{1},o_{2},\ldots ,o_{t},\ldots,o_{T}\right\}$，计算 $\lambda^* =\arg \max P\left( O|\lambda \right) $</li><li>预测（编码）：已知$\lambda =\left( A,B.\pi \right)$和$O=\left\{o_{1},o_{2},\ldots ,o_{t},\ldots,o_{T}\right\} $，计算 $I^* =\arg \max P\left( I|O \lambda \right) $</li></ol><h2 id="2-概率计算算法"><a href="#2-概率计算算法" class="headerlink" title="2 概率计算算法"></a>2 概率计算算法</h2><p>前向概率 \begin{align} &amp; \alpha _{t}\left( i\right) =P\left(o_{1},o_{2},\ldots ,o_{t}, i_{t}=q_{i}| \lambda \right) \end{align}<br>给定模型$\lambda$，时刻$t$部分观测序列为$o_{1},o_{2},\ldots ,o_{t}$且状态为$q_{i}$的概率。</p><p>前向概率递推计算\begin{align} &amp; \alpha _{t}\left( i\right) =P\left(o_{1},o_{2},\ldots ,o_{t}, i_{t}=q_{i}| \lambda \right)＝P\left(i_{t}=q_{i},o_{1}^t \right) \\ &amp; =\sum _{j=1}^{N}P\left(i_{t-1}=q_{j},i_{t}=q_{i},o_{1}^{t-1},o_{t}\right) \\ &amp; =\sum _{j=1}^{N}P\left(i_{t}=q_{i},o_{t}|i_{t-1}=q_{j},o_{1}^{t-1}\right)\cdot P\left(i_{t-1}=q_{j},o_{1}^{t-1} \right) \\ &amp; =\sum _{j=1}^{N}P\left(i_{t}=q_{i},o_{t}|i_{t-1}=q_{j}\right)\cdot \alpha _{t-1}\left( j\right)\\ &amp; =\sum _{j=1}^{N}P\left(o_{t}|i_{t}=q_{i},i_{t-1}=q_{j}\right)\cdot P\left(i_{t}=q_{i}|i_{t-1}=q_{j}\right)\cdot \alpha _{t-1}\left( j\right) \\ &amp; =\sum _{j=1}^{N}b_{i}\left(o_{t}\right)\cdot a_{ji}\cdot \alpha _{t-1}\left( j\right)\end{align}</p><p>概率计算\begin{align} &amp; P\left(O| \lambda \right) =P\left(o_{1}^{T}| \lambda\right) \\ &amp; = \sum_{i=1}^{N}P\left(o_{1}^{T},i_{T}=q_{i}\right)\\ &amp; = \sum _{i=1}^{N}\alpha _{T}\left( i\right)\end{align} </p><p>观测序列概率计算的前向算法：<br>输入：隐马尔科夫模型$\lambda$,观测序列$O$;<br>输出：观测序列概率$P\left(O| \lambda \right)$；</p><ol><li>初值\begin{align} &amp; \alpha _{1}\left( i\right)= \pi_{i}b_{i}\left(o_{1}\right) \quad \left(t=1,2,\ldots,N\right) \end{align}</li><li>递推  对$t=1,2,\ldots,T-1$ \begin{align} &amp; \alpha _{t+1}\left( i\right) =\sum _{j=1}^{N}b_{i}\left(o_{t+1}\right)\cdot a_{ji}\cdot \alpha _{t}\left( j\right) \quad \left(t=1,2,\ldots,N\right) \end{align}</li><li>终止  \begin{align} &amp; P\left(O| \lambda \right)= \sum _{j=1}^{N}\alpha _{T}\left( i\right)\end{align}</li></ol><p>后向概率 \begin{align} &amp; \beta_{t}\left( i\right) =P\left(o_{t+1},o_{t+2},\ldots ,o_{T}| i_{t}=q_{i} \lambda \right) \end{align}<br>给定模型$\lambda$，时刻$t$状态为$q_{i}$的条件下，从时刻$t+1$到时刻$T$的部分观测序列为$o_{t+1},o_{t+2},\ldots ,o_{T}$的概率。</p><p>后向概率递推计算\begin{align} &amp; \beta _{t}\left( i\right) =P\left(o_{t+1},o_{t+2},\ldots ,o_{T}| i_{t}=q_{i}, \lambda \right)＝P\left(o_{t+1}^T |i_{t}=q_{i}\right) \\ &amp; =\dfrac {P\left(o_{t+1}^{T}, i_{t}=q_{i}\right)} {P\left(i_{t}=q_{i}\right)}\\ &amp; =\dfrac {\sum_{j=1}^{N} P\left(o_{t+1}^{T},i_{t}=q_{i},i_{t+1}=q_{j}\right)}{P\left(i_{t}=q_{i}\right)}\\ &amp; =\sum_{j=1}^{N} \dfrac {P\left(o_{t+1}^{T}|i_{t}=q_{i},i_{t+1}=q_{j}\right) \cdot P\left(i_{t}=q_{i},i_{t+1}=q_{j} \right)}{P\left(i_{t}=q_{i}\right)} \\ &amp; = \sum_{j=1}^{N} P\left(o_{t+1}^{T}|i_{t+1}=q_{j}\right) \cdot \dfrac {P\left(i_{t+1}=q_{j}|i_{t}=q_{i}\right)  \cdot P\left(i_{t}=q_{i} \right)}{P\left(i_{t}=q_{i} \right)} \\ &amp; = \sum_{j=1}^{N} P\left(o_{t+2}^{N},o_{t+1}|i_{t+1}=q_{j}\right) \cdot a_{ij} \\ &amp; = \sum_{j=1}^{N} P\left(o_{t+2}^{T}|i_{t+1}=q_{j}\right) \cdot P\left(o_{t+1}|i_{t+1}=q_{j}\right) \cdot a_{ij} \\ &amp; = \sum_{j=1}^{N} \beta_{t+1}\left(j\right) \cdot b_{j}\left(o_{t+1}\right) \cdot a_{ij}\end{align}</p><p>概率计算\begin{align} &amp; P\left(O| \lambda \right) =P\left(o_{1}^{T}| \lambda\right) \\ &amp; = \sum_{i=1}^{N}P\left(o_{1}^{T},i_{1}=q_{i}\right)\\ &amp; = \sum_{i=1}^{N}P\left(i_{1}=q_{i}\right) \cdot P\left(o_{1}|i_{1}=q_{i}\right)\cdot P\left(o_{2}^{T}|i_{1}=q_{i}\right) \\ &amp; = \sum_{i=1}^{N} \pi_{i} b_{i}\left(o_{1}\right) \beta_{1}\left(i\right)\end{align} </p><p>观测序列概率计算的后向算法：<br>输入：隐马尔科夫模型$\lambda$,观测序列$O$;<br>输出：观测序列概率$P\left(O| \lambda \right)$；</p><ol><li>初值\begin{align} &amp; \beta_{T}\left( i\right)= 1 \quad \left(t=1,2,\ldots,N\right) \end{align}</li><li>递推  对$t=T-1,T-2,\ldots,1$ \begin{align} &amp; \beta_{t}\left( i\right) =\sum_{j=1}^{N} \beta_{t+1}\left(j\right) \cdot b_{j}\left(o_{t+1}\right) \cdot a_{ij} \quad \left(t=1,2,\ldots,N\right) \end{align}</li><li>终止  \begin{align} &amp; P\left(O| \lambda \right)= \sum _{j=1}^{N}\pi_{i} b_{i}\left(o_{1}\right)\beta _{1}\left( i\right) \end{align}</li></ol><p>$P \left( O | \lambda \right)$的前向概率、后向概率的表示<br>\begin{align} \\ &amp; P \left( O | \lambda \right) ＝ P \left( o_{1}^{T} \right)<br>\\ &amp; ＝ \sum_{i=1}^{N} \sum_{j=1}^{N} P \left( o_{1}^{t}, o_{t+1}^{T}, i_{t}=q_{i}, i_{t+1}=q_{j} \right)<br>\\ &amp; ＝ \sum_{i=1}^{N} \sum_{j=1}^{N} P \left( o_{1}^{t},  i_{t}=q_{i}, i_{t+1}=q_{j} \right) P \left( o_{t+1}^{T} | i_{t+1}=q_{j} \right)<br>\\ &amp;  = \sum_{i=1}^{N} \sum_{j=1}^{N} P \left( o_{1}^{t},  i_{t}=q_{i} \right) P \left( i_{t+1}=q_{j} | i_{t}=q_{i} \right) P \left( o_{t+1}^{T} | i_{t+1}=q_{j} \right)<br>\\ &amp; = \sum_{i=1}^{N} \sum_{j=1}^{N} P \left( o_{1}^{t},  i_{t}=q_{i} \right) P \left( i_{t+1}=q_{j} | i_{t}=q_{i} \right) P \left( o_{t+1} | i_{t+1}=q_{j} \right) P \left( o_{t+2}^{T} | i_{t+1}=q_{j} \right)<br>\\ &amp; = \sum_{i=1}^{N} \sum_{j=1}^{N} \alpha_{t} \left( i \right) a_{ij} b_{j} \left( o_{t+1} \right) \beta_{t+1} \left( j \right) \quad \quad \quad t=1, 2, \cdots, T-1\end{align}  </p><p>给定模型$\lambda$和观测$O$，在时刻$t$处于状态$q_{i}$的概率<br>\begin{align} \\ &amp; \gamma_{t} \left( i \right) = P \left( i_{t}=q_{i} | O, \lambda \right)<br>\\ &amp; =  \dfrac{ P \left( i_{t}=q_{i}, O | \lambda \right) } { P \left( O | \lambda \right) }<br>\\ &amp; = \dfrac{ P \left( i_{t}=q_{i}, O | \lambda \right) } { \sum_{j=1}^{N} \left( i_{t}=q_{i}, O | \lambda \right) }<br>\\ &amp; = \dfrac{ P \left( o_{1}^{t}, i_{t}=q_{i} \right) P \left( o_{t+1}^{T}| i_{t}=q_{i} \right) } { \sum_{j=1}^{N} P \left( o_{1}^{t}, i_{t}=q_{i} \right) P \left( o_{t+1}^{T}| i_{t}=q_{i} \right) }<br>\\ &amp; = \dfrac{ \alpha_{t} \left( i \right) \beta_{t} \left( i \right)} { \sum_{j=1}^{N}  \alpha_{t} \left( i \right) \beta_{t} \left( i \right) }\end{align}</p><p>给定模型$\lambda$和观测$O$，在时刻$t$处于状态$q_{i}$且在时刻$t+1$处于状态$q_{j}$的概率<br>\begin{align} \\ &amp; \xi_{t} \left( i,j \right) = P \left( i_{t}=q_{i}, i_{t+1}=q_{j} | O ,\lambda \right)<br>\\ &amp; = \dfrac{ P \left( i_{t}=q_{i}, i_{t+1}=q_{j},O | \lambda \right) } { P \left( O | \lambda \right) }<br>\\ &amp; = \dfrac{ P \left( i_{t}=q_{i}, i_{t+1}=q_{j}, O | \lambda \right) } { \sum_{i=1}^{N} \sum_{j=1}^{N}  P \left( i_{t}=q_{i}, i_{t+1}=q_{j}, O|\lambda \right) }<br>\\ &amp; = \dfrac{ \alpha_{t} \left( i \right) a_{ij} b_{j} \left( o_{t+1} \right) \beta_{t+1} \left( j \right) } { \sum_{i=1}^{N} \sum_{j=1}^{N}  \alpha_{t} \left( i \right) a_{ij} b_{j} \left( o_{t+1} \right) \beta_{t+1} \left( j \right)}\end{align}</p><p>在观测$O$下状态$i$出现的期望\begin{align} &amp; \sum_{t=1}^{T} \gamma_{t} \left( i \right) =  \sum_{t=1}^{T} P \left( i_{t}=q_{i} | O, \lambda \right) \end{align}</p><p>在观测$O$下由状态$i$转移的期望\begin{align} &amp; \sum_{t=1}^{T－1} \gamma_{t} \left( i \right) =  \sum_{t=1}^{T－1} P \left( i_{t}=q_{i} | O, \lambda \right) \end{align}</p><p>在观测$O$下由状态$i$转移到状态$j$的期望\begin{align} &amp; \sum_{t=1}^{T－1} \xi_{t} \left( i,j \right) =  \sum_{t=1}^{T－1} P \left( i_{t}=q_{i}, i_{t+1}=q_{j} | O, \lambda \right) \end{align}</p><h2 id="3-学习算法"><a href="#3-学习算法" class="headerlink" title="3 学习算法"></a>3 学习算法</h2><p>将观测序列作为观测数据$O$,将状态序列作为隐数据$I$，则隐马尔科夫模型是含有隐变量的概率模型<br> \begin{align} &amp; P \left( O | \lambda \right) = \sum_{I} P \left( O | I, \lambda \right) P \left( I | \lambda \right)\end{align}  </p><p>完全数据<br> \begin{align} &amp; \left( O, I \right) = \left(o_{1}, o_{2}, \cdots, o_{T}, i_{1}, i_{2}, \cdots, o_{T} \right)\end{align}  </p><p>完全数据的对数似然函数<br>\begin{align} &amp; \log P \left( O, I | \lambda \right) \end{align}  </p><p>$Q \left( \lambda, \overline{\lambda} \right)$函数<br>\begin{align} \\&amp; Q \left( \lambda, \overline{\lambda} \right) = E_{I} \left[ \log P \left( O, I | \lambda \right) | O, \overline{\lambda} \right]<br>\\ &amp; = \sum_{I} \log P \left( O, I | \lambda \right) P \left( I | O, \overline{\lambda} \right)<br>\\ &amp; = \sum_{I} \log \dfrac{P \left( O, I | \lambda \right) P \left( O, I | \overline{\lambda} \right) }{P \left( O | \overline{\lambda} \right)}\end{align}<br>其中，$\overline{\lambda}$是隐马尔科夫模型参数的当前估计值，$\lambda$是隐马尔科夫模型参数。</p><p>由于对最大化$Q \left( \lambda, \overline{\lambda} \right)$函数，$P \left( O | \overline{\lambda} \right)$为常数因子，<br>以及\begin{align} &amp; P \left( O, I | \lambda \right) = \pi_{i_{1}} b_{i_{1}} \left( o_{1} \right) a_{i_{1}i_{2}} b_{i_{2}} \left( o_{2} \right) \cdots a_{i_{T-1}i_{T}}b_{T}\left( o_{T} \right)\end{align}<br>所以求$Q \left( \lambda, \overline{\lambda} \right)$函数对$\lambda$的最大\begin{align} &amp; \lambda = \arg \max{Q \left( \lambda, \overline{\lambda} \right) }\Leftrightarrow \arg\max \sum_{I} \log P \left( O, I | \lambda \right) P \left( O, I | \overline{\lambda} \right)<br>\\ &amp;  = \sum_{I} \log \pi_{i_{1}} P \left( O, I | \overline{\lambda} \right) + \sum_{I} \left( \sum_{t=1}^{T-1} \log a_{i_{t}i_{t+1}} \right) P \left( O, I | \overline{\lambda} \right) + \sum_{I} \left( \sum_{t=1}^{T} \log b_{i_{t}} \left( o_{t} \right) \right) P \left( O, I | \overline{\lambda} \right)\end{align}    </p><p>对三项分别进行极大化：  </p><ol><li>\begin{align} &amp; \max \sum_{I} \log \pi_{i_{1}} P \left( O, I | \overline{\lambda} \right) = \sum_{i=1}^{N} \log \pi_{i_{1}} P \left( O, i_{1}=i | \overline{\lambda} \right)<br>\\ &amp; s.t. \sum_{i=1}^{N} \pi_{i} = 1 \end{align}<br>构造拉格朗日函数，对其求偏导，令结果为0 \begin{align} &amp; \dfrac{\partial}{\partial \pi_{i}} \left[ \sum_{i=1}^{N} \log \pi_{i_{1}} P \left( O, i_{1}=i | \overline{\lambda} \right) + \gamma \left( \sum_{i=1}^{N} \pi_{i} - 1 \right) \right] = 0\end{align}<br>得\begin{align} &amp; P \left( O, i_{1} = i | \overline{\lambda} \right) + \gamma \pi_{i} = 0<br>\\ &amp; \sum_{i=1}^{N} \left[ P \left( O, i_{1} = i | \overline{\lambda} \right) + \gamma \pi_{i} \right] = 0<br>\\ &amp; \sum_{i=1}^{N} P \left( O, i_{1} = i | \overline{\lambda} \right) + \gamma \sum_{i=1}^{N} \pi_{i}  = 0<br>\\ &amp; P \left( O | \overline{\lambda} \right) + \gamma  = 0<br>\\ &amp; \gamma  = - P \left( O | \overline{\lambda} \right)\end{align}<br>代入$P \left( O, i_{1} = i | \overline{\lambda} \right) + \gamma \pi_{i} = 0$，得<br>\begin{align} &amp; \pi_{i} = \dfrac{P \left( O, i_{1} = i | \overline{\lambda} \right)}{P \left( O | \overline{\lambda} \right)}<br>\\ &amp; = \gamma_{1} \left( i \right) \end{align}    </li><li>\begin{align} \\ &amp; \max \sum_{I} \left( \sum_{t=1}^{T-1} \log a_{i_{t}i_{t+1}} \right) P \left( O, I | \overline{\lambda} \right) = \sum_{i=1}^{N} \sum_{j=1}^{N} \sum_{t=1}^{T-1} \log a_{ij} P \left( O, i_{t}=i, i_{t+1}=j | \overline{\lambda} \right)<br>\\ &amp; s.t. \sum_{j=1}^{N} a_{ij} = 1 \end{align}<br>得\begin{align} \\ &amp; a_{ij} = \dfrac{\sum_{t=1}^{T-1} P \left( O, i_{t}=i, i_{t+1}=j | \overline{\lambda} \right)}{\sum_{t=1}^{T-1} P \left( O, i_{t}=i | \overline{\lambda} \right)}<br>\\ &amp; = \dfrac{\sum_{t=1}^{T-1} \xi_{t} \left( i,j \right) }{\sum_{t=1}^{T-1} \gamma_{t} \left( i \right)}\end{align}   </li><li>\begin{align} \\ &amp; \max \sum_{I} \left( \sum_{t=1}^{N} \log b_{i_{t}} \left( o_{t} \right) \right) P \left( O, I | \overline{\lambda} \right) = \sum_{j=1}^{N} \sum_{t=1}^{T} \log b_{j} \left( o_{t} \right) P \left( O, i_{t}=j | \overline{\lambda} \right)<br>\\ &amp; s.t. \sum_{k=1}^{M} b_{j} \left( k \right) = 1 \end{align}<br>得\begin{align} \\ &amp; b_{j} \left( k \right) = \dfrac{\sum_{t=1}^{T} P \left( O, i_{t}=j | \overline{\lambda} \right) I \left( o_{t} = v_{k} \right)}{\sum_{t=1}^{T} P \left( O, i_{t}=j | \overline{\lambda} \right)}<br>\\ &amp; = \dfrac{ \sum_{t=1,o_{t}=v_{k}}^{T} \gamma_{t} \left( j \right)}{\sum_{t=1}^{T} \gamma_{t} \left( j \right)}\end{align}</li></ol><p>Baum-Welch算法：<br>输入：观测数据$O = \left( o_{1}, o_{2}, \cdots, o_{T} \right)$<br>输出：隐马尔科夫模型参数  </p><ol><li>初始化<br>对$n=0$，选取$a_{ij}^{ \left( 0 \right) },b_{j} \left( k \right)^{\left( 0 \right)},\pi_{i}^{\left( 0 \right)}$，得到模型$\lambda^{\left( 0 \right)} = \left( a_{ij}^{ \left( 0 \right) },b_{j} \left( k \right)^{\left( 0 \right)},\pi_{i}^{\left( 0 \right)} \right)$  </li><li>递推<br>对$n=1,2, \cdots,$<br>\begin{align} \\ &amp; a_{ij}^{\left( n+1 \right)} = \dfrac{\sum_{t=1}^{T-1} \xi_{t} \left( i,j \right) }{\sum_{t=1}^{T-1} \gamma_{t} \left( i \right)}<br>\\ &amp; b_{j} \left( k \right)^{\left( n+1 \right)} = \dfrac{ \sum_{t=1,o_{t}=v_{k}}^{T} \gamma_{t} \left( j \right)}{\sum_{t=1}^{T} \gamma_{t} \left( j \right)}<br>\\ &amp; \pi_{i}^{\left( n+1 \right)} = \dfrac{P \left( O, i_{1} = i | \overline{\lambda} \right)}{P \left( O | \overline{\lambda} \right)} \end{align}<br>其中，右端各值按观测数据$O = \left( o_{1}, o_{2}, \cdots, o_{T} \right)$和模型$\lambda^{\left( n \right)} = \left( A^{\left( n \right)},B^{\left( n \right)},\pi^{\left( n \right)} \right)$计算。  </li><li>终止<br>得到模型$\lambda^{\left( n＋1 \right)} = \left( A^{\left( n+1 \right)},B^{\left( n+1 \right)},\pi^{\left( n+1 \right)} \right)$</li></ol><h2 id="4-预测算法"><a href="#4-预测算法" class="headerlink" title="4 预测算法"></a>4 预测算法</h2><p>在时刻$t$状态为$i$的所有单个路径$\left( i_{1}, i_{2}, \cdots, i_{t} \right)$中概率最大值<br>\begin{align} \\ &amp; \delta_{t} \left( i \right) = \max_{i_{1}, i_{2}, \cdots, i_{t-1}} P \left(i_{t}=i, i_{t-1}, \cdots, i_{1}, o_{t}, \cdots, o_{1} | \lambda \right) \quad \quad \quad i = 1, 2, \cdots, N  \end{align}</p><p>得递推公式\begin{align} \\ &amp; \delta_{t+1} \left( i \right) = \max_{i_{1}, i_{2}, \cdots, i_{t}} P \left(i_{t+1}=i, i_{t}, \cdots, i_{1}, o_{t+1}, \cdots, o_{1} | \lambda \right)<br>\\ &amp; = \max_{1 \leq j \leq N} \left[ \max_{i_{1}, i_{2}, \cdots, i_{t-1}} P \left( i_{t+1}=i, i_{t}=j, i_{t-1}, \cdots, i_{1}, o_{t+1}, o_{t}, \cdots, o_{1} | \lambda \right) \right]<br>\\ &amp; = \max_{1 \leq j \leq N} \left[ \max_{i_{1}, i_{2}, \cdots, i_{t-1}} P \left( i_{t+1}=i, i_{t}=j, i_{t-1}, \cdots, i_{1}, o_{t}, o_{t-1}, \cdots, o_{1} | \lambda \right) P \left( o_{t+1} | i_{t+1}=i, \lambda \right)\right]<br>\\ &amp; = \max_{1 \leq j \leq N} \left[ \max_{i_{1}, i_{2}, \cdots, i_{t-1}} P \left( i_{t}=j, i_{t-1}, \cdots, i_{1}, o_{t}, o_{t-1}, \cdots, o_{1} | \lambda \right) P \left( i_{t+1}=i | i_{t}=j, \lambda \right)P \left( o_{t+1} | i_{t+1}=i, \lambda \right)\right]<br>\\ &amp; =  \max_{1 \leq j \leq N} \left[ \delta_{t} \left( j \right) a_{ji}\right] b_{i} \left( o_{t+1} \right)\quad \quad \quad i = 1, 2, \cdots, N  \end{align}</p><p>在时刻$t$状态为$i$的所有单个路径$\left( i_{1}, i_{2}, \cdots, i_{t} \right)$中概率最大值的路径的第$t-1$个结点<br>\begin{align} \\ &amp; \psi_{t} \left( i \right) = \arg \max_{1 \leq j \leq N} \left[ \delta_{t-1} \left( j \right) a_{ji} \right] \quad \quad \quad i = 1, 2, \cdots, N \end{align}</p><p>维特比算法：<br>输入：模型$\lambda = \left( A, B, \pi \right)$和观测数据$O = \left( o_{1}, o_{2}, \cdots, o_{T} \right)$<br>输出：最优路径$I^{<em>} = \left( i_{1}^{</em>}, i_{2}^{<em>}, \cdots, i_{T}^{</em>} \right)$  </p><ol><li>初始化<br>\begin{align} \\ &amp; \delta_{1} \left( i \right) = \pi_{i} b_{i} \left( o_{1} \right) \quad \quad \quad i = 1, 2, \cdots, N<br>\\ &amp; \psi_{1} \left( i \right) = 0  \end{align}  </li><li>递推<br>对$t=2,3, \cdots, T$<br>\begin{align} \\ &amp; \delta_{t} \left( i \right) = \max_{1 \leq j \leq N} \left[ \delta_{t-1} \left( j \right) a_{ji}\right] b_{i} \left( o_{t} \right)\quad \quad \quad i = 1, 2, \cdots, N<br>\\ &amp; \psi_{t} \left( i \right) = \arg \max_{1 \leq j \leq N} \left[ \delta_{t-1} \left( j \right) a_{ji} \right] \quad \quad \quad i = 1, 2, \cdots, N  \end{align}  </li><li>终止<script type="math/tex; mode=display">\begin{align}& P^* = \max_{1 \leq j \leq N} \delta_{T} \left( i \right) \\& i_{T}^{*} = \arg \max_{1 \leq j \leq N} \left[ \delta_{T} \left( i \right) \right] \end{align}</script></li><li>最优路径回溯<br>对$t=T-1,T-2, \cdots, 1$ <script type="math/tex; mode=display">\begin{align}  & i_{t}^{*} = \psi_{t+1} \left( i_{t+1}^{*} \right) \end{align}</script>求得最优路径<script type="math/tex; mode=display">I^* = \left( i_{1}^*, i_{2}^{*}, \cdots, i_{T}^* \right)</script></li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LDA</title>
    <link href="/2020/05/05/LDA/"/>
    <url>/2020/05/05/LDA/</url>
    
    <content type="html"><![CDATA[<h1 id="潜在狄利克雷分配（latent-Dirichlet-allocation-LDA）"><a href="#潜在狄利克雷分配（latent-Dirichlet-allocation-LDA）" class="headerlink" title="潜在狄利克雷分配（latent Dirichlet allocation, LDA）"></a>潜在狄利克雷分配（latent Dirichlet allocation, LDA）</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><p><img src="https://i.loli.net/2020/05/05/bMnltBuR5DsYwpT.png" srcset="/img/loading.gif"  align=center  width = "450" height = "250" /></p><p>潜在狄利克雷分配模型定义：  </p><p>单词集合</p><script type="math/tex; mode=display">W=\left\{w_1,w_2,\cdots,w_v,\cdots,w_V\right\}</script><p>其中，$w_v$是第$v$个单词，$v=1,2,\cdots,V$，$V$是单词的个数。  </p><p>文本集合  </p><script type="math/tex; mode=display">D=\left\{\mathbf{w}_1,\mathbf{w}_2,\cdots,\mathbf{w}_m,\cdots,\mathbf{w}_M\right\}</script><p>其中，$\mathbf{w}_m$是第$m$个文本，$m=1,2,\cdots,M$，$M$是文本个数。<br>文本$\mathbf{w}_m=\left(w_{m1},w_{m2},\cdots,w_{mn},\cdots,w_{mN_m}\right)$是一个单词序列，其中$w_{mn}$是文本$\mathbf{w}_m$的第$n$个单词，$n=1,2,\cdots,N_m$，$N_m$是文本$\mathbf{w}_m$中的单词个数。  </p><p>话题集合  </p><script type="math/tex; mode=display">Z=\left\{z_1,z_2,\cdots,z_k,\cdots,z_K\right\}</script><p>其中，$z_k$是第$k$个话题，$k=1,2,\cdots,K$，$K$是话题个数。</p><p>每一个话题$z_k$包含的单词由条件概率分布$p\left(w|z_k\right)$决定，$w\in W$。分布$p\left(w|z_k\right)$服从多项分布，其参数为$\varphi_k=\left(\varphi_{k1},\varphi_{k2},\cdots,\varphi_{kV}\right)$是一个$V$维向量，$\varphi_{kv}$表示话题$z_k$生成单词$w_v$的概率。所有话题的参数向量构成一个$K\times V$矩阵，$\boldsymbol{\varphi}=\left\{\varphi_k\right\}_{k=1}^K$。参数$\varphi_k$服从狄利克雷分布，其超参数$\beta=\left(\beta_1,\beta_2,\cdots,\beta_V\right)$也是一个$V$维向量。</p><p>每一个文本$\mathbf{w}_m$包含的话题由条件概率分布$p\left(z|\mathbf{w}_m\right)$决定，$z\in Z$。分布$p\left(z|\mathbf{w}_m\right)$服从多项分布，其参数为$\theta_m=\left(\theta_{m1},\theta_{m2},\cdots,\theta_{mK}\right)$是一个$K$维向量，$\theta_{mk}$表示文本$\mathbf{w}_m$生成话题$z_k$的概率。所有文本的参数向量构成一个$M\times K$矩阵，$\boldsymbol{\theta}=\left\{\theta_m\right\}_{m=1}^M$。参数$\theta_m$服从狄利克雷分布，其超参数$\alpha=\left(\alpha_1,\alpha2,\cdots,\alpha_K\right)$也是一个$K$维向量。</p><p>潜在狄利克雷分配文本集合的生成过程：<br>给定单词集合$W$，文本集合$D$，话题集合$Z$，狄利克雷分布的参数$\alpha$和$\beta$。</p><ol><li>生成话题的单词分布<br>随机生成$K$个话题的单词分布。按照狄利克雷分布$Dir\left(\beta\right)$随机生成一个参数向量$\varphi_k$，$\varphi_k\thicksim Dir\left(\beta\right)$，作为话题$z_k$的单词分布$p\left(w|z_k\right),w\in W,k=1,2,\cdots,K$。</li><li>生成文本的话题分布<br>随机生成$M$个文本的话题分布。按照狄利克雷分布$Dir\left(\alpha\right)$随机生成一个参数向量$\theta_m$，$\theta_m\thicksim Dir\left(\alpha\right)$，作为文本$\mathbf{w}_m$的话题分布$p\left(z|\mathbf{w}_m\right),z\in Z,m=1,2,\cdots,M$。</li><li>生成文本的单词序列<br>随机生成$M$个文本的$N_m$个单词。文本$\mathbf{w}_m\left(m=1,2,\cdots,M\right)$的单词$w_{mn}\left(n=1,2,\cdots,N_m\right)$的生成过程：<br>a. 按照多项分布$Mult\left(\theta_m\right)$随机生成一个话题$z_{mn},z_{mn}\thicksim Mult\left(\theta_m\right)$。<br>b. 按照多项分布$Mult\left(\varphi_{z_{mn}}\right)$随机生成一个单词$w_{mn},w_{mn}\thicksim Mult\left(\varphi_{z_{mn}}\right)$。</li></ol><p>文本$\mathbf{w}_m$本身是单词序列$\mathbf{w}_m=\left(w_{m1},w_{m2},\cdots,w_{mN_m}\right)$，对应隐式的话题序列$\mathbf{z}_m=\left(z_{m1},z_{m2},\cdots,z_{mN_m}\right)$。</p><p>LDA文本生成算法：<br>输入：单词集合$W$，话题集合$Z$，狄利克雷函数分布的参数$\alpha$和$\beta$；<br>输出：文本集合$D$;  </p><ol><li>对于话题$z_k\left(k=1,2,\cdots,K\right)$:<br>生成多项分布参数$\varphi_k\thicksim Dir\left(\beta\right)$，作为话题的单词分布$p\left(w|z_k\right)$;</li><li>对于文本$\mathbf{w}_m\left(m=1,2,\cdots,M\right)$:<br>生成所想分布参数$\theta_m\thicksim Dir\left(\alpha\right)$，作为文本的话题分布$p\left(z|\mathbf{w}_m\right)$;</li><li>对于文本$\mathbf{w}_m$的单词$w_{mn}\left(m=1,2,\cdots,M;n=1,2,\cdots,N_m\right)$:<br>a. 生成话题$z_{mn}\thicksim Mult\left(\theta_m\right)$，作为单词对应的话题；<br>b. 生成单词$w_{mn}\thicksim Mult\left(\varphi_{z_{mn}}\right)$。</li></ol><p>LDA的文本生成过程中，假定话题个数$K$给定，实际通常通过实验选定。狄利克雷分布的参数$\alpha$和$\beta$通常也是事先给定。在没有其他先验知识的情况下，可以假设向量$\alpha$和$\beta$的所有分量均为1。</p><p>LDA模型整体是由观测变量和隐变量组成的联合概率分布</p><script type="math/tex; mode=display">p\left(\mathbf{w},\mathbf{z},\theta,\varphi|\alpha,\beta\right)=\prod_{k=1}^K p\left(\varphi_k|\beta\right)\prod_{m=1}^M p\left(\theta_m|\alpha\right)\prod_{n=1}^{N_m}p\left(z_{mn}|\theta_m\right)p\left(w_{mn}|z_{mn},\varphi\right)</script><p>超参数$\alpha$和$\beta$给定条件下文本和话题的联合概率</p><script type="math/tex; mode=display">p\left(\mathbf{w},\mathbf{z}|\alpha,\beta\right)=p\left(\mathbf{w}|\mathbf{z},\alpha,\beta\right)p\left(\mathbf{z}|\alpha,\beta\right)=p\left(\mathbf{w}|\mathbf{z},\beta\right)p\left(\mathbf{z}|\alpha\right)</script><p>由于</p><script type="math/tex; mode=display">p\left(\mathbf{w}|\mathbf{z},\varphi\right)=\prod_{k=1}^K\prod_{v=1}^V\varphi_{kv}^{n_{kv}}</script><p>其中，$\varphi_{kv}$是第$k$个话题生成的单词集合中第$v$个单词的概率，$n_{kv}$是数据中第$k$个话题生成第$v$个单词的次数。<br>所以</p><script type="math/tex; mode=display">\begin{align}p\left(\mathbf{w}|\mathbf{z},\beta\right) &= \int p\left(\mathbf{w}|\mathbf{z},\varphi\right) p\left(\varphi|\beta\right)\mathrm{d}\varphi  \\&= \int \prod_{k=1}^K\frac{1}{B\left(\beta\right)}\prod_{v=1}^V\varphi_{kv}^{n_{kv}+\beta_v-1}\mathrm{d}\varphi \\&= \prod_{k=1}^K\frac{1}{B\left(\beta\right)}\int\prod_{v=1}^V\varphi_{kv}^{n_{kv}+\beta_v-1}\mathrm{d}\varphi \\&= \prod_{k=1}^K\frac{B\left(n_k+\beta\right)}{B\left(\beta\right)}\end{align}</script><p>其中，$n_k=\left(n_{k1},n_{k2},\cdots,n_{kV}\right)$。</p><p>由于</p><script type="math/tex; mode=display">p\left(\mathbf{z}|\theta\right)=\prod_{m=1}^M\prod_{k=1}^K\varphi_{mk}^{n_{mk}}</script><p>其中，$\theta_{mk}$是第$m$个文本生成的第$k$个话题的概率，$n_{mk}$是数据中第$m$个文本生成第$k$个话题的次数。<br>所以</p><script type="math/tex; mode=display">\begin{align}p\left(\mathbf{z}|\alpha\right) &= \int p\left(\mathbf{z}|\theta\right) p\left(\theta|\alpha\right)\mathrm{d}\theta  \\&= \int \prod_{m=1}^M\frac{1}{B\left(\beta\right)}\prod_{k=1}^K\theta_{mk}^{n_{mk}+\alpha_k-1}\mathrm{d}\theta \\&= \prod_{m=1}^M\frac{1}{B\left(\beta\right)}\int\prod_{k=1}^K\theta_{mk}^{n_{mk}+\alpha_k-1}\mathrm{d}\theta \\&= \prod_{m=1}^M\frac{B\left(n_m+\alpha\right)}{B\left(\alpha\right)}\end{align}</script><p>其中，$n_m=\left(n_{m1},n_{m2},\cdots,n_{mV}\right)$。</p><p>由以上，得</p><script type="math/tex; mode=display">p\left(\mathbf{w},\mathbf{z}|\alpha,\beta\right)=\prod_{k=1}^K\frac{B\left(n_k+\beta\right)}{B\left(\beta\right)} \cdot \prod_{m=1}^M\frac{B\left(n_m+\alpha\right)}{B\left(\alpha\right)}</script><p>由于</p><script type="math/tex; mode=display">p\left(\mathbf{z}|\mathbf{w},\alpha,\beta\right)=\frac{p\left(\mathbf{w},\mathbf{z}|\alpha,\beta\right)}{p\left(\mathbf{w}|\alpha,\beta\right)}\propto p\left(\mathbf{w},\mathbf{z}|\alpha,\beta\right)</script><p>所以吉布斯抽样分布</p><script type="math/tex; mode=display">p\left(\mathbf{z}|\mathbf{w},\alpha,\beta\right)\propto \prod_{k=1}^K\frac{B\left(n_k+\beta\right)}{B\left(\beta\right)} \cdot \prod_{m=1}^M\frac{B\left(n_m+\alpha\right)}{B\left(\alpha\right)}</script><p>由于$w_i$是可观测到的，所以分布$p\left(\mathbf{z}|\mathbf{w},\alpha,\beta\right)$的满条件分布</p><script type="math/tex; mode=display">\begin{align}p\left(z_i|\mathbf{z}_{-i},\mathbf{w},\alpha,\beta\right) &\propto p\left(z_i,w_i|\mathbf{z}_{-i},\mathbf{w}_{-i},\alpha,\beta\right)  \\&=\iint p\left(z_i,w_i,\theta_m,\varphi_k|\mathbf{z}_{-i},\mathbf{w}_{-i},\alpha,\beta\right)\mathrm{d}\theta_m\mathrm{d}\varphi_k \\&= \iint p\left(z_i,\theta_m|\mathbf{z}_{-i},\mathbf{w}_{-i},\beta\right) p\left(w_i,\varphi_k|\mathbf{z}_{-i},\mathbf{w}_{-i},\alpha\right)\mathrm{d}\theta_m\mathrm{d}\varphi_k \\&= \iint p\left(z_i|\theta_m\right)p\left(\theta_m|\mathbf{z}_{-i},\mathbf{w}_{-i},\beta\right)p\left(w_i|\varphi_k\right)p\left(\varphi_k|\mathbf{z}_{-i},\mathbf{w}_{-i},\alpha\right)\mathrm{d}\theta_m\mathrm{d}\varphi_k  \\&= \int p\left(z_i|\theta_m\right) Dir\left(\theta_m|n_m+\alpha\right)\mathrm{d}\theta_m \cdot \int p\left(w_i|\varphi_k\right)Dir\left(\varphi_k|n_k+\beta\right)\mathrm{d}\varphi_k  \\&= \int \theta_{mk} Dir\left(\theta_m|n_m+\alpha\right)\mathrm{d}\theta_m \cdot \int \varphi_{kv} Dir\left(\varphi_k|n_k+\beta\right)\mathrm{d}\varphi_k \\&= E_{Dir\left(\theta_m|n_m+\alpha\right)}\left[\theta_m\right]\cdot E_{Dir\left(\varphi_k|n_k+\beta\right)}\left[\varphi_k\right]  \\&= \frac{n_{mk}+\alpha_k}{\sum_{k=1}^K\left(n_{mk}+\alpha_k\right)}\cdot\frac{n_{kv}+\beta_v}{\sum_{v=1}^V\left(n_{kv}+\beta_v\right)}\end{align}</script><p>其中，$w_i$表示所有文本的单词序列的第$i$个位置的单词，是单词集合中的第$v$个单词；$z_i$表示单词$w_i$对应的话题，是话题集合中的第$k$个话题；$i=\left(m,n\right)$是二维下标，对应第$m$篇文档的第$n$个单词；$n_{mk}$表示第$m$个文本中第$k$个话题的计数，但减去当前单词的话题的计数；$n_{kv}$表示第$k$个话题中第$v$个话题的计数，但减去当前单词的话题的计数。</p><p>参数$\theta_m$的后验概率</p><script type="math/tex; mode=display">\begin{align}p\left(\theta_m|\mathbf{z}_m,\alpha\right) &= \frac{p\left(\mathbf{z}_m|\theta_m\right)p\left(\theta_m|\alpha\right)}{\int p\left(\mathbf{z}_m|\theta_m\right)p\left(\theta_m|\alpha\right)\mathrm{d}\theta} \\&=\frac{\prod_{k=1}^K\frac{1}{B\left(\alpha\right)}\theta_{mk}^{\alpha_{mk}-1}\cdot\prod_{k=1}^K\theta_{mk}^{n_{mk}}}{\int \prod_{k=1}^K\frac{1}{B\left(\alpha\right)}\theta_{mk}^{\alpha_{mk}-1}\cdot\prod_{k=1}^K\theta_{mk}^{n_{mk}}\mathrm{d}\theta} \\&= \frac{\frac{1}{B\left(\alpha\right)}\prod_{k=1}^K\theta_{mk}^{n_{mk}+\alpha_k-1}}{\frac{1}{B\left(\alpha\right)}\int\prod_{k=1}^K\theta_{mk}^{n_{mk}+\alpha_k-1}\mathrm{d}\theta} \\&= \frac{1}{B\left(n_m+\alpha\right)}\prod_{k=1}^K\theta_{mk}^{n_{mk}+\alpha_k-1} \\&= Dir\left(\theta_m|n_m+\alpha\right)\end{align}</script><p>其中，$n_m=\left(n_{m1},n_{m2},\cdots,n_{mK}\right)$是第$m$个文本的话题的计数。</p><p>所以，参数$\theta_m$的估计</p><script type="math/tex; mode=display">\theta_{mk}=E_{Dir\left(\theta_m|n_m+\alpha\right)}\left[\theta_m\right]=\frac{n_{mk}+\alpha}{\sum_{k=1}^K\left(n_{mk}+\alpha\right)}\quad m=1,2,\cdots,M;\quad k=1,2,\cdots,K</script><p>参数$\varphi_k$的后验概率</p><script type="math/tex; mode=display">\begin{align}p\left(\varphi_k|\mathbf{w},\mathbf{z},\beta\right) &= \frac{p\left(\mathbf{w}|\mathbf{z}_k,\varphi_k\right)p\left(\varphi_k|\beta\right)}{\int p\left(\mathbf{w}|\mathbf{z}_k,\varphi_k\right)p\left(\varphi_k|\beta\right)\mathrm{d}\varphi} \\&=\frac{\prod_{v=1}^V\frac{1}{B\left(\beta\right)}\varphi_{kv}^{\beta_{kv}-1}\cdot\prod_{v=1}^K\varphi_{kv}^{n_{kv}}}{\int \prod_{v=1}^V\frac{1}{B\left(\beta\right)}\varphi_{kv}^{\beta_{kv}-1}\cdot\prod_{v=1}^K\varphi_{kv}^{n_{kv}}\mathrm{d}\varphi} \\&= \frac{\frac{1}{B\left(\beta\right)}\prod_{v=1}^V\varphi_{kv}^{n_{kv}+\beta_v-1}}{\frac{1}{B\left(\beta\right)}\int\prod_{v=1}^V\varphi_{kv}^{n_{kv}+\beta_v-1}\mathrm{d}\varphi} \\&= \frac{1}{B\left(n_k+\beta\right)}\prod_{v=1}^V\varphi_{kv}^{n_{kv}+\beta_k-1} \\&= Dir\left(\varphi_k|n_k+\beta\right)\end{align}</script><p>其中，$n_k=\left(n_{k1},n_{k2},\cdots,n_{kV}\right)$是第$k$个文本的话题的计数。</p><p>所以，参数$\varphi_k$的估计</p><script type="math/tex; mode=display">\varphi_{kv}=E_{Dir\left(\varphi_k|n_k+\beta\right)}\left[\beta_k\right]=\frac{n_{kv}+\beta}{\sum_{v=1}^V\left(n_{kv}+\beta\right)}\quad k=1,2,\cdots,K;\quad v=1,2,\cdots,V</script><p>LDA吉布斯抽样算法<br>输入：文本的单词序列$\mathbf{w}=\left\{\mathbf{w}_1,\mathbf{w}_2,\cdots,\mathbf{w}_m,\cdots,\mathbf{w}_M\right\},\mathbf{w}_m=\left(w_{m1},w_{m2},\cdots,w_{mn},\cdots,w_{mN_m}\right)$；<br>输出：文本的话题序列$\mathbf{z}=\left\{\mathbf{z}_1,\mathbf{z}_2,\cdots,\mathbf{z}_m,\cdots,\mathbf{z}_M\right\},\mathbf{z}_m=\left(z_{m1},z_{m2},\cdots,z_{mn},\cdots,z_{mN_m}\right)$的后验概率分布$p\left(\mathbf{z}|\mathbf{w},\alpha,\beta\right)$的样本计数，模型的参数$\varphi$和$\theta$的估计值；<br>参数：超参数$\alpha$和$\beta$，话题个数$K$。</p><ol><li>设所有计数矩阵的元素$n_{mk},n_{kv}$，计数向量元素$n_m,n_k$初始值为$0$；</li><li>对所有文本$\mathbf{w}_m,m=1,2,\cdots,M$<br>对第$m$个文本中的所有单词$w_{mn},n=1,2,\cdots,N_m$<br>$\quad$抽样话题$z_{mn}=z_k\thicksim Mult\left(\frac{1}{K}\right)$；<br>$\quad$增加文本-话题计数$n_{mk}=n_{mk}+1$，<br>$\quad$增加文本-话题和计数$n_m=n_m+1$，<br>$\quad$增加话题-单词计数$n_{kv}=n_{kv}+1$，<br>$\quad$增加话题-单词和计数$n_k=n_k+1$；  </li><li>循环执行以下操作，知道进入燃烧期<br>对所有文本$\mathbf{w}_m,m=1,2,\cdots,M$<br>对第$m$个文本中的所有单词$w_{mn},n=1,2,\cdots,N_m$<br>(a)当前的单词$w_{mn}$是第$v$个单词，话题指派$z_{mn}$是第$k$个话题；<br>减少计数$n_{mk}=n_{mk}-1,n_m=n_m-1,n_{kv}=n_{kv}-1,n_k=n_k-1$；<br>(b)按照满条件分布进行抽样<script type="math/tex; mode=display">p\left(z_i|\mathbf{z}_{-i},\mathbf{w},\alpha,\beta\right)=\frac{n_{mk}+\alpha_k}{\sum_{k=1}^K\left(n_{mk}+\alpha_k\right)}\cdot\frac{n_{kv}+\beta_v}{\sum_{v=1}^V\left(n_{kv}+\beta_v\right)}</script>得到新的$k^{‘}$个话题，分配给$z_{mn}$；<br>(c)增加计数$n_{mk^{‘}}=n_{mk^{‘}}+1,n_m=n_m+1,n_{k^{‘}v}=n_{k^{‘}v}+1,n_{k^{‘}}=n_{k^{‘}}+1$；<br>(d)得到更新的计数矩阵$N_{K\times V}=\left[n_{kv}\right]$和$N_{M\times K}=\left[n_{mk}\right]$表示后验概率分布$p\left(\mathbf{z}|\mathbf{w},\alpha,\beta\right)$的样本计数；  </li><li>利用得到的样本计数，计算模型参数  <script type="math/tex; mode=display">\theta_{mk}=\frac{n_{mk}+\alpha}{\sum_{k=1}^K\left(n_{mk}+\alpha\right)} \\\varphi_{kv}=\frac{n_{kv}+\beta}{\sum_{v=1}^V\left(n_{kv}+\beta\right)}</script></li></ol><p>变分推理基本思想：<br>假设模型是联合概率分布$p\left(x,z\right)$，其中$x$是观测变量，$z$是隐变量，包括参数。目标是学习模型的后验概率分布$p\left(z|x\right)$，用模型进行推理。由于后验概率是复杂分布，直接估计分布参数困难。因此用概率分布$q\left(z\right)$近似条件概率分布$p\left(z|x\right)$。其中，$q\left(z\right)$称为变分分布，相似性度量采用KL散度$D\left(q\left(z\right)||p\left(z|x\right)\right)$计算。</p><p>分布$q\left(z\right)$与分布$p\left(z|z\right)$的KL散度</p><script type="math/tex; mode=display">\begin{align}D\left(q\left(z\right)||p\left(z|x\right)\right) &= \sum_z q\left(z\right)\log \frac{q\left(z\right)}{p\left(z|x\right)} \\&= \sum_z q\left(z\right) \log q\left(z\right) - \sum_z q\left(z\right) \log p\left(z|x\right) \\&= \sum_z q\left(z\right) \log q\left(z\right) - \sum_z q\left(z\right) \log \frac{p\left(z,x \right)}{p\left(x\right)} \\&= \sum_z q\left(z\right) \log q\left(z\right) - \sum_z q\left(z\right) \log p\left(z,x\right) + \sum_z q\left(z\right) \log p\left(x\right)  \\ &= E_q\left[\log q\left(z\right)\right] - E_q\left[\log p\left(x,z\right)\right] + \log p\left(x\right) \\&= \log p\left(x\right) - \left\{E_q\left[\log q\left(x,z\right)\right] - E_q\left[\log p\left(z\right)\right]\right\}\end{align}</script><p>由于分布$q\left(z\right)$与分布$p\left(z|z\right)$的KL散度大于等于零，当且仅当两个分布一致时为零。所以，</p><script type="math/tex; mode=display">\log p\left(x\right) \geq E_q\left[\log q\left(x,z\right)\right] - E_q\left[\log p\left(z\right)\right]</script><p>其中，不等式左端称为证据，不等式有端称为证据下界，证据下界记作</p><script type="math/tex; mode=display">L\left(q\right)=E_q\left[\log q\left(x,z\right)\right] - E_q\left[\log p\left(z\right)\right]</script><p>在KL散度下的最优近似分布</p><script type="math/tex; mode=display">\begin{align}q^*\left(z\right)&=\mathop{\arg\min}_q D\left(q\left(z\right)||p\left(z|x\right)\right)  \\&= \mathop{\arg\max}_q L\left(q\right)\end{align}</script><p>对变分分布$q\left(x\right)$通常假设对于$z$的所有分量都是互相独立的，即满足</p><script type="math/tex; mode=display">q\left(z\right)=q\left(z_1\right)q\left(q_2\right)\cdots q\left(z_n\right)</script><p>这样的变分分布称为平均场。</p><p>最优近似分布是在平均场的集合，即满足独立假设的分布集合</p><script type="math/tex; mode=display">Q=\left\{q\left(z\right)|q\left(z\right)=\prod_{i=1}^n q\left(z_i\right)\right\}</script><p>中进行的。</p><p>变分EM算法：<br>输入：联合概率$p\left(x,z|\theta\right)$，平均场$q\left(z\right)=\prod_{i=1}^n q\left(z_i\right)$<br>输出：模型参数$\theta$的估计值  </p><ol><li>初始化$\theta$；</li><li>E步：固定$\theta$，求$L\left(q,\theta\right)$对$q$的最大化；</li><li>M步：固定$q$，求$L\left(q,\theta\right)$对$\theta$的最大化；</li><li>得到模型参数$\theta$。</li></ol><p>文本的单词序列$\mathbf{w}=\left(w_1,w_2,\cdots,w_n,\cdots,w_N\right)$，对应的话题序列$\mathbf{z}=\left(z_1,z_2,\cdots,z_n,\cdots,z_N\right)$，话题分布$\theta$，随机变量$\mathbf{w},\mathbf{z},\theta$的联合分布</p><script type="math/tex; mode=display">p\left(\mathbf{w},\mathbf{z},\theta\right)=p\left(\theta|\alpha\right)\prod_{n=1}^N p\left(z_n|\theta\right)p\left(w_n|z_n,\varphi\right)</script><p>其中，$\mathbf{w}$是可观测变量，$\theta$和$\mathbf{z}$是隐变量，$\alpha$和$\varphi$是参数。</p><p>平均场变分分布</p><script type="math/tex; mode=display">q\left(\theta,\mathbf{z}|\gamma,\eta\right)=q\left(\theta|\gamma\right)\prod_{n=1}^N q\left(z_n|\eta_n\right)</script><p>其中，$\gamma$是狄利克雷分布参数，$\eta$是多项分布参数，变量$\theta$和$\mathbf{z}$的各个分量都是条件独立的。</p><p>文本集合的证据下界</p><script type="math/tex; mode=display">L\left(\gamma,\eta,\alpha,\varphi\right)=\sum_{m=1}^M\left\{E_{q_m}\left[\log p\left(\theta_m,\mathbf{z}_m,\mathbf{w}_m|\alpha,\varphi\right)\right]-E_{q_m}\left[\log q\left(\theta_m,\mathbf{z}_m|\gamma_m,\eta_m\right)\right]\right\}</script><p>文本$\mathbf{w}$的证据下界</p><script type="math/tex; mode=display">\begin{align}L_{\mathbf{w}}\left(\gamma,\eta,\alpha,\varphi\right)&=E_q\left[\log p\left(\theta,\mathbf{z},\mathbf{w}|\alpha,\varphi\right)\right]-E_q\left[\log q\left(\theta,\mathbf{z}|\gamma,\eta\right)\right] \\&=E_q\left[\log p\left(\theta|\alpha\right)\right]+E_q\left[\log p\left(\mathbf{z}|\theta\right)\right]+E_q\left[\log p\left(\mathbf{w}|\mathbf{z},\varphi\right)\right] \\&\quad-E_q\left[\log q\left(\theta|\gamma\right)\right]-E_q\left[\log q\left(\mathbf{z}|\eta\right)\right]\end{align}</script><p>其中，$E_q\left[\cdot\right]$是对分布$q\left(\theta,\mathbf{z}|\gamma,\eta\right)$的数学期望，$\gamma$和$\eta$是变分分布的参数，$\alpha$和$\varphi$是LDA模型参数。</p><script type="math/tex; mode=display">\begin{align}E_q\left[\log p\left(\theta|\alpha\right)\right]&=E_q\left[\log\frac{\Gamma\left(\sum_{l=1}^K\alpha_l\right)}{\prod_{k=1}^K\Gamma\left(\alpha_k\right)}\prod_{k=1}^K\theta_k^{\alpha_k-1}\right]  \\&= \log\Gamma\left(\sum_{l=1}^K\alpha_l\right)-\sum_{k=1}^K\log\Gamma\left(\alpha_k\right)+\sum_{k=1}^K\left(\alpha_k-1\right)E_q\left[\log\theta_k\right] \\&=\log\Gamma\left(\sum_{l=1}^K\alpha_l\right)-\sum_{k=1}^K\log\Gamma\left(\alpha_k\right)+\sum_{k=1}^K\left(\alpha_k-1\right)\left[\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right]\end{align}</script><p>其中$\theta\thicksim Dir\left(\theta|\gamma\right)$，狄利克雷分布的数学期望</p><script type="math/tex; mode=display">E_{q\left(\theta|\gamma\right)}\left[\log\theta_k\right]=\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)</script><p>$\Psi\left(\gamma_k\right)$是对数伽马函数的导数，即</p><script type="math/tex; mode=display">\Psi\left(\gamma_k\right)=\frac{\mathrm{d}}{\mathrm{d}\gamma_k}\log\Gamma\left(\gamma_k\right)</script><script type="math/tex; mode=display">\begin{align}E_q\left(\log p\left(\mathbf{z}|\theta\right)\right)&=\sum_{n=1}^N E_q\left[\log p\left(z_n|\theta\right)\right]\\&=\sum_{n=1}^N E_{q\left(\theta,z_n|\gamma,\eta\right)}\left[\log p\left(z_n|\theta\right)\right] \\&=\sum_{n=1}^N \sum_{k=1}^K q\left(\theta|\gamma\right)q\left(z_n|\eta\right)\log p\left(z_n|\theta\right) \\&=\sum_{n=1}^N \sum_{k=1}^K q\left(z_n|\eta\right) q\left(\theta|\gamma\right) \log\prod_{k=1}^K \theta_k^{n_k}\\&=\sum_{n=1}^N \sum_{k=1}^K q\left(z_n|\eta\right) q\left(\theta|\gamma\right) \sum_{k=1}^K n_k \log\theta_k \\&=\sum_{n=1}^N \sum_{k=1}^K q\left(z_n|\eta\right) E_{q\left(\theta|\gamma\right)}\left[\log\theta_k\right] \\&=\sum_{n=1}^N \sum_{k=1}^K \eta_{nk}\left[\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right] \end{align}</script><p>其中，$\eta_{nk}$为文档第$n$个位置的单词由第$k$个话题产生的概率。</p><script type="math/tex; mode=display">\begin{align}E_q\left[\log p\left(\mathbf{w}|\mathbf{z},\varphi\right)\right]&=\sum_{n=1}^N E_q\left[\log p\left(w_n|z_n,\varphi\right)\right]  \\&=\sum_{n=1}^N E_{q\left(z_n|\eta\right)}\left[\log p\left(w_n|z_n,\varphi\right)\right] \\&=\sum_{n=1}^N \sum_{k=1}^K q\left(z_{nk}|\eta\right)\log p\left(w_n|z_n,\varphi\right) \\&=\sum_{n=1}^N \sum_{k=1}^K \sum_{v=1}^V \eta_{nk}w_n^v\log\varphi_{kv}\end{align}</script><p>其中，$w_n^v$的取值在第$n$个位置的单词是单词集合的第$v$个单词时为$1$，否则为$0$。</p><script type="math/tex; mode=display">E_q\left[\log q\left(\theta|\gamma\right)\right]=\log\Gamma\left(\sum_{l=1}^K\gamma_l\right)-\sum_{k=1}^K\log\Gamma\left(\gamma_k\right)+\sum_{k=1}^K\left(\gamma_k-1\right)\left[\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right]</script><p>其中，$\gamma_k$表示第$k$个话题的狄利克雷分布参数。</p><script type="math/tex; mode=display">\begin{align}E_q\left[\log q\left(\mathbf{z}|\eta\right)\right]&=\sum_{n=1}^N E_q\left[\log q\left(z_n|\eta\right)\right]\\&=\sum_{n=1}^N E_{q\left(z_n|\eta\right)}\left[\log q\left(z_n|\eta\right)\right] \\&=\sum_{n=1}^N\sum_{k=1}^K q\left(z_{nk}|\eta\right)\log q\left(z_{nk}|\eta\right) \\&=\sum_{n=1}^N\sum_{k=1}^K\eta_{nk}\log\eta_{nk}\end{align}</script><p>其中，$\eta_{nk}$表示文档第$n$个位置的单词由第$k$个话题产生的概率。</p><p>文本$\mathbf{w}$的证据下界</p><script type="math/tex; mode=display">\begin{align}L_{\mathbf{w}}\left(\gamma,\eta,\alpha,\varphi\right)&=\log\Gamma\left(\sum_{l=1}^K\alpha_l\right)-\sum_{k=1}^K\log\Gamma\left(\alpha_k\right)+\sum_{k=1}^K\left(\alpha_k-1\right)\left[\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right] \\&+\sum_{n=1}^N \sum_{k=1}^K \eta_{nk}\left[\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right] \\&+\sum_{n=1}^N \sum_{k=1}^K \sum_{v=1}^V \eta_{nk}w_n^v\log\varphi_{kv} \\&-\log\Gamma\left(\sum_{l=1}^K\gamma_l\right)+\sum_{k=1}^K\log\Gamma\left(\gamma_k\right)-\sum_{k=1}^K\left(\gamma_k-1\right)\left[\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right] \\&+\sum_{n=1}^N\sum_{k=1}^K\eta_{nk}\log\eta_{nk}\end{align}</script><p>变分参数$\eta$的估计  </p><p>约束条件$\sum_{l=1}^K\eta_{nl}=1$下，证据下界关于参数$\eta$的拉格朗日函数</p><script type="math/tex; mode=display">L_{\left[\eta_{nk}\right]}=\eta_{nk}\left[\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right]+\eta_{nk}\log\varphi_{kv}-\eta_{nk}\log\eta_{nk}+\lambda_n\left(\sum_{l=1}^K\eta_{nl}-1\right)</script><p>其中，$\varphi_{kv}$表示（在第$n$个位置）由第$k$个话题生成第$v$个单词的概率。</p><p>对$\eta_{nk}$求偏导，得</p><script type="math/tex; mode=display">\frac{\partial L}{\partial\eta_{nk}}=\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)+\log\varphi_{kv}-\log\eta_{nk}-1+\lambda_n</script><p>令偏导数为零，得到参数$\eta_{nk}$的估计值</p><script type="math/tex; mode=display">\eta_{nk}\propto\varphi_{kv}\exp\left(\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right)</script><p>变分参数$\gamma$的估计</p><p>关于参数$\gamma$的证据下界函数</p><script type="math/tex; mode=display">\begin{align}L_{\left[\gamma_k\right]}&=\sum_{k=1}^K\left(\alpha_k-1\right)\left[\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right]+\sum_{n=1}^N \sum_{k=1}^K \eta_{nk}\left[\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right] \\&-\log\Gamma\left(\sum_{l=1}^K\gamma_l\right)+\log\Gamma\left(\gamma_k\right)-\sum_{k=1}^K\left(\gamma_k-1\right)\left[\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right] \\&=\sum_{k=1}^K\left[\Psi\left(\gamma_k\right)-\Psi\left(\sum_{l=1}^K\gamma_l\right)\right]\left(\alpha_k+\sum_{n=1}^N\eta_{nk}-\gamma_k\right)-\log\Gamma\left(\sum_{l=1}^K\gamma_l\right)+\log\Gamma\left(\gamma_k\right)\end{align}</script><p>对$\gamma_k$求偏导，得</p><script type="math/tex; mode=display">\frac{\partial L}{\partial\gamma_k}=\left[\Psi^{'}\left(\gamma_k\right)-\Psi^{'}\left(\sum_{l=1}^K\gamma_l\right)\right]\left(\alpha_k+\sum_{n=1}^N\eta_{nk}-\gamma_k\right)</script><p>令偏导数为零，得到参数$\eta_k$的估计值</p><script type="math/tex; mode=display">\gamma_k=\alpha_k+\sum_{n=1}^N\eta_{nk}</script><p>LDA的变分参数估计算法：</p><ol><li>初始化：对所有$k$和$n$，$\eta_{nk}^{\left(0\right)}=1/K$</li><li>初始化：对所有$k$，$\gamma_k=\alpha_k+N/K$</li><li>重复<br>$\quad$对$n=1$到$N$<br>$\quad\quad$对$k=1$到$K$<br>$\quad\quad\quad n_{nk}^{\left(t+1\right)}=\varphi_{kv}\exp\left[\Psi\left(\gamma_k^{\left(t\right)}\right)-\Psi\left(\sum_{l=1}^K\gamma_l^{\left(t\right)}\right)\right]$<br>$\quad\quad$规范化$\eta_{nk}^{\left(t+1\right)}$使其和为$1$<br>$\quad\gamma^{\left(t+1\right)}=\alpha+\sum_{n=1}^N\eta_n^{\left(t+1\right)}$<br>直到收敛</li></ol><p>给定一个文本集合$D=\left\{\mathbf{w}_1,\cdots,\mathbf{w}_m,\cdots,\mathbf{w}_M\right\}$，模型参数$\alpha$和$\varphi$的估计是对所有文本同时进行。</p><p>模型参数$\varphi$的估计</p><p>在约束条件$\sum_{v=1}^V=1,k=1,2,\cdots,K$下，证据下界关于参数$\varphi$的拉格朗日函数</p><script type="math/tex; mode=display">L_{\left[\varphi_{kv}\right]}=\sum_{m=1}^M \sum_{n=1}^{N_m} \sum_{k=1}^K \sum_{v=1}^V \eta_{mnk}w_{mn}^v\log\varphi_{kv}+\sum_{k=1}^K\lambda_k\left(\sum_v=1^V\phi_{kv}-1\right)</script><p>对$\varphi_{kv}$求偏导并令其为零，归一化求解，得到参数$\varphi_{kv}$的估计值</p><script type="math/tex; mode=display">\varphi_{kv}=\sum_{m=1}^M\sum_{n=1}^{N_m}\eta_{mnk}w_{mn}^v</script><p>其中，$\eta_{mnk}$为第$m$个文本的第$n$个单词属于第$k$个话题的概率，$w_{mn}^v$在第$m$个文本的第$n$个单词是单词集合的第$v$个单词时取值为$1$，否则为$0$。</p><p>模型参数$\alpha$的估计</p><p>关于参数$\alpha$的证据下界函数</p><script type="math/tex; mode=display">L_{\left[\alpha_k\right]}=\sum_{m=1}^M\left\{\log\Gamma\left(\sum_{l=1}^K\alpha_l\right)-\sum_{k=1}^K\log\Gamma\left(\alpha_k\right)+\sum_{k=1}^K\left(\alpha_k-1\right)\left[\Psi\left(\gamma_{mk}\right)-\Psi\left(\sum_{l=1}^K\gamma_{ml}\right)\right]\right\}</script><p>对$\alpha_k$求偏导，得</p><script type="math/tex; mode=display">\frac{\partial L}{\partial\alpha_k}=M\left[\Psi\left(\sum_{l=1}^K\alpha_l\right)-\Psi\left(\alpha_k\right)\right]+\sum_{m=1}^M\left[\Psi\left(\gamma_{mk}\right)-\Psi\left(\sum_{l=1}^K\gamma_{ml}\right)\right]</script><p>再对$\alpha_l$求偏导，得</p><script type="math/tex; mode=display">\frac{\partial^2 L}{\partial\alpha_k\partial\alpha_l}=M\left[\Psi^{'}\left(\sum_{l=1}^K\alpha_l\right)-\delta\left(k,l\right)\Psi^{'}\left(\alpha_k\right)\right]</script><p>其中，$\delta\left(k,l\right)$是delta函数。</p><p>应用牛顿法，用以下公式迭代</p><script type="math/tex; mode=display">\alpha_{new}=\alpha_{old}-H\left(\alpha_{old}\right)^{-1}g\left(\alpha_{old}\right)</script><p>其中，$g\left(\alpha\right)$是变量$\alpha$的梯度，$H\left(\alpha\right)$是变量$\alpha$的Hessian矩阵。得到参数$\alpha$的估计值。</p><p>LDA的变分EM算法：<br>输入：给定文本集合$D=\left\{\mathbf{w}_1,\cdots,\mathbf{w}_m,\cdots,\mathbf{w}_M\right\}$<br>输出：变分参数$\gamma,\eta$，模型参数$\alpha,\varphi$<br>交替执行E步和M步，直到收敛  </p><ol><li>E步<br>固定模型参数$\alpha,\varphi$，通过关于变分参数$\gamma,\eta$的证据下界最大化，估计变分参数$\gamma,\eta$。  </li><li>M步<br>固定变分参数$\gamma,\eta$，通过关于模型参数$\alpha,\varphi$的证据下界最大化，估计模型参数$\alpha,\varphi$。    </li></ol><p>根据变分参数$\left(\gamma,\eta\right)$，可以估计模型参数$\theta=\left(\theta_1,\cdots,\theta_m,\cdots,\theta_M\right),\mathbf{z}=\left(z_1,\cdots,z_m,\cdots,z_M\right)$。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>topic model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>MCMC</title>
    <link href="/2020/05/05/MCMC/"/>
    <url>/2020/05/05/MCMC/</url>
    
    <content type="html"><![CDATA[<h1 id="马尔科夫链蒙特卡罗法（Markov-Chain-Monte-Carlo，MCMC）"><a href="#马尔科夫链蒙特卡罗法（Markov-Chain-Monte-Carlo，MCMC）" class="headerlink" title="马尔科夫链蒙特卡罗法（Markov Chain Monte Carlo，MCMC）"></a>马尔科夫链蒙特卡罗法（Markov Chain Monte Carlo，MCMC）</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><h2 id="1-蒙特卡洛法"><a href="#1-蒙特卡洛法" class="headerlink" title="1 蒙特卡洛法"></a>1 蒙特卡洛法</h2><p>蒙特卡洛法要解决的问题是，假设概率分布的定义已知，通过抽样获得概率分布的随机样本，并通过得到的随机样本对概率分布的特征进行分析。</p><p>蒙特卡洛的法有直接抽样法、接受-拒绝抽样法、重要性抽样法等，其核心是随机抽样。</p><p>接受-拒绝算法：<br>输入：抽样的目标概率分布的概率密度函数$p\left(x\right)$；<br>输出：概率分布的随机样本$x_1,x_2,\cdots,x_n$。  </p><ol><li>选择概率密度函数为$q\left(x\right)$的概率分布作为建议分布，使其对任一$x$满足$cq\left(x\right)\geq \left(x\right)$，其中$c&gt;0$。</li><li>按照建议分布$q\left(x\right)$随机抽样得到样本$x’$，再按照均匀分布在$\left(0,1\right)$范围内抽样得到$u$。</li><li>如果$u\leq\frac{p\left(x’\right)}{cq\left(x’\right)}$，则将$x’$作为抽样结果；否则，回到步骤2.。</li><li>直至得到$n$个随机样本，结束。</li></ol><h2 id="2-马尔科夫链"><a href="#2-马尔科夫链" class="headerlink" title="2 马尔科夫链"></a>2 马尔科夫链</h2><p>假设在时刻$0$的随机变量$X_0$遵循概率分布$P\left(X_0\right)=\pi_0$，称为初始状态分布。在某个时刻$t\geq 1$的随机变量$X_t$与前一个时刻的随机变量$X_{t-1}$之间有条件分布$P\left(X_t|X_{t-1}\right)$，如果$X_t$只依赖于$X_{t-1}$，这一性质称为马尔科夫性，即</p><script type="math/tex; mode=display">P\left(X_t|X_0,X_1,\cdots,X_{t-1}\right)=P\left(X_t|X_{t-1}\right), t=1,2,\cdots</script><p>具有马尔科夫性的随机序列$X=\{X_0,X_1,\cdots,X_t,\cdots\}$称为马尔科夫链或马尔科夫过程。每个随机变量$X_t\left(t=0,1,2,\cdots\right)$的取值集合相同，称为状态空间$\mathcal{S}$。</p><p>离散状态马尔科夫链$X=\{X_0,X_1,\cdots,X_t,\cdots\}$，随机变量$X_t\left(t=0,1,2,\cdots\right)$定义在离散空间$\mathcal{S}$，转移概率分布可以由矩阵表示。</p><p>若马尔科夫链咋时刻$t-1$处于状态$j$，在时刻$t$移动到状态$i$，将转移概率记作</p><script type="math/tex; mode=display">p_{ij}=\left(X_t=i|X_{t-1}=j\right),i=1,2,\cdots;j=1,2,\cdots</script><p>满足</p><script type="math/tex; mode=display">p_{ij}\geq 0, \sum_i p_{ij}=1</script><p>马尔科夫链的转移矩阵可表示为</p><script type="math/tex; mode=display">P=\left[p_{ij}\right]</script><p>马尔科夫链$X=\{X_0,X_1,\cdots,X_t,\cdots\}$在时刻$t,t=0,1,2,\cdots$的概率分布，称为时刻$t$的状态分布，记作</p><script type="math/tex; mode=display">\pi\left(t\right)=\left[\begin{matrix}   \pi_1\left(t\right) \\   \pi_2\left(t\right) \\   \vdots \\  \end{matrix}\right]</script><p>其中$\pi_i\left(t\right)$表示时刻$t$状态为$i$的概率$P\left(X_t=i\right)$</p><script type="math/tex; mode=display">\pi_i\left(t\right)=P\left(X_t=i\right),i=1,2,\cdots</script><p>特别地，马尔科夫链的初始状态分布可表示为</p><script type="math/tex; mode=display">\pi\left(0\right)=\left[\begin{matrix}   \pi_1\left(0\right) \\   \pi_2\left(0\right) \\   \vdots \\  \end{matrix}\right]</script><p>其中$\pi_i\left(0\right)$表示时刻$0$状态为$i$的概率$P\left(X_t=i\right)$</p><script type="math/tex; mode=display">\pi_i\left(0\right)=P\left(X_t=i\right)</script><p>通常初始分布$\pi\left(0\right)$的向量只有一个分量是$1$，其余分量是$0$，表示马尔科夫链从一个具体状态开始。</p><p>马尔科夫链$X$在时刻$t$的状态分布，可以由在时刻$t-1$的状态分布以及转移概率分布决定</p><script type="math/tex; mode=display">\pi\left(t\right)=P\pi\left(t-1\right)</script><p>设有马尔科夫链$X=\{X_0,X_1,\cdots,X_t,\cdots\}$，其状态空间为$\mathcal{S}$，转移概率矩阵为$P=\left[p_{ij}\right]$，如果状态空间$\mathcal{S}$上的一个分布</p><script type="math/tex; mode=display">\pi=\left[\begin{matrix}   \pi_1 \\   \pi_2 \\   \vdots \\  \end{matrix}\right]</script><p>使得</p><script type="math/tex; mode=display">\pi=P\pi</script><p>则称$\pi$为马尔科夫链$X=\{X_0,X_1,\cdots,X_t,\cdots\}$的平稳分布。</p><p>连续状态马尔科夫链$X=\{X_0,X_1,\cdots,X_t,\cdots\}$，随机变量$X_t\left(t=0,1,2,\cdots\right)$定义在连续状态空间$\mathcal{S}$，转移状态分布由概率转移核或转移核表示。</p><p>设$\mathcal{S}$是连续状态空间，对任意的$x\in\mathcal{S},A\subset\mathcal{S}$，转移核定义为</p><script type="math/tex; mode=display">P\left(x,A\right)=\int_A p\left(x,y\right)\mathrm{d}y</script><p>其中，$p\left(x,\centerdot\right)$是概率密度函数，满足$p\left(x,\centerdot\right)\geq0,P\left(x,\mathcal{S}\right)=\int_{\mathcal{S}}p\left(x,y\right)\mathrm{d}y=1$。转移核$P\left(x,A\right)$表示从$x\thicksim A$的转移概率</p><script type="math/tex; mode=display">P\left(X_t=A|X_{t-1}=x\right)=P\left(x,A\right)</script><p>有时也将概率密度函数$p\left(x,\centerdot\right)$称为转移核。</p><p>若马尔科夫链的转态空间$\mathcal{S}$上的概率分布$\pi\left(x\right)$满足条件</p><script type="math/tex; mode=display">\pi\left(y\right)=\int p\left(x,y\right)\pi\left(x\right)\mathrm{d}x,\quad\forall y\in\mathcal{S}</script><p>则称分布$\pi\left(x\right)$为该马尔科夫链的平稳分布。等价地，</p><script type="math/tex; mode=display">\pi\left(A\right)=\int P\left(x,A\right)\pi\left(x\right)\mathrm{d}x,\quad\forall A\subset\mathcal{S}</script><p>或简写成</p><script type="math/tex; mode=display">\pi=P\pi</script><h2 id="3-马尔科夫链蒙特卡洛法——Metropolis-Hastings算法"><a href="#3-马尔科夫链蒙特卡洛法——Metropolis-Hastings算法" class="headerlink" title="3 马尔科夫链蒙特卡洛法——Metropolis-Hastings算法"></a>3 马尔科夫链蒙特卡洛法——Metropolis-Hastings算法</h2><p>假设要抽样的概率分布为$p\left(x\right)$。MH算法采用转移核为$p\left(x,x^{‘}\right)$的马尔科夫链：</p><script type="math/tex; mode=display">p\left(x,x^{'}\right)=q\left(x,x^{'}\right)\alpha\left(x,x^{'}\right)</script><p>其中，$q\left(x,x^{‘}\right)$为建议分布，$\alpha\left(x,x^{‘}\right)$为接受分布。</p><p>建议分布$q\left(x,x^{‘}\right)$是另一个马尔科夫链的转移核，并且其概率值恒不为$0$，同时是一个容易抽样的分布。</p><p>接受分布$\alpha\left(x,x^{‘}\right)$是</p><script type="math/tex; mode=display">\alpha\left(x,x^{'}\right)=\min\left\{1,\frac{p\left(x^{'}\right)q\left(x^{'},x\right)}{p\left(x\right)q\left(x,x^{'}\right)}\right\}</script><p>转移核可表示为</p><script type="math/tex; mode=display">p\left(x,x^{'}\right)= \left\{ \begin{array}  & q\left(x,x^{'}\right)  & p\left(x^{'}\right)q\left(x^{'},x\right)\geq p\left(x\right)q\left(x,x^{'}\right)\\ q\left(x,x^{'}\right) \frac{p\left(x^{'}\right)}{p\left(x\right)}  & p\left(x^{'}\right)q\left(x^{'},x\right)< p\left(x\right)q\left(x,x^{'}\right) \end{array} \right.</script><p>转移核为$p\left(x,x^{‘}\right)$的马尔科夫链上的随机游走以以下方式进行：如果在时刻$t-1$处于状态$x$，即$X_{t-1}=x$，则先按建议分布$q\left(x,x^{‘}\right)$抽样一个候选状态$x^{‘}$，然后按照接受分布$\alpha\left(x,x^{‘}\right)$抽样决定是否接受状态$x^{‘}$。以概率$\alpha\left(x,x^{‘}\right)$接受$x^{‘}$，$X_t=x^{‘}$；以概率$1-\alpha\left(x,x^{‘}\right)$拒绝$x^{‘}$，$X_t=x$。</p><p>具体地，在区间$\left(0,1\right)$上的均匀分布中抽取一个随机数$u$，决定时刻$t$的状态</p><script type="math/tex; mode=display">X_t= \left\{ \begin{array}  & x^{'}, & u\leq\alpha\left(x,x^{'}\right) \\ x, & u>\alpha\left(x,x^{'}\right) \end{array} \right.</script><p>可以证明，转移核为$p\left(x,x^{‘}\right)$的马尔科夫链的平稳状态就是$p\left(x\right)$，即要抽样的目标分布。</p><p>建议分布的对称形式，即对任意的$x$和$x^{‘}$有</p><script type="math/tex; mode=display">q\left(x,x^{'}\right)=q\left(x^{'},x\right)</script><p>这样的建议分布称为Metropolis选择。此时，接受分布$\alpha\left(x,x^{‘}\right)$简化为</p><script type="math/tex; mode=display">\alpha\left(x,x^{'}\right)=\min\left\{1,\frac{p\left(x^{'}\right)}{p\left(x\right)}\right\}</script><p>建议分布的独立抽样形式，即$q\left(x,x^{‘}\right)$与当前状态$x$无关，<script type="math/tex">q\left(x,x^{'}\right)=q\left(x^{'}\right)</script><br>此时，接受分布$\alpha\left(x,x^{‘}\right)$可写成</p><script type="math/tex; mode=display">\alpha\left(x,x^{'}\right)=\min\left\{1,\frac{w\left(x^{'}\right)}{w\left(x\right)}\right\}</script><p>其中，$w\left(x^{‘}\right)=p\left(x^{‘}\right)/q\left(x^{‘}\right)$，$w\left(x\right)=p\left(x\right)/q\left(x\right)$。</p><p>Metropolis-Hastings算法：<br>输入：待抽样的目标分布的密度函数$p\left(x\right)$，收敛步骤$m$，迭代步骤$n$<br>输出：$p\left(x\right)$的随机样本$x_{m+1},x_{m+2},\cdots,x_n$</p><ol><li>任意选择一个初始状态值$X_0$</li><li>对$i=1,2,\cdots,n$ 循环执行<br>(a)设状态 $X_{i-1}=x$，按照建议分布$q\left(x,x^{‘}\right)$随机抽样一个候选状态$x^{‘}$<br>(b)计算接受概率  <script type="math/tex; mode=display">\alpha\left(x,x^{'}\right)=\min\left\{1,\frac{p\left(x^{'}\right)q\left(x^{'},x\right)}{p\left(x\right)q\left(x,x^{'}\right)}\right\}</script>(c)从区间$\left(0,1\right)$中按均匀分布随机抽取数$u$<br>若$u\leq\alpha\left(x,x^{‘}\right)$，则状态$X_i=x^{‘}$；否则，状态$X_i=x$。</li><li>得到样本集合$\{x_{m+1},x_{m+2},\cdots,x_n\}$</li></ol><h2 id="4-单分量Metropolis-Hastings算法"><a href="#4-单分量Metropolis-Hastings算法" class="headerlink" title="4 单分量Metropolis-Hastings算法"></a>4 单分量Metropolis-Hastings算法</h2><p>假设马尔科夫链的状态由$k$维随机变量表示</p><script type="math/tex; mode=display">x=\left(x_1,x_2,\cdots,x_k\right)^\top</script><p>其中，$x_j$表示随机变量$x$的第$j$个分量，$j=1,2,\cdots,k$</p><p>马尔科夫链在时刻$i$的状态</p><script type="math/tex; mode=display">x^{\left(i\right)}=\left(x_1^{\left(i\right)},x_2^{\left(i\right)},\cdots,x_k^{\left(i\right)}\right)</script><p>其中，$x_j^{\left(i\right)}$是随机变量$x^{\left(i\right)}$的第$j$个分量，$j=1,2,\cdots,k$。</p><p>单分量Metropolis-Hastings算法由下面的$k$步迭代实现Metropolis-Hastings算法的一次迭代。</p><p>设在第$i-1$次迭代结束时分量$x_j$的取值为$x_j^{\left(i-1\right)}$，在第$i$次迭代的第$j$步，对分量$x_j$根据Metropolis-Hastings算法更新，得到其新的取值$x_j^{\left(i\right)}$。</p><p>首先，由建议分布$q\left(x_j^{\left(i-1\right)},x_j|x_{-j}^{\left(i\right)}\right)$抽样产生分量$x_j$的候选值$x_j^{‘\left(i\right)}$。$x_{-j}^{\left(i\right)}$表示在第$i$次迭代的第$j-1$步后的$x^{\left(i\right)}$除去$x_j^{\left(i-1\right)}$的所有值，即</p><script type="math/tex; mode=display">x_{-j}^{\left(i\right)}=\left(x_1^{\left(i\right)},\cdots,x_{j-1}^{\left(i\right)},x_{j+1}^{\left(i-1\right)},\cdots,x_{k}^{\left(i-1\right)}\right)^\top</script><p>其中分量$x_1^{\left(i\right)},\cdots,x_{j-1}^{\left(i\right)}$已经更新。</p><p>然后，按照接受概率</p><script type="math/tex; mode=display">\alpha\left(x_j^{\left(i-1\right)},x_j^{'\left(i\right)}|x_{-j}^{\left(i\right)}\right)=\min\left\{1,\frac{p\left(x_j^{'\left(i\right)}|x_{-j}^{\left(i\right)}\right)q\left(x_j^{'\left(i\right)},x_j^{\left(i-1\right)}|x_{-j}^{\left(i\right)}\right)}{p\left(x_j^{\left(i-1\right)}|x_{-j}^{\left(i\right)}\right)q\left(x_j^{\left(i-1\right)},x_j^{'\left(i\right)}|x_{-j}^{\left(i\right)}\right)}\right\}</script><p>抽样决定是否接受候选值$x_j^{‘\left(i\right)}$。如果接受，则令$x_j^{\left(i\right)}=x_j^{‘\left(i\right)}$；否则，令$x_j^{\left(i\right)}=x_j^{\left(i-1\right)}$。其余分量在第$j$步不改变。</p><p>马尔科夫链的转移概率为</p><script type="math/tex; mode=display">p\left(x_j^{\left(i-1\right)},x_j^{'\left(i\right)}|x_{-j}^{\left(i\right)}\right)=q\left(x_j^{\left(i-1\right)},x_j^{'\left(i\right)}|x_{-j}^{\left(i\right)}\right)\alpha\left(x_j^{\left(i-1\right)},x_j^{'\left(i\right)}|x_{-j}^{\left(i\right)}\right)</script><p>由于建议分布可能不被接受，Metropolis-Hastings算法可能在一些相邻的时刻不产生移动。</p><h2 id="5-吉布斯抽样"><a href="#5-吉布斯抽样" class="headerlink" title="5 吉布斯抽样"></a>5 吉布斯抽样</h2><p>吉布斯抽样用于多元变量联合分布的抽样和评估。</p><p>吉布斯抽样是单分量Metropolis-Hastings算法的特殊情况。定义建议分布是当前变量$x_j\left(j=1,2,\cdots,k\right)$的满概率分布</p><script type="math/tex; mode=display">q\left(x,x^{'}\right)=p\left(x_j^{'}|x_{-j}\right)</script><p>则接受概率</p><script type="math/tex; mode=display">\begin{align}\alpha\left(x,x^{'}\right) &= \min\left\{1,\frac{p\left(x^{'}\right)q\left(x^{'},x\right)}{p\left(x\right)q\left(x,x^{'}\right)} \right\} \\&=\min\left\{1, \frac{p\left(x_j^{'}|x_{-j}^{'}\right)p\left(x_{-j}^{'}\right)p\left(x_j|x_{-j}^{'}\right)}{p\left(x_j|x_{-j}\right)p\left(x_{-j}\right)p\left(x_j^{'}|x_{-j}\right)}\right\}\end{align}</script><p>由于在对第$j$个分量的采样过程中其余分量不变，即$x_{-j}=x_{-j}^{‘}$，所以可得$\alpha\left(x,x^{‘}\right)=1$，即转移概率为$1$。</p><p>由以上，可得吉布斯抽样的转移核</p><script type="math/tex; mode=display">\begin{align}p\left(x,x^{'}\right) &= q\left(x,x^{'}\right)\alpha\left(x,x^{'}\right)  \\&= p\left(x_j^{'}|x_{-j}\right)\end{align}</script><p>即一次按照单变量的满条件概率分布$p\left(x_j^{‘}|x_{-j}\right)$进行随机抽样，就能实现但分量Metropolis-Hastings算法。</p><p>吉布斯抽样对每次抽样的结果都接受，没有拒绝。抽样会在样本点之间持续移动。</p><p>吉布斯抽样算法：<br>输入：目标概率分布的密度函数$p\left(x\right)$，迭代步数$n$，收敛步数$m$；<br>输出：$p\left(x\right)$的随机样本$x_{m+1},x_{m+2},\cdots,x_n$；</p><ol><li>给出初始样本$x^{\left(0\right)}=\left(x_1^{\left(0\right)},x_2^{\left(0\right)},\cdots,x_k^{\left(0\right)}\right)^\top$</li><li>对$i\left(i=1,2,\cdots,n\right)$循环执行<br>设第$i-1$轮迭代结束时的样本为$x^{\left(i-1\right)}=\left(x_1^{\left(i-1\right)},x_2^{\left(i-1\right)},\cdots,x_k^{\left(i-1\right)}\right)^\top$<br>在第$i$轮迭代中，对$j\left(j=1,2,\cdots,k\right)$循环执行<br>$\quad$由满条件分布$p\left(x_j|x_1^{\left(i-1\right)},\cdots,x_{j-1}^{\left(i-1\right)},x_{j+1}^{\left(i-1\right)}\cdots,x_k^{\left(i-1\right)}\right)$抽取$x_j^{\left(i\right)}$<br>得到第$i$轮迭代值$x^{\left(i\right)}=\left(x_1^{\left(i\right)},x_2^{\left(i\right)},\cdots,x_k^{\left(i\right)}\right)^\top$</li><li>得到样本集合<br>$\left\{x^{\left(m+1\right)},x^{\left(m+2\right)},\cdots,x^{\left(n\right)}\right\}$</li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>topic model</tag>
      
      <tag>markov chain</tag>
      
      <tag>monte carlo</tag>
      
      <tag>metropolis hastings</tag>
      
      <tag>gibbs sampling</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>pLSA</title>
    <link href="/2020/05/05/pLSA/"/>
    <url>/2020/05/05/pLSA/</url>
    
    <content type="html"><![CDATA[<h1 id="概率潜在语义分析（Probabilistic-Latent-Semantic-Analysis-PLSA）"><a href="#概率潜在语义分析（Probabilistic-Latent-Semantic-Analysis-PLSA）" class="headerlink" title="概率潜在语义分析（Probabilistic Latent Semantic Analysis, PLSA）"></a>概率潜在语义分析（Probabilistic Latent Semantic Analysis, PLSA）</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><p>单词集合$W=\{w_1,w_2,\cdots,w_M\}$<br>文本集合$D=\{d_1,d_2,\cdots,d_N\}$<br>话题集合$Z=\{z_1,z_2,\cdots,z_K\}$<br>单词-文本共现数据$T=\left[n\left(w_i,d_j\right)\right],i=1,2,\cdots,M;j=1,2,\cdots,N;$</p><p>文本-单词共现数据的生成过程：</p><ol><li>依据概率分布$P\left(d\right)$，从文本集合中随机选取一个文本$d$，共生成$N$个文本；针对每个文本，执行以下操作；</li><li>在文本$d$给定条件下，依据条件概率分布$P\left(z|d\right)$，从话题集合中随机选取一个话题$z$，共生成$L$个话题（$L$为文本长度）；</li><li>在话题$z$给定条件下，依据条件概率分布$P\left(w|z\right)$，从单词集合中随机选取一个单词$w$。</li></ol><p>文本-单词共现数据$T$的生成概率</p><script type="math/tex; mode=display">P\left(T\right)=\prod_{\left(w,d\right)} P\left(w,d\right)^{n\left(w,d\right)}</script><p>其中，$n\left(w,d\right)$表示$\left(w,d\right)$的出现次数，单词-文本对出现的总次数是$N\times L$。</p><p>每个单词-文本对$\left(w,d\right)$的生成概率</p><script type="math/tex; mode=display">\begin{align}P\left(w,d\right)&=P\left(d\right)P\left(w|d\right)  \\&=P\left(d\right)\sum_z P\left(w,z|d\right)          \\&=P\left(d\right)\sum_z P\left(z|d\right)P\left(w|z\right) \end{align}</script><p>假设在话题$z$给定条件下，单词$w$与文本$d$条件独立</p><script type="math/tex; mode=display">\begin{align}P\left(w,z|d\right)&=P\left(z|d\right)P\left(w|z,d\right) \\&=P\left(z|d\right)P\left(w|z\right) \end{align}</script><p>单词-文本共现数据$T$的对数似然函数</p><script type="math/tex; mode=display">\begin{align}L\left(T\right) &=\log\prod_{w,d}P\left(w,d\right)^{n\left(w,d\right)} \\&=\sum_{i=1}^M\sum_{j=1}^N n\left(w_i,d_j\right)\log P\left(w_i,d_j\right) \\&=\sum_{i=1}^M\sum_{j=1}^N n\left(w_i,d_j\right)\log \sum_{k=1}^K P\left(w_i,d_j,z_k\right) \\&=\sum_{i=1}^M\sum_{j=1}^N n\left(w_i,d_j\right)\log \sum_{k=1}^K P\left(w_i,z_k|d_j\right) P\left(d_j\right) \\&=\sum_{i=1}^M\sum_{j=1}^N n\left(w_i,d_j\right)\log \sum_{k=1}^K P\left(w_i|z_k\right) P\left(z_k|d_j\right) P\left(d_j\right) \\\end{align}</script><p>应用EM算法求解含有隐变量的对数似然函数优化问题。<br>E步：定义Q函数</p><script type="math/tex; mode=display">\begin{align}Q &= \sum_z\log\prod_{w,d}P\left(w,d,z\right)^{n\left(w,d\right)}P\left(z|w,d\right) \\&= \sum_z\log\prod_{w,d}\left[P\left(w,z|d\right)P\left(d\right)\right]^{n\left(w,d\right)}P\left(z|w,d\right) \\&= \sum_z\left\{\sum_{w,d}n\left(w,d\right)\left[\log P\left(d\right)+\log P\left(w,z|d\right)\right]\right\}P\left(z|w,d\right) \\&= \sum_z\left\{\sum_d\left[\sum_w n\left(w,d\right)\log P\left(d\right)+\sum_w n\left(w,d\right)\log P\left(w|z,d\right)P\left(z|d\right)\right]\right\}P\left(z|w,d\right) \\&= \sum_z\left\{\sum_d\left[n\left(d\right)\log P\left(d\right)+\sum_w n\left(w,d\right)\log P\left(w|z\right)P\left(z|d\right)\right]\right\}P\left(z|w,d\right) \\&= \sum_z\left\{\sum_d n\left(d\right)\left[\log P\left(d\right)+\sum_w\frac{n\left(w,d\right)}{n\left(d\right)}\log P\left(w|z\right)P\left(z|d\right)\right]\right\}P\left(z|w,d\right) \\&= \sum_{k=1}^K\left\{\sum_{j=1}^N n\left(d_j\right)\left[\log P\left(d_j\right)+\sum_{i=1}^M\frac{n\left(w_i,d_j\right)}{n\left(d_j\right)}\log P\left(w_i|z_k\right)P\left(z_k|d_j\right)\right]\right\}P\left(z_k|w_i,d_j\right)\end{align}</script><p>其中，$n\left(w,d\right)$表示单词$w$在文本$d$中出现的次数，$n\left(d\right)=\sum_w\left(w,d\right)$表示文本$d$中单词的个数。</p><p>由于可以从数据中直接统计得出$P\left(d_j\right)$的估计，可将$Q$函数简化为只考虑$P\left(w_i|z_k\right)$和$P\left(z_k|d_j\right)$的函数$Q^{‘}$</p><script type="math/tex; mode=display">Q^{'}=\sum_{i=1}^M \sum_{j=1}^N n\left(w_i,d_j\right)\sum_{k=1}^K P\left(z_k|w_i,d_j\right)\log\left[P\left(w_i|z_k\right)P\left(z_k|d_j\right)\right]</script><p>$Q^{‘}$函数中$P\left(z_k|w_i,d_j\right)$根据贝叶斯公式</p><script type="math/tex; mode=display">\begin{align}P\left(z_k|w_i,d_j\right)&=\frac{P\left(w_i,z_k|d_j\right)}{P\left(w_i|d_j\right)} \\&=\frac{P\left(w_i,z_k|d_j\right)}{\sum_{k=1}^K P\left(w_i,z_k|d_j\right)} \\&=\frac{P\left(w_i|z_k\right)P\left(z_k|d_j\right)}{\sum_{k=1}^K P\left(w_i|z_k\right)P\left(z_k|d_j\right)} \end{align}</script><p>其中，$P\left(z_k|d_j\right)$和$P\left(w_i|z_k\right)$可由上一次迭代得到。</p><p>M步：极大化Q函数</p><script type="math/tex; mode=display">\begin{align}\max Q^{'} \\s.t.\quad \sum_{i=1}^M P\left(w_i|z_k\right)&=1,\quad k-1,2,\cdots,K \\\sum_{k=1}^K P\left(z_k|d_j\right)&=1,\quad j=1,2,\cdots,N\end{align}</script><p>定义拉格朗日函数</p><script type="math/tex; mode=display">\Lambda=Q^{'}+\sum_{k=1}^K\tau_k\left(1-\sum_{i=1}^M P\left(w_i|z_k\right)\right)+\sum_{j=1}^N\rho_j\left(1-\sum_{k=1}^K P\left(z_k|d_j\right)\right)</script><p>拉格朗日函数$\Lambda$对$P\left(w_i|z_k\right)$求偏导，并令其等于0，得</p><script type="math/tex; mode=display">\frac{\partial\Lambda}{\partial P\left(w_i|z_k\right)}=\sum_{j=1}^N n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right)\frac{1}{P\left(w_i|z_k\right)}-\tau_k=0,\quad i=1,2,\cdots,M; \quad k=1,2,\cdots,K</script><script type="math/tex; mode=display">\begin{align}\tau_k P\left(w_i|z_k\right) &= \sum_{j=1}^N n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right) \\\sum_{m=1}^M \tau_k P\left(w_m|z_k\right) &= \sum_{m=1}^M \sum_{j=1}^N n\left(w_m,d_j\right)P\left(z_k|w_m,d_j\right) \\\tau_k &= \sum_{m=1}^M \sum_{j=1}^N n\left(w_m,d_j\right)P\left(z_k|w_m,d_j\right) \end{align}</script><script type="math/tex; mode=display">P\left(w_i|z_k\right)=\frac{\sum_{j=1}^N n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right)}{\sum_{m=1}^M \sum_{j=1}^N n\left(w_m,d_j\right)P\left(z_k|w_m,d_j\right) }</script><p>拉格朗日函数$\Lambda$对$P\left(z_k|d_j\right)$求偏导，并令其等于0，得</p><script type="math/tex; mode=display">\frac{\partial\Lambda}{\partial P\left(z_k|d_j\right)}=\sum_{i=1}^M n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right)\frac{1}{P\left(z_k|d_j\right)}-\rho_j=0,\quad j=1,2,\cdots,N; \quad k=1,2,\cdots,K</script><script type="math/tex; mode=display">\begin{align}\rho_j P\left(z_k|d_j\right) &= \sum_{i=1}^M n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right) \\\sum_{k=1}^K\rho_j P\left(z_k|d_j\right) &= \sum_{k=1}^K\sum_{i=1}^M n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right) \\\rho_j  &= \sum_{k=1}^K\sum_{i=1}^M n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right)\end{align}</script><script type="math/tex; mode=display">\begin{align}P\left(z_k|d_j\right)&=\frac{\sum_{i=1}^M n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right)}{\sum_{k=1}^K\sum_{i=1}^M n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right)} \\&= \frac{\sum_{i=1}^M n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right)}{n\left(d_j\right)}\end{align}</script><p>概率潜在语义模型参数估计的EM算法：<br>输入：设单词集合为$W=\{w_1,w_2,\cdots,w_M\}$，文本集合为$D=\{d_1,d_2,\cdots,d_N\}$，话题集合为$Z=\{z_1,z_2,\cdots,z_K\}$，共现数据$\{n\left(w_i,d_j\right)\},i=1,2,\cdots,M,j=1,2,\cdots,N$<br>输出：$P\left(w_i|z_k\right)$和$P\left(z_k|d_j\right)$</p><ol><li>设置参数$P\left(w_i|z_k\right)$和$P\left(z_k|d_j\right)$的初始值；</li><li>迭代执行以下E步，M步，直到收敛为止<br>E步骤：<script type="math/tex; mode=display">\begin{align}P\left(z_k|w_i,d_j\right)=\frac{P\left(w_i|z_k\right)P\left(z_k|d_j\right)}{\sum_{k=1}^K P\left(w_i|z_k\right)P\left(z_k|d_j\right)} \end{align}</script>M步骤： <script type="math/tex; mode=display">\begin{align}P\left(w_i|z_k\right)&=\frac{\sum_{j=1}^N n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right)}{\sum_{m=1}^M \sum_{j=1}^N n\left(w_m,d_j\right)P\left(z_k|w_m,d_j\right) } \\P\left(z_k|d_j\right)&=\frac{\sum_{i=1}^M n\left(w_i,d_j\right)P\left(z_k|w_i,d_j\right)}{n\left(d_j\right)}\end{align}</script></li></ol>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>topic model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>LSA</title>
    <link href="/2020/05/05/LSA/"/>
    <url>/2020/05/05/LSA/</url>
    
    <content type="html"><![CDATA[<h1 id="潜在语义分析（Latent-Semantic-Analysis-LSA）"><a href="#潜在语义分析（Latent-Semantic-Analysis-LSA）" class="headerlink" title="潜在语义分析（Latent Semantic Analysis, LSA）"></a>潜在语义分析（Latent Semantic Analysis, LSA）</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><p>向量空间模型：给定一个文本，用一个向量表示该文本的”语义“，向量的每以一维对应一个单词，其数值为该单词在文本中出现的频数或权值。</p><p>基本假设：</p><ol><li>文本中所有单词出现的情况表示了文本的语义内容；</li><li>文本集合中的每个文本都表示为一个向量，存在于一个向量空间；</li><li>向量空间的度量，如内积或标准化内积表示文本之间的”语义相似度“。</li></ol><p>文本集合$D=\{d_1,d_2,\cdots,d_n\}$<br>单词集合$W=\{w_1,w_2,\cdots,w_m\}$</p><p>单词向量空间：单词-文本矩阵</p><script type="math/tex; mode=display">X= \left[\begin{matrix}   x_{11} & x_{12} & \cdots & x_{1n} \\   x_{21} & x_{22} & \cdots & x_{2n} \\   \vdots & \vdots &        & \vdots \\   x_{m1} & x_{m2} & \cdots & x_{mn}  \end{matrix}\right]</script><p>其中，元素$x_{ij}$表示单词$w_i$在文本$d_j$中出现的频次或权值。</p><p>单词频率-逆文本频率</p><script type="math/tex; mode=display">TFIDF_{ij}=\frac{tf_{ij}}{tf_{\bullet j}}\log\frac{df}{df_i},\quad i=1,2,\cdots,m;\quad j=1,2,\cdots,n</script><p>其中，$tf_{ij}$是单词$w_i$出现在文本$d_j$中的频数，$tf_{\bullet j}$是文本$d_j$中出现的所有单词的频数之和，$df_i$是含有单词$w_i$的文本数，$df$是文本集合$D$的全本文本数。</p><p>单词向量：单词文本矩阵的第$j$列向量$x_j$表示文本$d_j$</p><script type="math/tex; mode=display">x_j=\left[\begin{matrix}   x_{1j} \\   x_{2j} \\   \vdots \\   x_{mj}   \end{matrix}\right], \quad j=1,2,\cdots,n</script><p>则单词文本矩阵$X$可表示为$X=\left[x_1,x_2,\cdots,x_n\right]$。</p><p>文本$d_i$与文本$d_j$之间的相似度可表示为文本单词向量$x_i$与文本单词向量$x_j$的内积</p><script type="math/tex; mode=display">x_i \cdot x_j</script><p>或标准化内积（余弦）</p><script type="math/tex; mode=display">\frac{x_i \cdot x_j}{\|x_i\| \|x_j\|}</script><p>其中，$\cdot$表示向量内积，$|\cdot|$表示向量的范数。</p><p>话题：文本所讨论的内容或主题。一个文本一般包含若干个话题。两个文本的话题相似，那么两者的语义应该也相似。<br>话题集合$L=\{l_1,l_2,\cdots,l_k\}$</p><p>话题向量：假设所有文本共含有$k$个话题，每个话题由定义在单词集合$W$上的$m$维向量表示</p><script type="math/tex; mode=display">t_l=\left[\begin{matrix}   t_{1l} \\   t_{2l} \\   \vdots \\   t_{ml}   \end{matrix}\right], \quad l=1,2,\cdots,k</script><p>其中，$t_{il}$是单词$w_i$在话题$t_l$的取值。</p><p>话题向量空间：单词-话题矩阵</p><script type="math/tex; mode=display">T= \left[\begin{matrix}   t_{11} & t_{12} & \cdots & t_{1k} \\   t_{21} & t_{22} & \cdots & t_{2k} \\   \vdots & \vdots &        & \vdots \\   t_{m1} & t_{m2} & \cdots & t_{mk}  \end{matrix}\right]</script><p>其中，元素$x_{ij}$表示单词$w_i$在文本$d_j$中出现的频次或权值。单词-话题矩阵$T$可表示为$T=\left[t_1,t_2,\cdots,t_k\right]$。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>topic model</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>word2vec</title>
    <link href="/2020/05/03/word2vec/"/>
    <url>/2020/05/03/word2vec/</url>
    
    <content type="html"><![CDATA[<h1 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><p><img src="https://i.loli.net/2020/05/03/iDAS7JfsQGxFrUO.png" srcset="/img/loading.gif"  align=center  width = "300" height = "200" /></p><h2 id="1-连续词袋模型（CBOW）与跳字模型（Skip-gram）"><a href="#1-连续词袋模型（CBOW）与跳字模型（Skip-gram）" class="headerlink" title="1 连续词袋模型（CBOW）与跳字模型（Skip-gram）"></a>1 连续词袋模型（CBOW）与跳字模型（Skip-gram）</h2><p>单词$w$；<br>词典$\mathcal{D}=\{w_1,w_2,\dots,w_N\}$，由单词组成的集合；<br>语料库$\mathcal{C}$，由单词组成的文本序列；<br>单词$w_t$的上下文是语料库中由单词$w_t$的前$c$个单词和后$c$个单词组成的文本序列，$w_t$称为中心词。</p><script type="math/tex; mode=display">Context\left(w_t\right)=\left(w_{t-c},\cdots,w_{t-2},w_{t-1},w_{t+1},w_{t+2},\cdots,w_{t+c}\right)</script><p><img src="https://i.loli.net/2020/05/04/DcTPmFWsXBCeiOo.png" srcset="/img/loading.gif"  align=center  width = "450" height = "300" /></p><p>连续词袋模型（CBOW, Continuous Bag-of-Words Model）假设中心词由该词在文本序列中的上下文来生成。<br>跳字模型（Skip-gram）假设中心词生成该词在文本序列中的上下文。</p><h2 id="2-基于层序softmax（Hierarchical-softmax）方法的连续词袋模型训练"><a href="#2-基于层序softmax（Hierarchical-softmax）方法的连续词袋模型训练" class="headerlink" title="2 基于层序softmax（Hierarchical softmax）方法的连续词袋模型训练"></a>2 基于层序softmax（Hierarchical softmax）方法的连续词袋模型训练</h2><p>基于层序softmax方法的连续词袋模型网络结构：<br>输入层：$\mathbf{v}\left(Context\left(w\right)_1\right),\mathbf{v}\left(Context\left(w\right)_2\right),\cdots,\mathbf{v}\left(Context\left(w\right)_{2c}\right)\in\mathbb{R}^m$，其中$\mathbf{v}\left(\cdot\right)$为单词的向量化表示；<br>投影层：$\mathbf{x}_w=\sum_{i=1}^{2c}\mathbf{v}\left(Context\left(w\right)_i\right)\in\mathbb{R}^m$；<br>输出层：$T_{Huff}\left(\mathbf{x}_w\right)=s_{q\left(\mathbf{x}_w\right)},s\in\mathbb{R}^N,q:\mathbb{R}^m\to\{1,2,\cdots,N\}$，其中$N$为哈夫曼树叶子结点个数。</p><p><img src="https://i.loli.net/2020/05/04/dC3aNnprhKwEDQP.png" srcset="/img/loading.gif"  align=center  width = "450" height = "350" /></p><p>记<script type="math/tex">p^w=\left(p_1^w,p_2^w\cdots,p_{l^w}^w\right)</script>为从根节点出发到达$w$对应的叶子结点的路径。其中，$l^w$为路径长度，即路径中结点数目；$p_i^w$为路径中的结点，$p_1^w$为根结点，$p_{l^w}^w$为$w$对应的叶子结点。</p><p>记</p><script type="math/tex; mode=display">d^w=\left(d_2^w,d_3^w\cdots,d_{l^w}^w\right)</script><p>为$w$的Huffman编码。其中，$d_i^w\in\{0,1\}$为路径$p^w$中第$i$个结点对应的编码（根结点不对应编码）。</p><p>记</p><script type="math/tex; mode=display">\theta^w=\left(\theta_1^w,\theta_2^w,\cdots,\theta_{l^w-1}^w\right)</script><p>为路径$p^w$中非叶子结点对应的参数向量。其中，$\theta_i^w\in\mathbb{R}^m$为路径$p^w$中第$i$个非叶子结点对应的参数向量。</p><p>条件概率</p><script type="math/tex; mode=display">p\left(w|Context\left(w\right)\right)=\prod_{j=2}^{l^w}p\left(d_j^w|\mathbf{x}_w,\theta_{j-1}^w\right)</script><p>其中</p><script type="math/tex; mode=display">p\left(d_j^w|\mathbf{x}_w,\theta_{j-1}^w\right)=\begin{equation} \left\{ \begin{array}{lr} \sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right),d^w_j =0; \\ 1-\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right),d^w_j=1, & \end{array} \right. \end{equation}</script><p>或者</p><script type="math/tex; mode=display">p\left(d_j^w|\mathbf{x}_w,\theta_{j-1}^w\right)=\left[\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right)\right]^{1-d_j^w}\cdot\left[1-\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right)\right]^{d_j^w}</script><p>似然函数</p><script type="math/tex; mode=display">\begin{align}\ell&=\prod_{w\in\mathcal{C}}p\left(w|Context\left(w\right)\right)\\&=\prod_{w\in\mathcal{C}}\prod_{j=2}^{l^w}p\left(d_j^w|\mathbf{x}_w,\theta_{j-1}^w\right)\end{align}</script><p>对数似然函数</p><script type="math/tex; mode=display">\begin{align}\mathcal{L} &= \log\prod_{w\in\mathcal{C}}\prod_{j=2}^{l^w}p\left(d_j^w|\mathbf{x}_w,\theta_{j-1}^w\right) \\&=\sum_{w\in\mathcal{C}}\log \prod_{j=2}^{l^w} p\left(d_j^w|\mathbf{x}_w,\theta_{j-1}^w\right) \\&=\sum_{w\in\mathcal{C}}\sum_{j=2}^{l^w}\left\{\left(1-d_j^w\right)\cdot\log\left[\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right)\right]+d_j^w\cdot\log\left[1-\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right)\right]\right\}\end{align}</script><p>对数似然函数$\mathcal{L}$关于$\theta_{j-1}^w$的偏导</p><script type="math/tex; mode=display">\begin{align}\frac{\partial\mathcal{L}}{\partial\theta_{j-1}^w}&= \frac{\partial}{\partial\theta_{j-1}^w}\left\{\sum_{w\in\mathcal{C}}\sum_{j=2}^{l^w}\left\{\left(1-d_j^w\right)\cdot\log\left[\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right)\right]+d_j^w\cdot\log\left[1-\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right)\right]\right\}\right\} \\&=\left(1-d_j^w\right)\left[1-\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right)\right]\mathbf{x}_w-d_j^w\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right)\mathbf{x}_w   \\&=\left[1-d_j^w-\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right)\right]\mathbf{x}_w\end{align}</script><p>$\theta_{j-1}^w$的更新</p><script type="math/tex; mode=display">\theta_{j-1}^w=\theta_{j-1}^w+\eta\left[1-d_j^w-\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right)\right]\mathbf{x}_w</script><p>其中，$\eta$为学习率。</p><p>对数似然函数$\mathcal{L}$关于$\mathbf{x}_w$的偏导</p><script type="math/tex; mode=display">\frac{\partial\mathcal{L}}{\partial\mathbf{x}_w}=\sum_{j=2}^{l^w}\left[1-d_j^w-\sigma\left(\mathbf{x}^\top_w\theta^w_{j-1}\right)\right]\theta_{j-1}^w</script><p>$\mathbf{v}\left(\tilde{w}\right)$的更新</p><script type="math/tex; mode=display">\mathbf{v}\left(\tilde{w}\right)=\mathbf{v}\left(\tilde{w}\right)+\eta\frac{\partial\mathcal{L}}{\partial\mathbf{x}_w}</script><p>其中，$\tilde{w}\in Context\left(w\right)$。</p><h2 id="3-基于层序softmax（Hierarchical-softmax）方法的跳字模型训练"><a href="#3-基于层序softmax（Hierarchical-softmax）方法的跳字模型训练" class="headerlink" title="3 基于层序softmax（Hierarchical softmax）方法的跳字模型训练"></a>3 基于层序softmax（Hierarchical softmax）方法的跳字模型训练</h2><p>基于层序softmax方法的跳字模型网络结构：<br>输入层：$\mathbf{v}\left(w\right)\in\mathbb{R}^m$<br>输出层：$T_{Huff}\left(\mathbf{v}_w\right)=s_{q\left(\mathbf{v}_w\right)},s\in\mathbb{R}^N,q:\mathbb{R}^m\to\{1,2,\cdots,N\}$</p><p>条件概率</p><script type="math/tex; mode=display">p\left(Context\left(w\right)|w\right)=\prod_{u\in Context\left(w\right)}p\left(u|w\right)</script><p>其中</p><script type="math/tex; mode=display">p\left(u|w\right)=\prod_{j=2}^{l^u}p\left(d_j^u|\mathbf{v}\left(w\right),\theta_{j-1}^u\right)</script><p>且</p><script type="math/tex; mode=display">p\left(d_j^u|\mathbf{v}\left(w\right),\theta_{j-1}^u\right)=\left[\sigma\left(\mathbf{v}\left(w\right)^\top\theta^u_{j-1}\right)\right]^{1-d_j^u}\cdot\left[1-\sigma\left(\mathbf{v}\left(w\right)^\top\theta^u_{j-1}\right)\right]^{d_j^u}</script><p>似然函数</p><script type="math/tex; mode=display">\begin{align}\ell&=\prod_{w\in\mathcal{C}} p\left(Context\left(w\right)|w\right)\\&=\prod_{w\in\mathcal{C}}\prod_{u\in Context\left(w\right)}\prod_{j=2}^{l^u}p\left(d_j^u|\mathbf{v}\left(w\right),\theta_{j-1}^u\right)\end{align}</script><p>对数似然函数</p><script type="math/tex; mode=display">\begin{align}\mathcal{L}&=\sum_{w\in\mathcal{C}}log\prod_{u\in Context\left(w\right)}\prod_{j=2}^{l^u}\left\{\left[\sigma\left(\mathbf{v}\left(w\right)^\top\theta^u_{j-1}\right)\right]^{1-d_j^u}\cdot\left[1-\sigma\left(\mathbf{v}\left(w\right)^\top\theta^u_{j-1}\right)\right]^{d_j^u}\right\}  \\&=\sum_{w\in\mathcal{C}}\sum_{u\in Context\left(w\right)}\sum_{j=2}^{l^u}\left\{\left(1-d_j^u\right)\cdot\log\left[\sigma\left(\mathbf{v}\left(w\right)^\top\theta_{j-1}^u\right)\right]+d_j^u\cdot\log\left[1-\sigma\left(\mathbf{v}\left(w\right)^\top\theta_{j-1}^u\right)\right]\right\}\end{align}</script><p>对数似然函数$\mathcal{L}$关于$\theta_{j-1}^u$的偏导</p><script type="math/tex; mode=display">\begin{align}\frac{\partial\mathcal{L}}{\partial\theta_{j-1}^u} &= \frac{\partial}{\theta_{j-1}^u}\left\{\sum_{w\in\mathcal{C}}\sum_{u\in Context\left(w\right)}\sum_{j=2}^{l^u}\left\{\left(1-d_j^u\right)\cdot\log\left[\sigma\left(\mathbf{v}\left(w\right)^\top\theta_{j-1}^u\right)\right]+d_j^u\cdot\log\left[1-\sigma\left(\mathbf{v}\left(w\right)^\top\theta_{j-1}^u\right)\right]\right\}\right\}\\&=\sum_{w\in\mathcal{C}}\left\{\left(1-d_j^u\right)\left[1-\sigma\left(\mathbf{v}\left(w\right)^\top\theta_{j-1}^u\right)\right]\mathbf{v}\left(w\right)-d_j^u\sigma\left(\mathbf{v}\left(w\right)^\top\theta_{j-1}^u\right)\mathbf{v}\left(w\right)\right\} \\&=\sum_{w\in\mathcal{C}}\left[1-d_j^u-\sigma\left(\mathbf{v}\left(w\right)^\top\theta_{j-1}^u\right)\right]\mathbf{v}\left(w\right)\end{align}</script><p>$\theta_{j-1}^u$的更新</p><script type="math/tex; mode=display">\theta_{j-1}^u=\theta_{j-1}^u+\eta\sum_{w\in\mathcal{C}}\left[1-d_j^u-\sigma\left(\mathbf{v}\left(w\right)^\top\theta_{j-1}^u\right)\right]\mathbf{v}\left(w\right)</script><p>其中，$\eta$为学习率。</p><p>对数似然函数$\mathcal{L}$关于$\mathbf{v}\left(w\right)$的偏导</p><script type="math/tex; mode=display">\frac{\partial\mathcal{L}}{\partial\mathbf{v}\left(w\right)}=\sum_{u\in Context\left(w\right)}\sum_{j=2}^{l^u}\left[1-d_j^u-\sigma\left(\mathbf{v}\left(w\right)^\top\theta_{j-1}^u\right)\right]\theta_{j-1}^u</script><p>$\mathbf{v}\left(w\right)$的跟新</p><script type="math/tex; mode=display">\mathbf{v}\left(w\right)=\mathbf{v}\left(w\right)+\eta\sum_{u\in Context\left(w\right)}\sum_{j=2}^{l^u}\left[1-d_j^u-\sigma\left(\mathbf{v}\left(w\right)^\top\theta_{j-1}^u\right)\right]\theta_{j-1}^u</script><h2 id="4-基于负采样（Negative-Sampling）方法的连续词袋模型训练"><a href="#4-基于负采样（Negative-Sampling）方法的连续词袋模型训练" class="headerlink" title="4 基于负采样（Negative Sampling）方法的连续词袋模型训练"></a>4 基于负采样（Negative Sampling）方法的连续词袋模型训练</h2><p>设$Context\left(w\right)$的负样本子集为</p><script type="math/tex; mode=display">NEG\left(w\right)\neq\emptyset</script><p>对于$\forall\tilde{w}\in\mathcal{D}$，定义</p><script type="math/tex; mode=display">\begin{equation} L^w\left(\tilde{w}\right)=\left\{ \begin{array}{lr} 1,\tilde{w}=w & \\ 0,\tilde{w}\neq w \end{array} \right. \end{equation}</script><p>表示词$\tilde{w}$的标签，正样本标签为$1$，负样本标签为$0$。</p><p>关于字典$\mathcal{D}$的子集$\{w\}\bigcup NEG\left(w\right)$的似然函数</p><script type="math/tex; mode=display">g\left(w\right)=\prod_{u\in\{w\}\bigcup NEG\left(w\right)} p\left(u|Context\left(w\right)\right)=\sigma\left(\mathbf{x}_w^\top\theta^w\right)\prod_{u\in NEG\left(w\right)}\left[1-\sigma\left(\mathbf{x}_w^\top\theta^w\right)\right]</script><p>其中</p><script type="math/tex; mode=display">\begin{equation} p\left(u|Context\left(w\right)\right)=\left\{ \begin{array}{lr} \sigma\left(\mathbf{x}_w^\top\theta^u\right),L^w\left(u\right)=1 & \\ 1-\sigma\left(\mathbf{x}_w^\top\theta^u\right),L^w\left(u\right)=0 \end{array} \right. \end{equation}</script><p>或者</p><script type="math/tex; mode=display">p\left(u|Context\left(w\right)\right)=\left[\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]^{L^w\left(u\right)}\cdot\left[1-\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]^{1-L^w\left(u\right)}</script><p>$\mathbf{x}_w$为$Context\left(w\right)$词向量之和，$\theta^u\in\mathbb{R}^m$为模型参数。</p><p>关于语料库$\mathcal{C}$的对数似然函数</p><script type="math/tex; mode=display">\begin{align}\mathcal{L}& =\log\prod_{w\in\mathcal{C}}g\left(w\right)=\sum_{w\in\mathcal{C}}\log g\left(w\right)  \\&=\sum_{w\in\mathcal{C}}\log\prod_{u\in\{w\}\bigcup NEG\left(w\right)}\left\{\left[\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]^{L^w\left(u\right)}\cdot\left[1-\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]^{1-L^w\left(u\right)}\right\} \\&=\sum_{w\in\mathcal{C}}\sum_{u\in\{w\}\bigcup NEG\left(w\right)}\left\{L^w\left(u\right)\cdot\log\left[\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]+\left[1-L^w\left(u\right)\right]\cdot\log\left[1-\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]\right\}\end{align}</script><p>对数似然函数$\mathcal{L}$关于$\theta^u$的偏导</p><script type="math/tex; mode=display">\begin{align}\frac{\partial\mathcal{L}}{\partial\theta^u}&=\frac{\partial}{\partial\theta^u}\left\{\sum_{w\in\mathcal{C}}\sum_{u\in\{w\}\bigcup NEG\left(w\right)}\left\{L^w\left(u\right)\cdot\log\left[\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]+\left[1-L^w\left(u\right)\right]\cdot\log\left[1-\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]\right\}\right\}\\&=L^w\left(u\right)\left[1-\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]\mathbf{x}_w-\left[1-L^w\left(u\right)\right]\sigma\left(\mathbf{x}_w^\top\theta^u\right)\mathbf{x}_w \\&=\left[L^w\left(u\right)-\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]\mathbf{x}_w\end{align}</script><p>$\theta^u$的更新</p><script type="math/tex; mode=display">\theta^u=\theta^u+\eta\left[L^w\left(u\right)-\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]\mathbf{x}_w</script><p>对数似然函数$\mathcal{L}$关于$\mathbf{x}_w$的偏导</p><script type="math/tex; mode=display">\frac{\partial\mathcal{L}}{\partial\mathbf{x}_w}=\sum_{u\in\left(w\right)\bigcup NEG\left(w\right)}\left[L^w\left(u\right)-\sigma\left(\mathbf{x}_w^\top\theta^u\right)\right]\theta^u</script><p>$\mathbf{v}\left(\tilde{w}\right)$的更新</p><script type="math/tex; mode=display">\mathbf{v}\left(\tilde{w}\right)=\mathbf{v}\left(\tilde{w}\right)+\eta\frac{\partial\mathcal{L}}{\partial\mathbf{x}_w}</script><p>其中，$\tilde{w}\in Context\left(w\right)$。</p><h2 id="5-基于负采样（Negative-Sampling）方法的跳字模型训练"><a href="#5-基于负采样（Negative-Sampling）方法的跳字模型训练" class="headerlink" title="5 基于负采样（Negative Sampling）方法的跳字模型训练"></a>5 基于负采样（Negative Sampling）方法的跳字模型训练</h2><p>关于字典$\mathcal{D}$的子集$\{w\}\bigcup NEG^{\tilde{w}}\left(w\right)$的似然函数</p><script type="math/tex; mode=display">g\left(w\right)=\prod_{\tilde{w}\in Context\left(w\right)}\prod_{u\in\{w\}\bigcup NEG^{\tilde{w}}\left(w\right)} p\left(u|\tilde{w}\right)</script><p>其中</p><script type="math/tex; mode=display">\begin{equation} p\left(u|\tilde{w}\right)=\left\{ \begin{array}{lr} \sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right),L^w\left(u\right)=1 & \\ 1-\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right),L^w\left(u\right)=0 \end{array} \right. \end{equation}</script><p>或者</p><script type="math/tex; mode=display">p\left(u|\tilde{w}\right)=\left[\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\right]^{L^w\left(u\right)}\cdot\left[1-\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\right]^{1-L^w\left(u\right)}</script><p>$NEG^{\tilde{w}}\left(w\right)$为处理词$\tilde{w}$时生成的负样本子集。</p><p>关于语料库$\mathcal{C}$的对数似然函数</p><script type="math/tex; mode=display">\begin{align}\mathcal{L}& =\log\prod_{w\in\mathcal{C}}g\left(w\right)=\sum_{w\in\mathcal{C}}\log g\left(w\right)  \\&=\sum_{w\in\mathcal{C}}\log\prod_{\tilde{w}\in Context\left(w\right)}\prod_{u\in\{w\}\bigcup NEG^{\tilde{w}}\left(w\right)}\left\{\left[\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\right]^{L^w\left(u\right)}\cdot\left[1-\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\right]^{1-L^w\left(u\right)}\right\} \\&=\sum_{w\in\mathcal{C}}\sum_{\tilde{w}\in Context\left(w\right)}\sum_{u\in\{w\}\bigcup NEG^{\tilde{w}}\left(w\right)}\left\{L^w\left(u\right)\cdot\log\left[\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\right]+\left[1-L^w\left(u\right)\right]\cdot\log\left[1-\sigma\left(\mathbf{v}\left(\tilde{w}\right)\top\theta^u\right)\right]\right\}\end{align}</script><p>对数似然函数$\mathcal{L}$关于$\theta^u$的偏导</p><script type="math/tex; mode=display">\begin{align}\frac{\partial\mathcal{L}}{\partial\theta^u}&=\frac{\partial}{\partial\theta^u}\left\{\sum_{w\in\mathcal{C}}\sum_{\tilde{w}\in Context\left(w\right)}\sum_{u\in\{w\}\bigcup NEG\left(w\right)}\left\{L^w\left(u\right)\cdot\log\left[\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\right]+\left[1-L^w\left(u\right)\right]\cdot\log\left[1-\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\right]\right\}\right\}\\&=L^w\left(u\right)\left[1-\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\right]\mathbf{v}\left(\tilde{w}\right)-\left[1-L^w\left(u\right)\right]\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\mathbf{v}\left(\tilde{w}\right) \\&=\left[L^w\left(u\right)-\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\right]\mathbf{v}\left(\tilde{w}\right)\end{align}</script><p>$\theta^u$的更新</p><script type="math/tex; mode=display">\theta^u=\theta^u+\eta\left[L^w\left(u\right)-\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\right]\mathbf{v}\left(\tilde{w}\right)</script><p>对数似然函数$\mathcal{L}$关于$\mathbf{v}\left(\tilde{w}\right)$的偏导</p><script type="math/tex; mode=display">\frac{\partial\mathcal{L}}{\partial\mathbf{v}\left(\tilde{w}\right)}=\sum_{u\in\left(w\right)\bigcup NEG^{\tilde{w}}\left(w\right)}\left[L^w\left(u\right)-\sigma\left(\mathbf{v}\left(\tilde{w}\right)^\top\theta^u\right)\right]\theta^u</script><p>$\mathbf{v}\left(\tilde{w}\right)$的更新</p><script type="math/tex; mode=display">\mathbf{v}\left(\tilde{w}\right)=\mathbf{v}\left(\tilde{w}\right)+\eta\frac{\partial\mathcal{L}}{\partial\mathbf{v}\left(\tilde{w}\right)}</script><h2 id="负采样算法"><a href="#负采样算法" class="headerlink" title="负采样算法"></a>负采样算法</h2><p><img src="https://i.loli.net/2020/05/04/PBpKFcatW5RZYqm.png" srcset="/img/loading.gif"  align=center  width = "550" height = "200" /></p><p>设词典$\mathcal{D}$中词$w_i$对应线段$l\left(w_i\right)$，长度为</p><script type="math/tex; mode=display">len\left(w_i\right)=\frac{counter\left(w_i\right)}{\sum_{u\in\mathcal{D}}counter\left(u\right)}</script><p>其中，$counter\left(\cdot\right)$为词在语料$\mathcal{C}$中的出现次数。可将线段$l\left(w_1\right)\cdots l\left(w_N\right)$拼接为长度为$1$的单位线段。</p><p>记</p><script type="math/tex; mode=display">\begin{align}l_0&=0 \\l_k&=\sum_{j=1}^k len\left(w_j\right),k=1,2,\cdots,N \end{align}</script><p>则以$\{l_j\}_{j=0}^N$为剖分点可得到区间$\left[0,1\right]$上的一个非等距剖分</p><script type="math/tex; mode=display">I_i=(l_{i-1},l_i],i=1,2,\cdots,N</script><p>在区间$\left[0,1\right]$上以剖分点$\left\{m_j\right\}_{j=0}^M$做等距剖分，其中$M\gg N$。</p><p>将等距剖分的内部点$\left\{m_j\right\}_{j=1}^{M-1}$投影到非等距剖分。则可建立$\left\{m_j\right\}_{j=1}^{M-1}$与区间$\left\{I_j\right\}_{j=1}^N$的映射，进一步建立与词$\left\{w_j\right\}_{j=1}^M$之间的映射</p><script type="math/tex; mode=display">w_k=Table\left(i\right),m_i\in I_k,i=1,2,\cdots,M-1</script>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>seq2seq_with_attention</title>
    <link href="/2020/05/03/seq2seq-with-attention/"/>
    <url>/2020/05/03/seq2seq-with-attention/</url>
    
    <content type="html"><![CDATA[<h1 id="Seq2Seq-with-Attention"><a href="#Seq2Seq-with-Attention" class="headerlink" title="Seq2Seq with Attention"></a>Seq2Seq with Attention</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><h2 id="1-序列到序列任务中的注意力机制"><a href="#1-序列到序列任务中的注意力机制" class="headerlink" title="1 序列到序列任务中的注意力机制"></a>1 序列到序列任务中的注意力机制</h2><h3 id="Seq2Seq-with-Attention网络架构"><a href="#Seq2Seq-with-Attention网络架构" class="headerlink" title="Seq2Seq with Attention网络架构"></a>Seq2Seq with Attention网络架构</h3><p><img src="https://i.loli.net/2020/05/03/tjJTB8Mbq3ngLW2.png" srcset="/img/loading.gif"  align=center  width = "300" height = "400" /></p><p>seq2seq with Attention神经网络架构中，编码器采用双向循环神经网络学习将输入序列$\mathbf{x}$编码成每个时刻的上下文向量（注意力分布）$c_i$，解码器学习将上下文向量$c_i$解码为输出序列$\mathbf{y}$。</p><p>源文本序列：$\mathbf{x}=\left(x_1,\cdots,x_{T_x}\right)$，其中$x_i\in\mathbb{R}^{K_x}$为one-of-K编码 ，$K_x$为源语言词表长度，$T_x$为源语料长度。<br>目标文本序列：$\mathbf{y}=\left(y_1,\cdots,y_{T_y}\right)$，其中$y_i\in\mathbb{R}^{K_y}$为one-of-K编码，$K_y$为目标语言词表长度，$T_y$为目标语料长度。</p><h3 id="Seq2Seq-with-Attention编码器Encoder原理"><a href="#Seq2Seq-with-Attention编码器Encoder原理" class="headerlink" title="Seq2Seq with Attention编码器Encoder原理"></a>Seq2Seq with Attention编码器Encoder原理</h3><p>编码器Encoder采用双向循环神经网络，前向状态计算</p><script type="math/tex; mode=display">\overrightarrow{h}_i=\left\{\begin{aligned}& \left(1-\overrightarrow{z}_i\right)\circ\overrightarrow{h}_{i-1}+\overrightarrow{z}_i\circ\overrightarrow{\underline{h}}_i& ,\mathbf{i}\mathbf{f}\ i>0\\& 0 &, \mathbf{i}\mathbf{f}\ i=0\end{aligned}\right.</script><p>其中，</p><script type="math/tex; mode=display">\overrightarrow{\underline{h}}_i=\tanh\left(\overrightarrow{W}\overline{E}x_i+\overrightarrow{U}\left[\overrightarrow{r}_i\circ\overrightarrow{h}_{i-1}\right]\right)  \\\overrightarrow{z}_i=\sigma\left(\overrightarrow{W}_z\overline{E}x_i+\overrightarrow{U}_z\overrightarrow{h}_{i-1}\right) \\\overrightarrow{r}_i=\sigma\left(\overrightarrow{W}_r\overline{E}x_i+\overrightarrow{U}_r\overrightarrow{h}_{i-1}\right)</script><p>$\overline{E}\in\mathbb{R}^{m\times K_x}$为词嵌入矩阵，$m$为词嵌入维度。$\overrightarrow{W},\overrightarrow{W}_z,\overrightarrow{W}_r\in\mathbb{R}^{n\times m}$和$\overrightarrow{U},\overrightarrow{U}_z,\overrightarrow{U}_r\in\mathbb{R}^{n\times n}$为权值矩阵，$n$为隐藏单元数。$\sigma\left(\cdot\right)$通常为sigmoid函数。</p><p>后向状态$\left(\overleftarrow{h}_1,\cdots,\overleftarrow{h}_{T_x}\right)$计算相同。与权值矩阵不同，我们在前向和后向RNN网络中共享词嵌入矩阵$\overline{E}$。</p><p>将前向和后向状态关联起来得到注释$\left(h_1,h_2,\cdots,h_{T_x}\right)$，<br>其中，<script type="math/tex">h_i=\begin{bmatrix}\overrightarrow{h}_i\\\overleftarrow{h}_i\end{bmatrix}</script></p><h3 id="Seq2Seq-with-Attention解码器Decoder原理"><a href="#Seq2Seq-with-Attention解码器Decoder原理" class="headerlink" title="Seq2Seq with Attention解码器Decoder原理"></a>Seq2Seq with Attention解码器Decoder原理</h3><p>解码器Decoder隐层转态$s_i$由解码器注释$h_i$计算的注意力分布$c_i$得到</p><script type="math/tex; mode=display">s_i=\left(1-z_i\right)\circ s_{i-1}+z_i\circ\tilde{s}_i</script><p>其中，</p><script type="math/tex; mode=display">\tilde{s}_i=\tanh\left(WEy_{i-1}+U\left[r_i\circ s_{i-1}\right]+Cc_i\right) \\z_i=\sigma\left(W_zEy_{i-1}+U_zs_{i-1}+C_rc_i\right) \\r_i=\sigma\left(W_rEy_{i-1}+U_rs_{i-1}+C_rc_i\right)</script><p>$E\in\mathbb{R}^{m\times K_y}$为目标语言的词嵌入矩阵，$m$为词嵌入维度。$W,W_z,W_r\in\mathbb{R}^{n\times m}$和$U,U_z,U_r\in\mathbb{R}^{n\times n}$以及<br>$C,C_z,C_r\in\mathbb{R}^{n\times 2n}$为权值矩阵，$n$为隐藏单元数。隐层初始状态$s_0=\tanh\left(W_s\overleftarrow{h}_1\right)$，其中$W_s\in\mathbb{R}^{n\times n}$。</p><p>每个时刻的上下文向量（注意力分布）$c_i$的计算</p><script type="math/tex; mode=display">c_i=\sum_{j=1}^{T_x}a_{ij}h_j</script><p>其中，</p><script type="math/tex; mode=display">a_{ij}=\frac{\exp\left(e_{ij}\right)}{\sum_{k=1}^{T_x}\exp\left(e_{ik}\right)}    \\ e_{ij}=v_a^\top\tanh\left(W_as_{i-1}+U_ah_j\right)</script><p>$h_j$为源文本序列第$j$个注释。$v_a\in\mathbb{R}^{n^{‘}},W_a\in\mathbb{R}^{n^{‘}\times n},U_a\in\mathbb{R}^{n^{‘}\times 2n}$为权值矩阵。</p><p>使用解码器状态$s_{i-1}$，上下文$c_i$和上时刻生成单词$y_{i-1}$定义目标单词$y_i$的概率</p><script type="math/tex; mode=display">p\left(y_i|s_i,y_{i-1},c_i\right)\propto\exp\left(y_i^\top W_o t_i\right)</script><p>其中，</p><script type="math/tex; mode=display">t_i=\left[\max\left\{\tilde{t}_{i,2j-1},\tilde{t}_{i,wj}\right\}\right]_{j=1,\cdots,l}^\top</script><p>$\tilde{t}_{i,k}$是向量$\tilde{t}_i$的第$k$个元素，</p><script type="math/tex; mode=display">\tilde{t}_i=U_os_{i-1}+V_oEy_{i-1}+C_oc_i</script><p>$W_o\in\mathbb{R}^{K_y\times l},U_o\in\mathbb{R}^{2l\times n},C_o\in\mathbb{R}^{2l\times 2n}$是权值矩阵。</p><h2 id="2-注意力机制"><a href="#2-注意力机制" class="headerlink" title="2 注意力机制"></a>2 注意力机制</h2><h3 id="柔性注意力机制（Soft-Attention）"><a href="#柔性注意力机制（Soft-Attention）" class="headerlink" title="柔性注意力机制（Soft Attention）"></a>柔性注意力机制（Soft Attention）</h3><p>输入信息$X=\left[\mathbf{x}_1,\cdots,\mathbf{x}_N\right]$</p><p>注意力机制的计算：</p><ol><li>在输入信息上计计算注意力分布；</li><li>根据注意力分布计算输入信息的加权平均。</li></ol><h4 id="注意力分布"><a href="#注意力分布" class="headerlink" title="注意力分布"></a>注意力分布</h4><p>给定一个和任务相关的查询向量$\mathbf{q}$，用注意力变量$z\in\left[1,N\right]$表示被选择信息的索引位置，即$z=i$表示选择了第$i$个输入信息。其中，查询向量$\mathbf{q}$可以是动态生成的，也可以是可学习的参数。</p><p>软性注意力的注意力分布<br>在给定输入信息$X$和查询变量$\mathbf{q}$下，选择第$i$个输入信息的概率</p><script type="math/tex; mode=display">\begin{align}\alpha_i&=p\left(z=i|X,\mathbf{q}\right) \\&=softmax\left(s\left(\mathbf{x}_i,\mathbf{q}\right)\right) \\&=\frac{\exp\left(s\left(\mathbf{x}_i,\mathbf{q}\right)\right)}{\sum_{j=1}^N\exp\left(s\left(\mathbf{x}_j,\mathbf{q}\right)\right)}\end{align}</script><p>其中，$\alpha_i$称为注意力分布，$s\left(\mathbf{x}_i,\mathbf{q}\right)$称为注意力打分函数。</p><p>注意力打分函数</p><ul><li>加性模型  $s\left(\mathbf{x}_i,\mathbf{q}\right)=\mathbf{v}^\top\tanh\left(W\mathbf{x}_i+U\mathbf{q}\right)$</li><li>点积模型  $s\left(\mathbf{x}_i,\mathbf{q}\right)=\mathbf{x}_i^\top\mathbf{q}$</li><li>缩放点积模型 $s\left(\mathbf{x}_i,\mathbf{q}\right)=\frac{\mathbf{x}_i^\top\mathbf{q}}{\sqrt{d}}$</li><li>双线性模型 $s\left(\mathbf{x}_i,\mathbf{q}\right)=\mathbf{x}_i^\top W\mathbf{q}$  </li></ul><p>其中，$W,U,\mathbf{v}$为可学习的网络参数，$d$为输入信息的维度。<br>加性模型和点积模型的复杂度近似，但点积模型可利用矩阵乘积，计算效率跟高。当输入信息的维度$d$比较高，点积模型值方差较大，导致softmax函数的梯度较小，缩放点积模型可以解决。双线性模型是泛化的点积模型。若假设$W=U^\top V$，则$s\left(\mathbf{x}_i,\mathbf{q}\right)=\mathbf{x}_i^\top U^\top V\mathbf{q}=\left(U\mathbf{x}_i\right)^\top\left(V\mathbf{q}\right)$，即分别对$\mathbf{x}_i$和$\mathbf{q}$进行线性变换后进行点积。相比点积模型，双线性模型在计算相似度是引入了非对称性。</p><p>注意力分布$\alpha_i$可解释为在给定相关查询$\mathbf{q}$时，第$i$个信息受关注的程度。</p><h4 id="加权平均"><a href="#加权平均" class="headerlink" title="加权平均"></a>加权平均</h4><p>注意力函数</p><script type="math/tex; mode=display">\begin{align}att\left(X,\mathbf{q}\right)&=\sum_{i=1}^N\alpha_i\mathbf{x}_i  \\&=\mathbb{E}_{z\thicksim p\left(z|X,\mathbf{q}\right)}\left[\mathbf{x}\right]\end{align}</script><p><img src="https://i.loli.net/2020/05/03/sHoKExjqJeTpgF6.png" srcset="/img/loading.gif"  align=center  width = "300" height = "300" /></p><h3 id="键值对注意力机制（Key-Value-Pair-Attention-Mechanism）"><a href="#键值对注意力机制（Key-Value-Pair-Attention-Mechanism）" class="headerlink" title="键值对注意力机制（Key-Value Pair Attention Mechanism）"></a>键值对注意力机制（Key-Value Pair Attention Mechanism）</h3><p>输入信息$\left(K,V\right)=\left[\left(\mathbf{k}_1,\mathbf{v}_1\right),\cdots,\left(\mathbf{k}_N,\mathbf{v}_N\right)\right]$，其中键用来计算注意力分布$\alpha_i$，值用来计算聚合信息。</p><p>给定任务相关查询向量$\mathbf{q}$时，注意力分布</p><script type="math/tex; mode=display">\alpha_i=\frac{\exp\left(s\left(\mathbf{k}_i,\mathbf{q}\right)\right)}{\sum_{j=1}^N\exp\left(s\left(\mathbf{k}_j,\mathbf{q}\right)\right)}</script><p>注意力函数</p><script type="math/tex; mode=display">\begin{align}att\left(\left(K,V\right),\mathbf{q}\right)&=\sum_{i=1}^N\alpha_i\mathbf{v}_i \\&=\sum_{i=1}^N\frac{\exp\left(s\left(\mathbf{k}_i,\mathbf{q}\right)\right)}{\sum_{j=1}^N\exp\left(s\left(\mathbf{k}_j,\mathbf{q}\right)\right)}\mathbf{v}_i\end{align}</script><p>其中，$s\left(\mathbf{k}_i,\mathbf{q}\right)$为打分函数。当$K=V$时，键值对注意力机制等价为柔性注意力机制。</p><p><img src="https://i.loli.net/2020/05/03/BJSWRXqLU1milEK.png" srcset="/img/loading.gif"  align=center  width = "300" height = "300" /></p><h3 id="多头注意力机制（Multi-Head-Attention-Mechanism）"><a href="#多头注意力机制（Multi-Head-Attention-Mechanism）" class="headerlink" title="多头注意力机制（Multi-Head Attention Mechanism）"></a>多头注意力机制（Multi-Head Attention Mechanism）</h3><p>多个查询$Q=\left[\mathbf{q}_1,\cdots,\mathbf{q}_M\right]$平行的计算从输入信息中选取多个信息。每个注意力关注输入信息的不同部分。</p><script type="math/tex; mode=display">att\left(\left(K,V\right),Q\right)=att\left(\left(K,V\right),\mathbf{q}_1\right)\oplus\cdots\oplus att\left(\left(K,V\right),\mathbf{q}_M\right)</script><p>其中，$\oplus$为向量拼接。</p><h3 id="自注意力模型（Self-Attention-Model）"><a href="#自注意力模型（Self-Attention-Model）" class="headerlink" title="自注意力模型（Self-Attention Model）"></a>自注意力模型（Self-Attention Model）</h3><p>输入序列$X=\left[\mathbf{x}_1,\cdots,\mathbf{x}_N\right]\in\mathbb{R}^{d_1\times N}$<br>输出序列$H=\left[\mathbf{h}_1,\cdots,\mathbf{h}_N\right]\in\mathbb{R}^{d_2\times N}$</p><p>通过线性变换得到向量序列：</p><script type="math/tex; mode=display">Q=W_QX\in\mathbb{R}^{d_3\times N}     \\K=W_KX\in\mathbb{R}^{d_3\times N}       \\V=W_VX\in\mathbb{R}^{d_2\times N}</script><p>其中，$Q$为查询向量序列，$K$为键向量序列，$V$为值向量序列，$W_Q,W_K,W_V$分别为可学习参数矩阵。</p><p>预测输出向量</p><script type="math/tex; mode=display">\begin{align}\hat{\mathbf{h}}_i&=att\left(\left(K,V\right),\mathbf{q}_i\right) \\&=\sum_{j=1}^N\alpha_{ij}\mathbf{v}_j \\&=\sum_{j=1}^Nsoftmax\left(s\left(\mathbf{k}_j,\mathbf{q}_i\right)\right)\mathbf{v}_j\end{align}</script><p>其中，$i,j\in\left[1,N\right]$为输出和输入向量序列的位置，连接权重$\alpha_{ij}$由注意力机制动态生成。</p><p>若使用缩放点积模型作为打分函数，则输出向量序列</p><script type="math/tex; mode=display">\begin{align}H_{d_2\times N}&=softmax\left(\frac{K^\top Q}{\sqrt{d_3}}\right)V_{d_2\times N} \\&=softmax\left(\frac{K^\top Q}{\sqrt{d_3}}\right)W_VX\end{align}</script><p>其中，softmax为按列归一化的函数。</p><p>自注意力模型计算的权重$\alpha_{ij}$只依赖$\mathbf{q}_i$和$\mathbf{k}_j$的相关性，而忽略了输入信息的位置信息。因此自注意力模型一般需要加入位置编码信息来进行修正。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>seq2seq</tag>
      
      <tag>attention</tag>
      
      <tag>soft attention</tag>
      
      <tag>key-value pair attention</tag>
      
      <tag>multi-head attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>seq2seq_with_Encoder-Decoder</title>
    <link href="/2020/05/03/seq2seq-with-Encoder-Decoder/"/>
    <url>/2020/05/03/seq2seq-with-Encoder-Decoder/</url>
    
    <content type="html"><![CDATA[<h1 id="seq2seq-with-Encoder-Decoder"><a href="#seq2seq-with-Encoder-Decoder" class="headerlink" title="seq2seq with Encoder-Decoder"></a>seq2seq with Encoder-Decoder</h1><script type="math/tex; mode=display">\begin{align}\end{align}</script><h2 id="1-RNN-Encoder-Decoder神经网络架构"><a href="#1-RNN-Encoder-Decoder神经网络架构" class="headerlink" title="1 RNN Encoder-Decoder神经网络架构"></a>1 RNN Encoder-Decoder神经网络架构</h2><p><img src="https://i.loli.net/2020/05/03/BpqV7omvnr1fLOd.png" srcset="/img/loading.gif"  align=center  width = "300" height = "300" /></p><p>RNN Encoder-Decoder神经网络架构使用循环神经网络学习将变长源序列$X$编码成定长向量表示$\mathbf{c}$，并将学习的定长向量表示$\mathbf{c}$解码成变长目标序列$Y$。模型的编码器和解码器被联合训练，以最大化给定源序列的目标序列的条件概率。</p><p>源文本序列：$X=\left(\mathbf{x}_{1}, \mathbf{x}_{2}, \dots, \mathbf{x}_{N}\right)$<br>其中，$\mathbf{x}_i=\left(l_1,l_2,\cdots,l_j,\cdots,l_K\right)$，其中$l_j=I\left(i=j\right),\quad\left(j=1,\cdots,K\right)$。</p><p>目标文本序列：$Y=\left(\mathbf{y}_{1}, \mathbf{y}_{2}, \dots, \mathbf{y}_{M}\right)$<br>其中，$\mathbf{y}_i=\left(l_1,l_2,\cdots,l_j,\cdots,l_K\right)$，其中$l_j=\left(i=j\right),\quad\left(j=1,\cdots,K\right)$</p><p>最大化条件似然函数</p><script type="math/tex; mode=display">\max_\theta \frac{1}{N}\sum_{n=1}^N \ln p_\theta\left(\mathbf{y}_n|\mathbf{x}_n\right)</script><p>其中，$\theta$是模型参数，$\left(\mathbf{y}_n,\mathbf{x}_n\right)$输入输出、输入序列对。</p><h2 id="2-编码器Encoder"><a href="#2-编码器Encoder" class="headerlink" title="2 编码器Encoder"></a>2 编码器Encoder</h2><p>源文本单词的词嵌入表示：$e\left(\mathbf{x}_i\right)\in\mathbb{R}^{500}$</p><p>编码器的隐藏状态由1000个隐藏单元组成。<br>编码器隐藏状态初始化，在$t=0$时刻第$j$个隐藏单元</p><script type="math/tex; mode=display">h_j^{\langle0\rangle}=0</script><p>在$t$时刻第$j$个隐藏单元<script type="math/tex">h_{j}^{\langle t\rangle}=z_{j} h_{j}^{\langle t-1\rangle}+\left(1-z_{j}\right) \tilde{h}_{j}^{\langle t\rangle}</script><br>其中，</p><script type="math/tex; mode=display">\begin{align}\tilde{h}_{j}^{\langle t \rangle}&=\tanh \left(\left[\mathbf{W} e\left(\mathbf{x}_{t}\right)\right]_{j}+\left[\mathbf{U}\left(\mathbf{r} \odot \mathbf{h}^{\langle t-1\rangle}\right)\right]_{j}\right)\\z_{j}&=\sigma\left(\left[\mathbf{W}_{z} e\left(\mathbf{x}_{t}\right)\right]_{j}+\left[\mathbf{U}_{z} \mathbf{h}^{\langle t-1\rangle}\right]_{j}\right) \\r_{j}&=\sigma\left(\left[\mathbf{W}_{r} e\left(\mathbf{x}_{t}\right)\right]_{j}+\left[\mathbf{U}_{r} \mathbf{h}^{\langle t-1\rangle}\right]_{j}\right)\end{align}</script><p>$\sigma\left(\cdot\right)$为sigmoid函数，$\odot$为向量元素乘法，$\mathbf{W},\mathbf{W}_z,\mathbf{W}_r\in\mathbb{R}^{1000\times 500}$和$\mathbf{U},\mathbf{U}_z,\mathbf{U}_r\in\mathbb{R}^{1000\times 1000}$为权值矩阵。为了使方程齐整，省略了偏置项。</p><p><img src="https://i.loli.net/2020/05/03/4nilNytzSkuX3jP.png" srcset="/img/loading.gif"  align=center tyle="zoom:10%" /></p><p>源文本最后第$N$时刻，编码器的隐藏状态计算完成，源文本的定长向量表示<script type="math/tex">\mathbf{c}=\tanh \left(\mathbf{V h}^{\langle N\rangle}\right)</script><br>其中，$\mathbf{V}\in\mathbb{R}^{1000\times 1000}$为权值矩阵。</p><h2 id="3-解码器Decoder"><a href="#3-解码器Decoder" class="headerlink" title="3 解码器Decoder"></a>3 解码器Decoder</h2><p>解码器隐藏状态初始化，在$t=0$时刻</p><script type="math/tex; mode=display">\mathbf{h}^{\prime\langle 0\rangle}=\tanh \left(\mathbf{V}^{\prime} \mathbf{c}\right)</script><p>其中，$\mathbf{V}\in\mathbb{R}^{1000\times 1000}$为权值矩阵。</p><p>在$t$时刻第$j$个隐藏单元</p><script type="math/tex; mode=display">h_{j}^{\prime\langle t\rangle}=z_{j}^{\prime} h_{j}^{\prime\langle t-1\rangle}+\left(1-z_{j}^{\prime}\right) \tilde{h^{\prime}}_{j}^{\langle t \rangle} )</script><p>其中，</p><script type="math/tex; mode=display">\begin{align}\tilde{h^{\prime}}_{j}^{\langle t\rangle}&=\tanh \left(\left[\mathbf{W}^{\prime} e\left(\mathbf{y}_{t-1}\right)\right]_{j}+r_{j}^{\prime}\left[\mathbf{U}^{\prime} \mathbf{h}_{\langle t-1\rangle}^{\prime}+\mathbf{C} \mathbf{c}\right]\right)  \\z_{j}^{\prime}&=\sigma\left(\left[\mathbf{W}_{z}^{\prime} e\left(\mathbf{y}_{t-1}\right)\right]_{j}+\left[\mathbf{U}_{z}^{\prime} \mathbf{h}^{\prime}_{\langle t-1\rangle}\right]_{j}+\left[\mathbf{C}_{z} \mathbf{c}\right]_{j}\right)  \\r_{j}^{\prime}&=\sigma\left(\left[\mathbf{W}_{r}^{\prime} e\left(\mathbf{y}_{t-1}\right)\right]_{j}+\left[\mathbf{U}_{r}^{\prime} \mathbf{h}^{\prime}_{\langle t-1\rangle}\right]_{j}+\left[\mathbf{C}_{r} \mathbf{c}\right]_{j}\right)\end{align}</script><p>其中，$\mathbf{W}^{\prime},\mathbf{W}_z^{\prime},\mathbf{W}_r^{\prime}\in\mathbb{R}^{1000\times 500}$和$\mathbf{U}^{\prime},\mathbf{U}_z^{\prime},\mathbf{U}_r^{\prime}\in\mathbb{R}^{1000\times 1000}$以及$\mathbf{C}^{\prime},\mathbf{C}_z^{\prime},\mathbf{C}_r^{\prime}\in\mathbb{R}^{1000\times 1000}$为权值矩阵。</p><p>目标文本单词的词嵌入表示：$e\left(\mathbf{y}_i\right)\in\mathbb{R}^{500}$，且在$t=0$时刻$e\left(\mathbf{y}_0\right)=\mathbf{0}$。</p><p>在每个时刻$t$，解码器计算生成第$j$个单词的概率</p><script type="math/tex; mode=display">p\left(y_{t, j}=1 | \mathbf{y}_{t-1}, \ldots, \mathbf{y}_{1}, X\right)=\frac{\exp \left(\mathbf{g}_{j} \mathbf{s}_{\langle t\rangle}\right)}{\sum_{j^{\prime}=1}^{K} \exp \left(\mathbf{g}_{j^{\prime}} \mathbf{s}_{\langle t\rangle}\right)}</script><p>其中，最大输出单元（maxout unit）</p><script type="math/tex; mode=display">s_{i}^{\langle t\rangle}=\max \left\{s_{2 i-1}^{\prime \langle t\rangle}, s_{2 i}^{\prime\langle t\rangle}\right\}</script><p>且</p><script type="math/tex; mode=display">\mathbf{s}^{\prime\langle t\rangle}=\mathbf{O}_{h} \mathbf{h}^{\prime\langle t\rangle}+\mathbf{O}_{y} \mathbf{y}_{t-1}+\mathbf{O}_{c} \mathbf{c}</script><p>$\mathbf{O}_h,\mathbf{O}_c\in\mathbb{R}^{500\times 1000}$和$\mathbf{O}_y\in\mathbb{R}^{500\times 500}$以及$\mathbf{G}=\left[\mathbf{g}_1,\cdots,\mathbf{g}_K\right]\in\mathbb{R}^{K\times 1000}$为权值矩阵。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>seq2seq</tag>
      
      <tag>encoder-decoder</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Transformer_Notes</title>
    <link href="/2020/05/03/Transformer-Notes/"/>
    <url>/2020/05/03/Transformer-Notes/</url>
    
    <content type="html"><![CDATA[<h1 id="Transformer特征提取器"><a href="#Transformer特征提取器" class="headerlink" title="Transformer特征提取器"></a>Transformer特征提取器</h1><script type="math/tex; mode=display">\begin{align}    \\     \end{align}    \\</script><p><img src="https://i.loli.net/2020/05/03/iQJMDVmbgIyx5F8.png" srcset="/img/loading.gif"  align=center  width = "300" height = "400" /></p><h2 id="1-输入序列、目标序列与输出序列"><a href="#1-输入序列、目标序列与输出序列" class="headerlink" title="1 输入序列、目标序列与输出序列"></a>1 输入序列、目标序列与输出序列</h2><p>输入序列$inputs=\left(i_1,i_2,\cdots,i_p,\cdots,i_N\right)$，其中$i_p\in\mathbb{N}$为输入符号表中的序号。<br>目标序列$targets=\left(t_1,t_2,\cdots,t_q,\cdots,t_M\right)$，其中$t_q\in\mathbb{N}$为目标符号表中的序号。        </p><script type="math/tex; mode=display">outputs\_probabilities=Transformer\left(inputs,targets\right)</script><p>其中，$outputs_probabilities=\left(o_1,o_2,\cdots,o_q,\cdots,o_M\right)$为预测序列，$o_q\in\mathbb{N*}$为目标符号表中的序号。</p><p>在自然语言处理任务中，当输入序列与目标序列中的元素较多，通常以句子为单位划分为若干个对应的“输入-目标”子序列进行学习。</p><h2 id="2-词嵌入与位置编码"><a href="#2-词嵌入与位置编码" class="headerlink" title="2 词嵌入与位置编码"></a>2 词嵌入与位置编码</h2><h3 id="输入序列词嵌入与位置编码"><a href="#输入序列词嵌入与位置编码" class="headerlink" title="输入序列词嵌入与位置编码"></a>输入序列词嵌入与位置编码</h3><p>输入序列词嵌入$Embedding\left(inputs\right)\in\mathbb{R}^{N\times d_{model}}$，其中，$N$为输入序列长度，$d_{model}$为词嵌入维度。</p><p>输入序列位置编码$Pos_Enc\left(inputs_position\right)\in\mathbb{R}^{N\times d_{model}}$，<br>其中，$inputs_position=\left(1,2,\cdots,p,\cdots,N\right)$为输入序列中输入符号对应的位置序号；  </p><script type="math/tex; mode=display">\begin{align}Pos\_Enc_{\left(pos,2i\right)}&=\sin\left(pos/10000^{2i/d_{model}}\right)  \\Pos\_Enc_{\left(pos,2i+1\right)}&=\cos\left(pos/10000^{2i/d_{model}}\right)\end{align}</script><p>其中，$pos\in inputs_position,i\in\left(0,1,\cdots,d_{model}/2\right)$。</p><h3 id="目标序列词嵌入与位置编码"><a href="#目标序列词嵌入与位置编码" class="headerlink" title="目标序列词嵌入与位置编码"></a>目标序列词嵌入与位置编码</h3><p>目标序列词嵌入$Embedding\left(targets\right)\in\mathbb{R}^{M\times d_{model}}$，其中$M$为目标序列长度，$d_{model}$为词嵌入维度。</p><p>目标序列位置编码$Pos_Enc\left(targets_position\right)\in\mathbb{R}^{M\times d_{model}}$，<br>其中，$targets_position=\left(1,2,\cdots,q,\cdots,M\right)$为目标序列的位置序号。</p><h2 id="3-编码器Encoder"><a href="#3-编码器Encoder" class="headerlink" title="3 编码器Encoder"></a>3 编码器Encoder</h2><h3 id="编码器结构"><a href="#编码器结构" class="headerlink" title="编码器结构"></a>编码器结构</h3><p>编码器结构：</p><script type="math/tex; mode=display">\begin{align}e_0&=Embedding\left(inputs\right)+Pos\_Enc\left(inputs\_position\right) \\e_l&=EncoderLayer\left(e_{l-1}\right),l\in\left[1,n\right] \\\end{align}</script><p>其中，$e_0\in\mathbb{R}^{N\times d_{model}}$为编码器输入，$EncoderLayer\left(\cdot\right)$为编码器层，$n$为层数，$e_l\in\mathbb{R}^{N\times d_{model}}$为第$l$层编码器层输出。</p><p>编码器层EncoderLayer：</p><script type="math/tex; mode=display">\begin{align}e_{mid}&=LayerNorm\left(e_{in}+MultiHeadAttention\left(e_{in}\right)\right) \\e_{out}&=LayerNorm\left(e_{mid}+FFN\left(e_{mid}\right)\right)\end{align}</script><p>其中，$e_{in}\in\mathbb{R}^{N\times d_{model}}$为编码器层输入，$e_{out}\in\mathbb{R}^{N\times d_{model}}$为编码器层输出，$MultiHeadAttention\left(\cdot\right)$为多头注意力机制，$FFN\left(\cdot\right)$为前馈神经网络，$LayerNorm\left(\cdot\right)$为层归一化。</p><h3 id="多头注意力机制与缩放点积"><a href="#多头注意力机制与缩放点积" class="headerlink" title="多头注意力机制与缩放点积"></a>多头注意力机制与缩放点积</h3><p><img src="https://i.loli.net/2020/05/03/3iRw2cN7gBqeWls.png" srcset="/img/loading.gif"  align=center tyle="zoom:50%" /></p><p>输入向量序列$e_{in}=\left(e_{in1},e_{in2},\cdots,e_{inN}\right)\in\mathbb{R}^{N\times d_{model}}$，分别得到查询向量序列$Q=e_{in}$，键向量序列$K=e_{in}$，值向量序列$V=e_{in}$。</p><p>多头注意力机制</p><script type="math/tex; mode=display">MultiHeadAttention\left(e_{in}\right)=MultiHead\left(Q,K,V\right)=Concat\left(head_1,\cdots,head_h\right)W^O</script><p>其中，多头输出$head_i=Attention\left(QW_i^Q,KW_i^K,VW_i^V\right)$，可学习的参数矩阵$W_i^Q\in\mathbb{R}^{d_{model}\times d_k},W_i^K\in\mathbb{R}^{d_{model}\times d_k},W_i^V\in\mathbb{R}^{d_{model}\times d_v},W^O\in\mathbb{R}^{hd_v\times d_{model}}$</p><p>使用缩放点积作为打分函数的自注意力机制</p><script type="math/tex; mode=display">Attention\left(QW_i^Q,KW_i^K,VW_i^K\right)=softmax\left(\frac{QW_i^Q\left(KW_i^K\right)^\top}{\sqrt{d_k}}\right)VW_i^V</script><h3 id="编码器pad掩码"><a href="#编码器pad掩码" class="headerlink" title="编码器pad掩码"></a>编码器pad掩码</h3><script type="math/tex; mode=display">enc\_pad\_mask_j=\left(e_{j1},e_{j2},\cdots,e_{jp},\cdots,e_{jN}\right)</script><p>其中，</p><script type="math/tex; mode=display">e_{jp}=\left\{\begin{array}{rcl}True,      &      & {i_p=0}\\False,     &      & {i_p \neq 0}\end{array} \right.   \quad j=1,2,\cdots,N</script><p>$enc_pad_mask\in\mathbb{R}^{N\times N}$，$i_p$为输入序列$inputs$对应位置序号。</p><h3 id="前馈神经网络"><a href="#前馈神经网络" class="headerlink" title="前馈神经网络"></a>前馈神经网络</h3><script type="math/tex; mode=display">\begin{align}FFN\left(e_{mid}\right)&=ReLU\left(e_{mid}W_1+b_1\right)W_2+b_2 \\&=\max\left(0,e_{mid}W_1+b_1\right)W_2+b_2\end{align}</script><p>其中，参数矩阵$W_1\in\mathbb{R}^{d_{model}\times d_{ff}},W_2\in\mathbb{R}^{d_{ff}\times d_{model}}$，偏置$b_1\in\mathbb{R}^{d_{ff}},b_2\in\mathbb{R}^{d_{model}}$。</p><h2 id="4-解码器Decoder"><a href="#4-解码器Decoder" class="headerlink" title="4 解码器Decoder"></a>4 解码器Decoder</h2><h3 id="解码器结构"><a href="#解码器结构" class="headerlink" title="解码器结构"></a>解码器结构</h3><script type="math/tex; mode=display">\begin{align}d_0&=Embedding\left(targets\right)+Pos\_Enc\left(targets\_position\right)  \\d_l&=DecoderLayer\left(d_{l-1}\right),l\in\left[1,n\right] \\outputs\_probabilities&=softmax\left(d_{n}W\right)\end{align}</script><p>其中，$d_0\in\mathbb{R}^{M\times d_{model}}$为解码器输入，$DecoderLayer\left(\cdot\right)$为解码器层，$n$为层数，$d_l\in\mathbb{R}^{M\times d_{model}}$为第$l$层解码器层输出，$W\in\mathbb{R}^{M\times tgt_vocab_size}$输入输出参数矩阵，$softmax\left(\cdot\right)$为softmax层。</p><p>解码器层DecoderLayer：</p><script type="math/tex; mode=display">\begin{align}d_{mid1}&=LayerNorm\left(d_{in}+MaskedMultiHeadAttention\left(d_{in}\right)\right) \\d_{mid2}&=LayerNorm\left(d_{mid1}+MultiHeadAttention\left(d_{mid1},e_{out}\right)\right) \\d_{out}&=LayerNorm\left(d_{mid2}+FFN\left(d_{mid2}\right)\right)\end{align}</script><p>其中，$d_{in}\in\mathbb{R}^{M\times d_{model}}$为解码器层输入，$d_{out}\in\mathbb{R}^{M\times d_{model}}$为解码器层输出，$MultiHeadAttention\left(\cdot\right)$为多头注意力机制，$FFN\left(\cdot\right)$为前馈神经网络，$LayerNorm\left(\cdot\right)$为层归一化。</p><h3 id="解码器pad掩码、解码器sequence掩码和编码器解码器pad掩码"><a href="#解码器pad掩码、解码器sequence掩码和编码器解码器pad掩码" class="headerlink" title="解码器pad掩码、解码器sequence掩码和编码器解码器pad掩码"></a>解码器pad掩码、解码器sequence掩码和编码器解码器pad掩码</h3><p>解码器pad掩码</p><script type="math/tex; mode=display">dec\_pad\_mask_j=\left(d_{j1},d_{j2},\cdots,d_{jq},\cdots,d_{jM}\right)</script><p>其中，</p><script type="math/tex; mode=display">d_{jq}=\left\{\begin{array}{rcl}True,      &      & {t_q=0}\\False,     &      & {t_q \neq 0}\end{array} \right.   \quad j=1,2,\cdots,M</script><p>$dec_pad_mask\in\mathbb{R}^{M\times M}$，$t_q$为目标序列$targets$对应位置序号。</p><p>解码器sequence掩码</p><script type="math/tex; mode=display">dec\_sequence\_mask_j=\left(s_{j1},s_{j2},\cdots,s_{jl},\cdots,s_{jM}\right)</script><p>其中，</p><script type="math/tex; mode=display">s_{jl}=\left\{\begin{array}{rcl}0,      &      & {j\geq l}\\1,     &      & {j < l}\end{array} \right.   \quad j=1,2,\cdots,M</script><p>$dec_sequence_mask\in\mathbb{R}^{M\times M}$，为非零元素为1的上三角矩阵。</p><p>解码器掩码</p><script type="math/tex; mode=display">dec\_mask=dec\_pad\_mask+dec\_sequence\_mask</script><p>编码器解码器pad掩码</p><script type="math/tex; mode=display">dec\_enc\_pad\_mask_j=\left(de_{j1},de_{j2},\cdots,de_{jp},\cdots,de_{jN}\right)</script><p>其中，</p><script type="math/tex; mode=display">de_{jp}=\left\{\begin{array}{rcl}True,      &      & {i_p=0}\\False,     &      & {i_p \neq 0}\end{array} \right.   \quad j=1,2,\cdots,M</script><p>$dec_enc_pad_mask\in\mathbb{R}^{M\times N}$，$i_p$为输入序列$inputs$对应位置序号。</p>]]></content>
    
    
    <categories>
      
      <category>Deep Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>nlp</tag>
      
      <tag>multi-head attention</tag>
      
      <tag>encoder-decoder</tag>
      
      <tag>self attention</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>XGBoost_Notes</title>
    <link href="/2020/05/02/XGBoost-Nots/"/>
    <url>/2020/05/02/XGBoost-Nots/</url>
    
    <content type="html"><![CDATA[<h1 id="XGBoost原理"><a href="#XGBoost原理" class="headerlink" title="XGBoost原理"></a>XGBoost原理</h1><script type="math/tex; mode=display">\begin{align}    \\ XGBoost&=eXtreme+GBDT  \\    &=eXtreme+(Gradient+BDT)    \\    &=eXtreme+Gradient+(Boosting+DecisionTree)   \\    \end{align}    \\</script><script type="math/tex; mode=display">Boosting \to BDT \to GBDT \to XGBoost</script><h2 id="1-提升方法（Boosting）"><a href="#1-提升方法（Boosting）" class="headerlink" title="1 提升方法（Boosting）"></a>1 提升方法（Boosting）</h2><p>提升方法使用加法模型和前向分步算法。</p><p>加法模型</p><script type="math/tex; mode=display">f\left(x\right)=\sum_{m=1}^M\beta_m b\left(x;\gamma_m\right)</script><p>其中，$b\left(x;\gamma_m\right)$为基函数，$\gamma_m$为基函数的参数，$\beta_m$为基函数的系数。</p><p>在给定训练数据$\{\left(x_i,y_i\right)\}_{i=1}^N$及损失函数$L\left(y,f\left(x\right)\right)$的条件下，学习加法模型$f\left(x\right)$成为经验风险极小化问题：</p><script type="math/tex; mode=display">\min_{\beta_m,\gamma_m}\sum_{i=1}^N L\left(y_i,\sum_{m=1}^M\beta_m b\left(x_i;\gamma_m\right)\right)</script><p>前向分步算法求解这一优化问题的思路：因为学习的是加法模型，可以从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数，则可以简化优化复杂度。具体地，每步只需优化如下损失函数：</p><script type="math/tex; mode=display">\min_{\beta,\gamma}\sum_{i=1}^N L\left(y_i,\beta b\left(x_i;\gamma\right)\right)</script><h3 id="算法1-1-前向分步算法"><a href="#算法1-1-前向分步算法" class="headerlink" title="算法1.1 前向分步算法"></a>算法1.1 前向分步算法</h3><p>输入：训练数据集$T=\{\left(x_1,y_1\right),\left(x_2,y_2\right),\dots,\left(x_N,y_N\right)\}$； 损失函数$L\left(y,f\left(x\right)\right)$；基函数集合$\{b\left(x;\gamma\right)\}$；   </p><p>输出：加法模型$f\left(x\right)$  </p><p>（1）初始化$f_0\left(x\right)=0$  </p><p>（2）对$m=1,2,\dots,M$  </p><p>（a）极小化损失函数</p><script type="math/tex; mode=display">\left(\beta_m,\gamma_m\right)=\mathop{\arg\min}_{\beta,\gamma} \sum_{i=1}^N L\left(y_i, f_{m-1}\left(x_i\right)+\beta b\left(x_i;\gamma\right)\right)</script><p>得到参数$\beta_m$，$\gamma_m$  </p><p>（b）更新</p><script type="math/tex; mode=display">f_m\left(x\right)=f_{m-1}\left(x\right)+\beta_m b\left(x;\gamma_m\right)</script><p>（3）得到加法模型  </p><script type="math/tex; mode=display">f\left(x\right)=f_M\left(x\right)=\sum_{m=1}^M\beta_m b\left(x;\gamma_m\right)</script><p>前向分步算法将同时求解从$m=1$到$M$所有参数$\beta_m,\gamma_m$的优化问题简化为逐次求解各个$\beta_m, \gamma_m$的优化问题。</p><h2 id="2-提升决策树-（BDT，Boosting-Decision-Tree）"><a href="#2-提升决策树-（BDT，Boosting-Decision-Tree）" class="headerlink" title="2 提升决策树 （BDT，Boosting Decision Tree）"></a>2 提升决策树 （BDT，Boosting Decision Tree）</h2><p>以决策树为基函数的提升方法为提升决策树。</p><p>提升决策树模型可以表示为决策树的加法模型：  </p><script type="math/tex; mode=display">f_M=\sum_{m=1}^M T\left(x;\Theta_m\right)</script><p>其中，$T\left(x;\Theta_m\right)$表示决策树；$\Theta_m$为决策树的参数；$M$为树的个数。</p><p>提升决策树采用前向分步算法。首先确定初始提升决策树$f_0\left(x\right)=0$，第$m$步的模型是</p><script type="math/tex; mode=display">f_m\left(x\right)=f_{m-1}\left(x\right)+T\left(x;\Theta_m\right)</script><p>其中，$f_{m-1}\left(x\right)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\Theta_m$，</p><script type="math/tex; mode=display">\hat{\Theta}_m=\mathop{\arg\min}_{\Theta_m}\sum_{i=1}^N L\left(y_i,f_{m-1}\left(x_i\right)+T\left(x_i;\Theta_m\right)\right)</script><p>已知训练数据集$T=\{\left(x_1,y_1\right),\left(x_2,y_2\right),\dots\left(x_N,y_N\right)\}$，$x_i\in\mathcal{X}\subseteq\mathbb{R}^n$，$\mathcal{X}$为输入空间，$y_i\in\mathcal{Y}\subseteq\mathbb{R}$，$\mathcal{Y}$为输出空间。如果将输入空间$\mathcal{X}$划分为$J$个互不相交的区域$R_1,R_2,\dots,R_J$，并且在每个区域上确定输出的常量$c_j$，那么决策树可表示为</p><script type="math/tex; mode=display">T\left(x;\Theta\right)=\sum_{j=1}^J c_j I\left(x\in R_j\right)</script><p>其中，参数$\Theta=\{\left(R_1,c_1\right),\left(R_2,c_2\right),\dots,\left(R_J,c_J\right)\}$表示决策树的区域划分和各区域上的常量值。$J$是决策树的复杂度即叶子结点个数。</p><p>提升决策树使用以下前向分步算法：</p><script type="math/tex; mode=display">\begin{align}f_0\left(x\right)&=0 \\f_m\left(x\right)&=f_{m-1}\left(x\right)+T\left(x;\Theta_m\right),\quad m=1,2,\dots,M        \\f_M\left(x\right)&=\sum_{m=1}^M T\left(x;\Theta_m\right)\end{align}</script><p>在前向分步算法的第$m$步，给定当前模型$f_{m-1}\left(x\right)$，需要求解</p><script type="math/tex; mode=display">\hat{\Theta}_m=\mathop{\arg\min}_{\Theta_m}\sum_{i=1}^N L\left(y_i,f_{m-1}\left(x_i\right)+T\left(x_i;\Theta_m\right)\right)</script><p>得到$\hat{\Theta}_m$，即第$m$棵树的参数。</p><p>当采用平方误差损失函数时，</p><script type="math/tex; mode=display">L\left(y,f\left(x\right)\right)=\left(y-f\left(x\right)\right)^2</script><p>其损失变为</p><script type="math/tex; mode=display">\begin{align}L\left(y,f_{m-1}\left(x\right)+T\left(x;\Theta_m\right)\right) &=\left[y-f_{m-1}\left(x\right)-T\left(x;\Theta_m\right)\right]^2 \\&=\left[r-T\left(x;\Theta_m\right)\right]^2\end{align}</script><p>其中，</p><script type="math/tex; mode=display">r=y-f_{m-1}\left(x\right)</script><p>是当前模型拟合数据的残差（residual）。对回归问题的提升决策树，只需要简单地拟合当前模型的残差。</p><h3 id="算法2-1-回归问题的提升决策树算法"><a href="#算法2-1-回归问题的提升决策树算法" class="headerlink" title="算法2.1 回归问题的提升决策树算法"></a>算法2.1 回归问题的提升决策树算法</h3><p>输入：训练数据集$T=\{\left(x_1,y_1\right),\left(x_2,y_2\right),\dots,\left(x_N,y_N\right)\}$；   </p><p>输出：提升决策树$f_M\left(x\right)$  </p><p>（1）初始化$f_0\left(x\right)=0$  </p><p>（2）对$m=1,2,\dots,M$  </p><p>（a）按照式（2.5）计算残差</p><script type="math/tex; mode=display">r_{mi}=y_i-f_{m-1}\left(x_i\right), \quad i=1,2,\dots,N</script><p>（b)拟合残差$r_{mi}$学习一个回归树，得到$T\left(x;\Theta_m\right)$  </p><p>（c）更新$f_m\left(x\right)=f_{m-1}\left(x\right)+T\left(x;\Theta_m\right) $  </p><p>（3）得到回归提升决策树 </p><script type="math/tex; mode=display">f_M\left(x\right)=\sum_{m=1}^M T\left(x;\Theta_m\right)</script><h2 id="3-梯度提升决策树-（GBDT，Gradient-Boosting-Decision-Tree）"><a href="#3-梯度提升决策树-（GBDT，Gradient-Boosting-Decision-Tree）" class="headerlink" title="3 梯度提升决策树 （GBDT，Gradient Boosting Decision Tree）"></a>3 梯度提升决策树 （GBDT，Gradient Boosting Decision Tree）</h2><p>梯度提升算法使用损失函数的负梯度在当前模型的值</p><script type="math/tex; mode=display">-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)}</script><p>作为回归问题提升决策树算法中残差的近似值，拟合一个回归树。</p><h3 id="算法3-1-梯度提升算法"><a href="#算法3-1-梯度提升算法" class="headerlink" title="算法3.1 梯度提升算法"></a>算法3.1 梯度提升算法</h3><p>输入：训练数据集$T=\{\left(x_1,y_1\right),\left(x_2,y_2\right),\dots,\left(x_N,y_N\right)\}$； 损失函数$L\left(y,f\left(x\right)\right)$  </p><p>输出：梯度提升决策树$\hat{f}\left(x\right)$  </p><p>（1）初始化</p><script type="math/tex; mode=display">f_0\left(x\right)=\mathop{\arg\min}_c\sum_{i=1}^N L\left(y_i,c\right)</script><p>（2）对$m=1,2,\dots,M$  </p><p>（a）对$i=1,2,\dots,N$，计算</p><script type="math/tex; mode=display">r_{mi}=-\left[\frac{\partial L\left(y,f\left(x_i\right)\right)}{\partial f\left(x_i\right)}\right]_{f\left(x\right)=f_{m-1}\left(x\right)}</script><p> （b)对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj},j=1,2,\dots,J$  </p><p>（c）对$j=1,2,\dots,J$，计算</p><script type="math/tex; mode=display">c_{mj}=\mathop{\arg\min}_c\sum_{x_i\in R_{mj}} L\left(y_i, f_{m-1}\left(x_i\right)+c\right)</script><p>（d）更新$f_m\left(x\right)=f_{m-1}\left(x\right)+\sum_{j=1}^J c_{mj} I\left(x\in R_{mj}\right)$  </p><p>（3）得到回归梯度提升决策树 </p><script type="math/tex; mode=display">\hat{f}\left(x\right)=f_M\left(x\right)=\sum_{m=1}^M \sum_{j=1}^J c_{mj} I\left(x\in R_{mj}\right)</script><h2 id="4-极致梯度提升决策树（XGBoost，eXtreme-Gradient-Boosting-Decision-Tree）"><a href="#4-极致梯度提升决策树（XGBoost，eXtreme-Gradient-Boosting-Decision-Tree）" class="headerlink" title="4 极致梯度提升决策树（XGBoost，eXtreme Gradient Boosting Decision Tree）"></a>4 极致梯度提升决策树（XGBoost，eXtreme Gradient Boosting Decision Tree）</h2><p>训练数据集$\mathcal{D}=\{\left(\mathbf{x}_i,y_i\right)\}$，其中$\mathbf{x}_i\in\mathbb{R}^m,y_i\in\mathbb{R},\left|\mathcal{D}\right|=n$。</p><p>决策树模型</p><script type="math/tex; mode=display">f\left(\mathbf{x}\right)=w_{q\left(\mathbf{x}\right)}</script><p>其中，$q:\mathbb{R}^m\to \{1,\dots,T\}$是有输入$\mathbf{x}$向叶子结点编号的映射，$w\in\mathbb{R}^T$是叶子结点向量，$T$为决策树叶子节点数。</p><p>提升决策树模型预测输出</p><script type="math/tex; mode=display">\hat{y}_i=\phi\left(\mathbf{x}_i\right)=\sum_{k=1}^K f_k\left(\mathbf{x}_i\right)</script><p>其中，$f_k\left(\mathbf{x}\right)$为第$k$棵决策树。</p><p>正则化目标函数</p><script type="math/tex; mode=display">\mathcal{L}\left(\phi\right)=\sum_i l\left(\hat{y}_i,y_i\right)+\sum_k \Omega\left(f_k\right)</script><p>其中，$\Omega\left(f\right)=\gamma T+\frac{1}{2}\lambda|w|^2=\gamma T+\frac{1}{2}\lambda\sum_{j=1}^T w_j^2$。</p><p>第$t$轮目标函数</p><script type="math/tex; mode=display">\mathcal{L}^{\left(t\right)}=\sum_{i=1}^n l\left(y_i,\hat{y}^{\left(t-1\right)}_i+f_t\left(\mathbf{x}_i\right)\right)+\Omega\left(f_t\right)</script><p>第$t$轮目标函数$\mathcal{L}^{\left(t\right)}$在$\hat{y}^{\left(t-1\right)}$处的二阶泰勒展开</p><script type="math/tex; mode=display">\begin{align}\mathcal{L}^{\left(t\right)}&\simeq\sum_{i=1}^n\left[l\left(y_i,\hat{y}^{\left(t-1\right)}\right)+\partial_{\hat{y}^{\left(t-1\right)}}l\left(y_i,\hat{y}^{\left(t-1\right)}\right) f_t\left(\mathbf{x}_i\right)+\frac{1}{2}\partial^2_{\hat{y}^{\left(t-1\right)}}l\left(y_i,\hat{y}^{\left(t-1\right)}\right) f^2_t\left(\mathbf{x}_i\right)\right]+\Omega\left(f_t\right)  \\&=\sum_{i=1}^n\left[l\left(y_i,\hat{y}^{\left(t-1\right)}\right)+g_i f_t\left(\mathbf{x}_i\right)+\frac{1}{2}h_i f^2_t\left(\mathbf{x}_i\right)\right]+\Omega\left(f_t\right)\end{align}</script><p>其中，$g_i=\partial_{\hat{y}^{\left(t-1\right)}}l\left(y_i,\hat{y}^{\left(t-1\right)}\right),h_i=\partial^2_{\hat{y}^{\left(t-1\right)}}l\left(y_i,\hat{y}^{\left(t-1\right)}\right)$。</p><p>第$t$轮目标函数$\mathcal{L}^{\left(t\right)}$的二阶泰勒展开移除关于$f_t\left(\mathbf{x}_i\right)$的常数项</p><script type="math/tex; mode=display">\begin{align}\tilde{\mathcal{L}}^{\left(t\right)}&=\sum_{i=1}^n\left[g_i f_t\left(\mathbf{x}_i\right)+\frac{1}{2}h_i f^2_t\left(\mathbf{x}_i\right)\right]+\Omega\left(f_t\right) \\&=\sum_{i=1}^n\left[g_i f_t\left(\mathbf{x}_i\right)+\frac{1}{2}h_i f^2_t\left(\mathbf{x}_i\right)\right]+\gamma T+\frac{1}{2}\lambda\sum_{j=1}^T w_j^2\end{align} \\</script><p>定义叶结点$j$上的样本的下标集合$I_j=\{i|q\left(\mathbf{x}_i\right)=j\}$，则目标函数可表示为按叶结点累加的形式</p><script type="math/tex; mode=display">\tilde{\mathcal{L}}^{\left(t\right)}=\sum_{j=1}^T\left[\left(\sum_{i\in I_j}g_i\right)w_j+\frac{1}{2}\left(\sum_{i\in I_j}h_i+\lambda\right)w_j^2\right]+\gamma T</script><p>由于</p><script type="math/tex; mode=display">w_j^*=\mathop{\arg\min}_{w_j}\tilde{\mathcal{L}}^{\left(t\right)}</script><p>可令</p><script type="math/tex; mode=display">\frac{\partial\tilde{\mathcal{L}}^{\left(t\right)}}{\partial w_j}=0</script><p>得到每个叶结点$j$的最优分数为</p><script type="math/tex; mode=display">w_j^*=-\frac{\sum_{i\in I_j}g_i}{\sum_{i\in I_j} h_i+\lambda}</script><p>代入每个叶结点$j$的最优分数，得到最优化目标函数值</p><script type="math/tex; mode=display">\tilde{\mathcal{L}}^{\left(t\right)}\left(q\right)=-\frac{1}{2}\sum_{j=1}^T \frac{\left(\sum_{i\in I_j} g_i\right)^2}{\sum_{i\in I_j} h_i+\lambda}+\gamma T</script><p>假设$I_L$和$I_R$分别为分裂后左右结点的实例集，令$I=I_L\cup I_R$，则分裂后损失减少量由下式得出</p><script type="math/tex; mode=display">\mathcal{L}_{split}=\frac{1}{2}\left[\frac{\left(\sum_{i\in I_L} g_i\right)^2}{\sum_{i\in I_L}h_i+\lambda}+\frac{\left(\sum_{i\in I_R} g_i\right)^2}{\sum_{i\in I_R}h_i+\lambda}-\frac{\left(\sum_{i\in I} g_i\right)^2}{\sum_{i\in I}h_i+\lambda}\right]-\gamma</script><p>用以评估待分裂结点。</p><h3 id="算法4-1-分裂查找的精确贪婪算法"><a href="#算法4-1-分裂查找的精确贪婪算法" class="headerlink" title="算法4.1 分裂查找的精确贪婪算法"></a>算法4.1 分裂查找的精确贪婪算法</h3><p>输入：当前结点实例集$I$;特征维度$d$  </p><p>输出：根据最大分值分裂  </p><p>（1）$gain\leftarrow 0$  </p><p>（2）$G\leftarrow\sum_{i\in I}g_i$，$H\leftarrow\sum_{i\in I}h_i$  </p><p>（3）for $k=1$ to $d$ do  </p><p>（3.1）$G_L \leftarrow 0$，$H_L \leftarrow 0$  </p><p>（3.2）for $j$ in sorted($I$, by $\mathbf{x}_{jk}$) do  </p><p>（3.2.1）$G_L \leftarrow G_L+g_j$，$H_L \leftarrow H_L+h_j$  </p><p>（3.2.2）$G_R \leftarrow G-G_L$，$H_R=H-H_L$  </p><p>（3.2.3）$score \leftarrow \max\left(score,\frac{G_L^2}{H_L+\lambda}+\frac{G_R^2}{H_R+\lambda}-\frac{G^2}{H+\lambda}\right)$  </p><p>（3.3）end  </p><p>（4）end</p><h3 id="算法4-2-分裂查找的近似贪婪算法"><a href="#算法4-2-分裂查找的近似贪婪算法" class="headerlink" title="算法4.2 分裂查找的近似贪婪算法"></a>算法4.2 分裂查找的近似贪婪算法</h3><p>输入：当前结点实例集$I$;特征维度$d$  </p><p>输出：根据最大分值分裂  </p><p>（1）for $k=1$ to $d$ do  </p><p>（1.1）通过特征$k$的百分位数求候选分割点$S_k=\{s_{k1},s_{k2},\dots,s_{kl}\}$   </p><p>（1.2）可以在每颗树生成后（全局），可以在每次分裂后（局部）  </p><p>（2）end  </p><p>（3）for $k=1$ to $m$ do  </p><p>（3.1）$G_{kv}\gets =\sum_{j\in\{j|s_{k,v}\geq\mathbf{x}_{jk}&gt;s_{k,v-1}\}}g_j$  </p><p>（3.2）$H_{kv}\gets =\sum_{j\in\{j|s_{k,v}\geq\mathbf{x}_{jk}&gt;s_{k,v-1}\}}h_j$  </p><p>（4）end    </p><p>按照与前一节相同的步骤，在提议的分割中找到最大值。</p><p>候选分割点$S_k=\{s_{k1},s_{k2},\dots,s_{kl}\}$中，  </p><p>令</p><script type="math/tex; mode=display">s_{k1}=\min_i\mathbf{x}_{ik},\;s_{kl}=\max_i\mathbf{x}_{ik}</script><p>其余各分割点满足</p><script type="math/tex; mode=display">|r_k\left(s_{k,j}\right)-r_k\left(s_{k,j+1}\right)|<\epsilon</script><p>其中，函数$r_k:\mathbb{R}\to[0,+\infty)$</p><script type="math/tex; mode=display">r_k\left(z\right)=\frac{1}{\sum_{\left(x,h\right)\in\mathcal{D}_k}h}\sum_{\left(x,h\right)\in\mathcal{D}_k,x<z}h</script><p>其中$\mathcal{D}_k=\{\left(x_{1k},h_1\right),\left(x_{2k},h_2\right),\dots,\left(x_{nk},h_n\right)\} $</p><p>如果$h_i$作为数据点权重</p><script type="math/tex; mode=display">\sum_{i=1}^n\frac{1}{2}h_i\left(f_t\left(\mathbf{x}_i\right)-g_i/h_i\right)^2+\Omega\left(f_t\right)+constant</script><p>即是权重为$h_i$的$f_t\left(\mathbf{x}_i\right)$对$g_i/h_i$的加权平方损失。</p>]]></content>
    
    
    <categories>
      
      <category>Machine Learning</category>
      
    </categories>
    
    
    <tags>
      
      <tag>decision tree</tag>
      
      <tag>boosting</tag>
      
      <tag>gbdt</tag>
      
      <tag>xgboost</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
